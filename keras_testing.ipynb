{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbd0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4e3cd",
   "metadata": {},
   "source": [
    "<b>First pass</b>: ordinary, small convolutional networks given image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6320685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_small = np.load('datasets/train_img_64.npy')\n",
    "train_lbl_small = np.load('datasets/train_lbl_64.npy')\n",
    "val_img_small = np.load('datasets/val_img_64.npy')\n",
    "val_lbl_small = np.load('datasets/val_lbl_64.npy')\n",
    "train_img_small = np.float32(train_img_small) / 255\n",
    "val_img_small = np.float32(val_img_small) / 255\n",
    "print(train_img_small.shape, train_lbl_small.shape)\n",
    "print(val_img_small.shape, val_lbl_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data pre-generated by albumentations\n",
    "train_img_synth = np.concatenate([np.load('datasets/train_img_64_synth.npy'), np.load('datasets/train_img_64.npy')], axis=0)\n",
    "train_lbl_synth = np.concatenate([np.load('datasets/train_lbl_64_synth.npy'), np.load('datasets/train_lbl_64.npy')], axis=0)\n",
    "train_img_synth = np.float32(train_img_synth) / 255\n",
    "print(train_img_synth.shape, train_lbl_synth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df2c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_prob = 0.2\n",
    "spatial_dropout_prob = 0.05\n",
    "reg_coef = 0.01   # experiments suggest this coef might be far too large\n",
    "noise_sigma = 0.04\n",
    "regulator = tf.keras.regularizers.L2(reg_coef)\n",
    "this_model3 = tf.keras.Sequential([ # tf.keras.layers.Rescaling(1. / 255),\n",
    "                                tf.keras.layers.GaussianNoise(noise_sigma),\n",
    "                                    tf.keras.layers.Convolution2D(64, 5, activation='relu', \n",
    "                                                               padding='same', use_bias = True,\n",
    "                                            input_shape = (64,64,3), kernel_regularizer=regulator),\n",
    "                                 tf.keras.layers.Dropout(dropout_prob),\n",
    "                                 tf.keras.layers.SpatialDropout2D(spatial_dropout_prob),\n",
    "                                 tf.keras.layers.Convolution2D(64, 3, activation='relu', \n",
    "                                                               padding='same', use_bias = True,\n",
    "                                                               kernel_regularizer=regulator),\n",
    "                                 tf.keras.layers.MaxPool2D(strides=(2,2)), # default pool size (2,2); cuts down to 32x32xch\n",
    "                                 tf.keras.layers.Convolution2D(128, 3, activation='relu', \n",
    "                                                               padding='same', use_bias = True,\n",
    "                                                              kernel_regularizer=regulator),\n",
    "                                 tf.keras.layers.SpatialDropout2D(spatial_dropout_prob),\n",
    "                                 tf.keras.layers.Dropout(dropout_prob),\n",
    "                                 tf.keras.layers.Convolution2D(128, 3, activation='relu', \n",
    "                                                               padding='same', use_bias = True,\n",
    "                                                              kernel_regularizer=regulator),\n",
    "                                 tf.keras.layers.MaxPool2D(strides=(2,2)), # cuts down to 16x16xch\n",
    "                                 tf.keras.layers.Convolution2D(128, 3, activation='relu', \n",
    "                                                               padding='same', use_bias = True,\n",
    "                                                              kernel_regularizer=regulator),\n",
    "                                 tf.keras.layers.SpatialDropout2D(spatial_dropout_prob),\n",
    "                                 tf.keras.layers.Dropout(dropout_prob),\n",
    "                                 tf.keras.layers.Convolution2D(128, 3, activation='relu', \n",
    "                                                               padding='same', use_bias = True,\n",
    "                                                              kernel_regularizer=regulator),\n",
    "                                 tf.keras.layers.MaxPool2D(strides=(2,2)), # cuts down to 8x8xch\n",
    "                                 tf.keras.layers.Flatten(), # 8192 outputs coming here\n",
    "                                 tf.keras.layers.Dense(512, activation='relu'),\n",
    "                                 tf.keras.layers.Dropout(dropout_prob),\n",
    "                                 tf.keras.layers.Dense(50, activation='softmax')])\n",
    "                                 \n",
    "this_model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0f54b",
   "metadata": {},
   "source": [
    "Results: On greyscale, 0.58 val (0.81 tr) after 11 epoch. Compare 0.718 val (0.899 tr) after 19 epochs, back on colour; with dropout 0.2, reg 0.01. Regressed with dropout 0.3, reg 0.02, with 0.65 val (0.81 tr) after 17 epochs. With 64/128/256 filters, 0.72 val (0.92 tr) after 18 epochs. With 3 layers in blocks 2 and 3 (back at 64/128/128), 0.675 (0.86) after 14 epochs. With Scharr filters in input channels, 0.66 (0.87) after 14 epochs.\n",
    "\n",
    "Moving from tanh to relu got us to 0.767 (0.95) after 19 epochs. Added GaussianNoise(0.1) and replaced Dropout with SpatialDropout(0.1). Ended at 0.658 (0.853) after 18 epochs. At this point I realised some the image set hadn't been standardised (as RGB). So I tried that again with SpatialDropout turned down to 0.05. Tried some synthetic data, things got worse. Back up some... take out all but L^2 reg, get 0.614 (0.930) after 8 epochs.\n",
    "\n",
    "Since we still get high scores on the training set it appears the network is expressive enough (at blocks of 2, with 64/128/128 filters) to handle most of that, and getting this generalisation difference down is what we need.\n",
    "\n",
    "So, 0.65 (0.93) after 14 epochs, with regular dropout. Next try, reintroduce gaussian noise at sigma=0.04; got 0.536 (0.88) at epoch 16. Add Dense(512) before the end; got 0.58 (0.95) at epoch 20. Return SpatialDropout, got 0.582 (0.95). Adding some synthetics, 0.608 (0.956)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089c8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 1192s 693ms/step - loss: 0.7291 - accuracy: 0.8630 - val_loss: 2.3349 - val_accuracy: 0.5951\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 1207s 702ms/step - loss: 0.6071 - accuracy: 0.8908 - val_loss: 2.2947 - val_accuracy: 0.6001\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 1195s 695ms/step - loss: 0.5447 - accuracy: 0.9089 - val_loss: 2.4485 - val_accuracy: 0.5882\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 1185s 689ms/step - loss: 0.4996 - accuracy: 0.9198 - val_loss: 2.7177 - val_accuracy: 0.5885\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 1179s 686ms/step - loss: 0.4688 - accuracy: 0.9274 - val_loss: 2.7426 - val_accuracy: 0.5899\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 1185s 689ms/step - loss: 0.4417 - accuracy: 0.9339 - val_loss: 2.5915 - val_accuracy: 0.6083\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 1180s 687ms/step - loss: 0.4213 - accuracy: 0.9404 - val_loss: 2.6203 - val_accuracy: 0.6040\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 1192s 693ms/step - loss: 0.3986 - accuracy: 0.9467 - val_loss: 2.8306 - val_accuracy: 0.6041\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 1197s 696ms/step - loss: 0.3841 - accuracy: 0.9496 - val_loss: 2.7694 - val_accuracy: 0.6073\n",
      "Epoch 10/20\n",
      " 844/1719 [=============>................] - ETA: 10:23 - loss: 0.3641 - accuracy: 0.9562"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mthis_model3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_img_synth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lbl_synth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_img_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_lbl_small\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "this_model3.fit(train_img_synth, train_lbl_synth, validation_data=(val_img_small,val_lbl_small), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8806a1",
   "metadata": {},
   "source": [
    "<b>Second pass</b>: The next model is a test of making a skip (\"residual\") connection in the network. The output of the first layer of each block becomes part of the output of the next layer. Since my blocks only have 2 layers in them this involves shrinking the output to match the size at the next block. Results were not encouraging, but I didn't try for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "664ee39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testSkipModel(tf.keras.Model):\n",
    "    def __init__(self, labels, filters, rec_field, dropout_prob = 0.2, reg_coef = 0.001):\n",
    "        super(testSkipModel, self).__init__()\n",
    "        filters_1, filters_2, filters_3 = filters\n",
    "        regulator = tf.keras.regularizers.L2(reg_coef)\n",
    "\n",
    "        self.conv_1a = tf.keras.layers.Convolution2D(filters_1, 5, padding='same', use_bias=True, \n",
    "                                                     activation='tanh', kernel_regularizer=regulator)\n",
    "        self.conv_1b = tf.keras.layers.Convolution2D(filters_1, 3, padding='same', use_bias=True, \n",
    "                                                     activation='tanh', kernel_regularizer=regulator)\n",
    "    \n",
    "        self.conv_2a = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, \n",
    "                                                     activation='tanh', kernel_regularizer=regulator)\n",
    "        self.conv_2b = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, \n",
    "                                                     activation='tanh', kernel_regularizer=regulator)\n",
    "        \n",
    "        self.conv_3a = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, \n",
    "                                                     activation='tanh', kernel_regularizer=regulator)\n",
    "        self.conv_3b = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, \n",
    "                                                     activation='tanh', kernel_regularizer=regulator)\n",
    "        \n",
    "        self.collate = tf.keras.layers.Dense(labels, kernel_regularizer=regulator, activation='softmax')\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        #out = tf.keras.layers.Rescaling(1. / 255)(input_tensor)\n",
    "        out = tf.keras.layers.Dropout(0.2)(self.conv_1a(input_tensor))\n",
    "        out_temp = tf.keras.layers.MaxPool2D(strides=(2,2))(out)\n",
    "        out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_1b(out))\n",
    "        \n",
    "        out = tf.raw_ops.Concat(concat_dim=3, values=[out, out_temp]) # skip connection from 1a\n",
    "        out = tf.keras.layers.Dropout(0.2)(self.conv_2a(out))\n",
    "        out_temp = tf.keras.layers.MaxPool2D(strides=(2,2))(out)\n",
    "        out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_2b(out))\n",
    "        \n",
    "        out = tf.raw_ops.Concat(concat_dim=3, values=[out, out_temp]) # skip connection from 2a\n",
    "        out = tf.keras.layers.Dropout(0.2)(self.conv_3a(out))\n",
    "        out_temp = tf.keras.layers.MaxPool2D(strides=(2,2))(out)\n",
    "        out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_3b(out))\n",
    "        \n",
    "        out = tf.raw_ops.Concat(concat_dim=3, values=[out, out_temp]) # skip connection from 3a        \n",
    "        out = tf.keras.layers.Flatten()(out)\n",
    "        out = self.collate(out)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11c98846",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSkipper = testSkipModel(50, (64,128,128),3)\n",
    "testSkipper.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSkipper.fit(train_img_small, train_lbl_small, validation_data=(val_img_small, val_lbl_small), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcba6b2",
   "metadata": {},
   "source": [
    "<b>Third pass</b>: At this point I started experiments with the \"hand geometry\" output of the MediaPipe detector, which places its 21 landmarks in space. Curiously, the detector has a bit of trouble with my working dataset, only detecting a hand in about 80% of it. I do know that the detector is sensitive to colour: swapping blue/red channels will lead to non-detection. Likewise greyscale is a problem. These are not the conditions it was trained for, apparently.\n",
    "\n",
    "Where the hand landmark data is available, it's enough alone for better results than the short CNNs I tried before. (The landmark data was previously normalised in position, orientation, and chirality.) The best score I got was 0.867 val_acc (0.94 train), with three dense layers of 256/256/256 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67ffa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_geom = np.load(\"datasets/train_geom.npy\")\n",
    "train_lbl = np.load(\"datasets/train_geom_lbl.npy\")\n",
    "val_geom = np.load(\"datasets/val_geom.npy\")\n",
    "val_lbl = np.load(\"datasets/val_geom_lbl.npy\")\n",
    "train_geom = train_geom.reshape((train_geom.shape[0],63))\n",
    "val_geom = val_geom.reshape((val_geom.shape[0], 63))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "800e6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_prob = 0.2\n",
    "reg_coef = 0.0001\n",
    "regulator = tf.keras.regularizers.L2(reg_coef)\n",
    "rng = np.random.default_rng()\n",
    "layers = 4\n",
    "seeds = [rng.integers(0,1024) for j in range(layers)]\n",
    "inits = [tf.keras.initializers.Orthogonal(seeds[j]) for j in range(layers)]\n",
    "\n",
    "geomModel = tf.keras.models.Sequential([#tf.keras.layers.Flatten(),\n",
    "                                       tf.keras.layers.Dense(256, activation='tanh', \n",
    "                                                             #kernel_initializer = inits[0],\n",
    "                                                             kernel_regularizer=regulator),\n",
    "                                        #tf.keras.layers.Dropout(dropout_prob),\n",
    "                                       tf.keras.layers.Dense(256, activation='tanh', \n",
    "                                                             #kernel_initializer = inits[1],\n",
    "                                                             kernel_regularizer=regulator),\n",
    "                                        #tf.keras.layers.Dropout(dropout_prob),\n",
    "                                       tf.keras.layers.Dense(256, activation='tanh', \n",
    "                                                             #kernel_initializer = inits[2],\n",
    "                                                             kernel_regularizer=regulator),\n",
    "                                        #tf.keras.layers.Dropout(0.5),\n",
    "                                       tf.keras.layers.Dense(50, activation='softmax', \n",
    "                                                             #kernel_initializer = inits[3],\n",
    "                                                             kernel_regularizer=regulator)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a0c78c",
   "metadata": {},
   "source": [
    "I looked into tensorflow's options for weight initialisation. Almost all of them are random initialisers, with various distributions (uniform or normal) and variances (people have looked at different normalisations in the quest to make training networks more tractable). The exception is the orthogonal initialiser, which essentially generates a random matrix like the others and then performs Gram-Schmidt/singular value decomposition on it to give an orthogonal matrix of weights.\n",
    "\n",
    "In terms of val_acc achieved, orthogonal initialisation did not yield improvement. It did yield a puzzle: although its accuracy scores are very close to the ordinary random initialisers given like amounts of training time, the reported cross-entropy loss was much higher, by a factor of tens of thousands. (In principle there is no upper-limit to the cross-entropy, the model simply needs to give high enough confidence to a particular wrong answer.) Curious, I tried letting it run for a long time, hundreds of epochs (with the network small enough that this was a matter of minutes rather than days). The cross-entropy does eventually come down, but the accuracy does nothing special. This sort of behaviour makes me think there must be interesting things to say about (for lack of a better expression) the dynamics of NN learning, but I don't know what they might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01f72c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    learning_rate,\n",
    "                    decay_steps=20000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "\n",
    "geomModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cdbf059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "699/699 [==============================] - 3s 5ms/step - loss: 51351.5000 - accuracy: 0.8723 - val_loss: 51276.8594 - val_accuracy: 0.7334\n",
      "Epoch 2/10\n",
      "699/699 [==============================] - 3s 5ms/step - loss: 51201.8164 - accuracy: 0.8730 - val_loss: 51127.4414 - val_accuracy: 0.7340\n",
      "Epoch 3/10\n",
      "699/699 [==============================] - 3s 4ms/step - loss: 51052.5977 - accuracy: 0.8752 - val_loss: 50978.5000 - val_accuracy: 0.7325\n",
      "Epoch 4/10\n",
      "699/699 [==============================] - 3s 4ms/step - loss: 50903.8633 - accuracy: 0.8795 - val_loss: 50829.9180 - val_accuracy: 0.7329\n",
      "Epoch 5/10\n",
      "699/699 [==============================] - 3s 4ms/step - loss: 50755.3398 - accuracy: 0.8802 - val_loss: 50681.5039 - val_accuracy: 0.7358\n",
      "Epoch 6/10\n",
      "699/699 [==============================] - 3s 4ms/step - loss: 50607.0312 - accuracy: 0.8841 - val_loss: 50533.2617 - val_accuracy: 0.7397\n",
      "Epoch 7/10\n",
      "699/699 [==============================] - 3s 5ms/step - loss: 50458.9336 - accuracy: 0.8838 - val_loss: 50385.3516 - val_accuracy: 0.7399\n",
      "Epoch 8/10\n",
      "699/699 [==============================] - 3s 5ms/step - loss: 50311.2852 - accuracy: 0.8846 - val_loss: 50237.9062 - val_accuracy: 0.7375\n",
      "Epoch 9/10\n",
      "699/699 [==============================] - 3s 4ms/step - loss: 50164.9766 - accuracy: 0.8860 - val_loss: 50096.0117 - val_accuracy: 0.7365\n",
      "Epoch 10/10\n",
      "699/699 [==============================] - 3s 4ms/step - loss: 50028.9102 - accuracy: 0.8872 - val_loss: 49962.5898 - val_accuracy: 0.7384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea6e230340>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geomModel.fit(train_geom, train_lbl, validation_data=(val_geom, val_lbl), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50641f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "for j in range(0, epochs):\n",
    "    geomModel.fit(train_geom, train_lbl, epochs=1)\n",
    "    geomModel.evaluate(val_geom, val_lbl, verbose=2)\n",
    "# 200 epochs later... \"you haven't converged or blown up yet? another round! Adam, what a dogged searcher.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e2800",
   "metadata": {},
   "source": [
    "<b>Fourth pass</b>: models combining image and geometric data. I'm looking at an attention-type mechanism where a short network uses the geometry to make weights for the convolutional network. Since the geometric data isn't there for every frame it also tries to train a 'back-up' layer just from the image data. It works better than previous tries. There's still a lot I don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic datasets: train and val at 64x64\n",
    "train_img_small = np.load('datasets/train_img_64.npy')\n",
    "train_img_small = np.float32(train_img_small) / 255\n",
    "train_geom = np.concatenate([np.load(\"datasets/train_geom_img.npy\"), np.load(\"datasets/train_geom_wrl.npy\")], axis=1)\n",
    "train_geom = train_geom.reshape((-1, 21*6))\n",
    "train_lbl = np.load('datasets/train_lbl.npy')\n",
    "print(train_img_small.shape, train_geom.shape, train_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9b9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5793, 64, 64, 3) (5793, 126) (5793,)\n"
     ]
    }
   ],
   "source": [
    "val_img_small = np.load('datasets/val_img_64.npy')\n",
    "val_img_small = np.float32(val_img_small) / 255\n",
    "val_geom = np.concatenate([np.load(\"datasets/val_geom_img.npy\"), np.load(\"datasets/val_geom_wrl.npy\")], axis=1)\n",
    "val_geom = val_geom.reshape((-1,21*6))\n",
    "val_lbl = np.load('datasets/val_lbl.npy')\n",
    "print(val_img_small.shape, val_geom.shape, val_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c8646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53118, 64, 64, 3) (53118, 126) (53118,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic data pre-generated by albumentations\n",
    "train_img_synth = np.concatenate([np.load('datasets/train_img_64_synth.npy'), \n",
    "                                  np.load('datasets/train_img_64.npy')], axis=0)\n",
    "train_lbl_synth = np.concatenate([np.load('datasets/train_lbl_64_synth.npy'), \n",
    "                                  np.load('datasets/train_lbl.npy')], axis=0)\n",
    "train_geom_synth = np.concatenate([np.concatenate([np.load('datasets/train_geom_img_64_synth.npy'), \n",
    "                                                   np.load('datasets/train_geom_img.npy')], axis=1),\n",
    "                                   np.concatenate([np.load('datasets/train_geom_wrl_64_synth.npy'),\n",
    "                                                   np.load('datasets/train_geom_wrl.npy')], axis=1)],\n",
    "                                   axis=0)\n",
    "train_img_synth = np.float32(train_img_synth) / 255\n",
    "train_geom_synth = train_geom_synth.reshape((-1, 21*6))\n",
    "print(train_img_synth.shape, train_geom_synth.shape, train_lbl_synth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95825a1a",
   "metadata": {},
   "source": [
    "Tensorboard is a profiling add-on. it can tell you lots of things about the statistics of your model's weights,\n",
    "how much time it takes doing what operations, and a lot more. I've barely taken a look.\n",
    "\n",
    "https://www.tensorflow.org/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f2ee4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5b65e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch = '2000,2010')\n",
    "# the data it logs can take up a lot of space, so they recommend using it only for 10 or 20 steps to gather its statistics,\n",
    "# and not steps at the beginning, where there can be overhead etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2a8e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation stuff...\n",
    "# tf.keras.layers.GaussianNoise(noise_sigma) (last used sigma = 0.04)\n",
    "# tf.keras.regularizers.L2(reg_coef) (last used coef 0.001 or 0.0001?)\n",
    "# tf.keras.layers.Dropout(dropout_prob) (last used prob = 0.2)\n",
    "# tf.keras.layers.SpatialDropout2D(spatial_dropout_prob) (last used prob = 0.05)\n",
    "\n",
    "# Augmentation stuff -- when using gpu it's advised to stick this on the dataset; as long as the preprocessing\n",
    "# consists only of tensorflow Graph-able operations it'll be executed in parallel when data is about to be called from it\n",
    "\n",
    "# train_img_tf = tf.data.Dataset.from_tensor_slices(train_img_small)\n",
    "# train_img_tf.map(lambda x: pre_process(x)), where pre_process could be a keras.Sequential object\n",
    "\n",
    "# tf.keras.layers.RandomBrightness(factor, value_range=(0, 1)) (factor = pair of floats in [-1,1])\n",
    "# tf.keras.layers.RandomContrast(factor in [0,1])\n",
    "# tf.keras.layers.RandomFlip(mode='horizontal')\n",
    "# tf.keras.layers.RandomRotation(fill_mode='constant', factor in [0,1]), rotation up to angle factor*2pi\n",
    "# tf.keras.layers.RandomZoom(height_factor=0.2, fill_mode='constant')  default arg width_factor=None preserves aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "c3c44fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(seq):\n",
    "    if tf.random.categorical(tf.math.log([[0.5, 0.5]]), 1).numpy()[0][0]:\n",
    "        return tf.raw_ops.Reverse(tensor=seq, dims=[False,False,False,True,False])\n",
    "    return seq\n",
    "\n",
    "def random_augment(seq):\n",
    "    seq = random_flip(seq)\n",
    "    seq = tf.image.random_brightness(seq, 0.15)\n",
    "    seq = tf.image.random_saturation(seq, 0.85, 1.15)\n",
    "    seq = tf.image.random_contrast(seq, 0.85, 1.15)\n",
    "    #seq = tf.image.random_hue(seq, 0.01)\n",
    "    return tf.raw_ops.ClipByValue(t=seq, clip_value_min=0, clip_value_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17393ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesiser_train = tf.keras.Sequential([tf.keras.layers.RandomBrightness(0.2, value_range=(0,1)),\n",
    "                                    tf.keras.layers.RandomFlip(mode = 'horizontal'),\n",
    "                                    tf.keras.layers.RandomRotation(0.05, fill_mode='constant'),\n",
    "                                    tf.keras.layers.RandomZoom(height_factor=0.2, fill_mode='constant')])\n",
    "batch_size = train_img_synth.shape[0]\n",
    "train_img_tf = tf.data.Dataset.from_tensor_slices(train_img_synth)\n",
    "train_geom_tf = tf.data.Dataset.from_tensor_slices(train_geom_synth).batch(batch_size).get_single_element()\n",
    "\n",
    "\n",
    "# this makes a dataset object with an attached function, rather than just applying a function once to its tensors\n",
    "train_synth = train_img_tf.map(lambda x: synthesiser_train(x),\n",
    "                                 num_parallel_calls=batch_size).batch(batch_size)\n",
    "train_proc = train_synth.get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0da06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testAttentionModel(tf.keras.Model):\n",
    "    def __init__(self, conv_filters, reg_coef=0, labels=50, use_geom_backup=True):\n",
    "        super(testAttentionModel, self).__init__()\n",
    "        filters_1, filters_2, filters_3 = conv_filters\n",
    "        conv_out_size = filters_3\n",
    "        self.reg = tf.keras.regularizers.L2(reg_coef)\n",
    "        self.spatial_dropout_prob = 0.02\n",
    "        self.dropout_prob = 0.1\n",
    "        self.use_geom_backup = use_geom_backup\n",
    "        \n",
    "        \n",
    "        # 64x64xch\n",
    "        self.conv_1a = tf.keras.layers.Convolution2D(filters_1, 5, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        self.conv_1b = tf.keras.layers.Convolution2D(filters_1, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        # 32x32xch\n",
    "        self.conv_2a = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        self.conv_2b = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True,\n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        # 16x16xch\n",
    "        self.conv_3a = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                    kernel_regularizer=self.reg)\n",
    "        self.conv_3b = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        # 8x8xch\n",
    "        #self.conv_4a = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "                                                    # activation='tanh', kernel_regularizer=regulator)\n",
    "        #self.conv_4b = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "                                                     #activation='tanh', kernel_regularizer=regulator)\n",
    "        # out: 4x4xch\n",
    "        \n",
    "        self.geom1 = tf.keras.layers.Dense(64, use_bias=True, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom_backup = tf.keras.layers.Dense(64, use_bias=True, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom2 = tf.keras.layers.Dense(64, use_bias=True, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.attention = tf.keras.layers.Dense(conv_out_size, use_bias=True, \n",
    "                                               activation='softmax', kernel_regularizer=self.reg)\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(labels, activation='softmax')\n",
    "    \n",
    "    def call(self, input_list, training=True):\n",
    "        c_out = tf.keras.layers.GaussianNoise(0.03)(input_list[0], #start 64x64x3\n",
    "                                                    training=training)\n",
    "        c_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_1a(c_out),\n",
    "                                                                            training=training)\n",
    "        c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_1b(c_out)) # to 32x32xch\n",
    "        c_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_2a(c_out),\n",
    "                                                                            training=training)\n",
    "        c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_2b(c_out))\n",
    "        c_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob) (self.conv_3a(c_out),\n",
    "                                                                            training=training)\n",
    "        c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_3b(c_out)) # to 16x16xch\n",
    "        #c_out = self.conv_4a(c_out)\n",
    "        #c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_4b(c_out))\n",
    "       \n",
    "        if tf.math.reduce_max(input_list[1]) == 0 and self.use_geom_backup:\n",
    "            g_out = self.geom_backup(tf.keras.layers.Flatten()(tf.keras.layers.AveragePooling2D(pool_size=(4, 4),\n",
    "                                                                                                strides=(4,4),\n",
    "                                                                                                padding='valid')(input_list[0])))\n",
    "        else:\n",
    "            g_out = self.geom1(input_list[1]) # if use_geom_backup is off this will output max(0, bias)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(g_out, training=training)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(self.geom2(g_out),training=training)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(self.attention(g_out),training=training)\n",
    "        g_out = tf.expand_dims(tf.expand_dims(g_out, axis=-2), axis=-2)\n",
    "       \n",
    "        return self.classifier(tf.keras.layers.Flatten()(tf.math.multiply(c_out, g_out)))\n",
    "    \n",
    "    #def build_graph(self):\n",
    "    #    in1 = tf.keras.layers.Input(shape=(64,64,3))\n",
    "    #    in2 = tf.keras.layers.Input(shape=(63))\n",
    "    #    return tf.keras.Model(inputs=[in1,in2], \n",
    "    #                          outputs=self.call([in1,in2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc2ac568",
   "metadata": {},
   "outputs": [],
   "source": [
    "testAttender = testAttentionModel((64,128,256), reg_coef=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "bcf93f5d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_34 (InputLayer)          [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " gaussian_noise_7 (GaussianNois  (None, 64, 64, 3)   0           ['input_34[0][0]']               \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 64, 64, 64)   4864        ['gaussian_noise_7[0][0]']       \n",
      "                                                                                                  \n",
      " spatial_dropout2d_11 (SpatialD  (None, 64, 64, 64)  0           ['conv2d_80[1][0]']              \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 64, 64, 64)   36928       ['spatial_dropout2d_11[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 64)  0           ['conv2d_81[1][0]']              \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 32, 32, 128)  73856       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " spatial_dropout2d_12 (SpatialD  (None, 32, 32, 128)  0          ['conv2d_82[1][0]']              \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 32, 32, 128)  147584      ['spatial_dropout2d_12[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 128)  0          ['conv2d_83[1][0]']              \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 16, 16, 256)  295168      ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " input_35 (InputLayer)          [(None, 63)]         0           []                               \n",
      "                                                                                                  \n",
      " spatial_dropout2d_13 (SpatialD  (None, 16, 16, 256)  0          ['conv2d_84[1][0]']              \n",
      " ropout2D)                                                                                        \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 64)           4096        ['input_35[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 16, 16, 256)  590080      ['spatial_dropout2d_13[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_47[1][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 8, 8, 256)   0           ['conv2d_85[1][0]']              \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 16384)        1064960     ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 16384)        0           ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 16384)        0           ['dense_49[1][0]']               \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 16384)        0           ['flatten_2[0][0]',              \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 50)           819250      ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,036,786\n",
      "Trainable params: 3,036,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using the 'functional API' of Keras \n",
    "testAttenderX.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d670176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get previously saved weights, make a model, start and stop fit, then call this to load\n",
    "testAttender.load_weights(\"testAttender.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfdc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    learning_rate,\n",
    "                    decay_steps=5000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "\n",
    "testAttender.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3b8e2",
   "metadata": {},
   "source": [
    "Results: 0.790 (0.964) with 3 blocks 64/128/256. Reg at 0.001 didn't help, 0.77 (0.966). (At this point cut half of the dataset grass.) Added spatial/regular dropout at 0.04/0.2, reg=0.001. Slower, val stalled around .745 (tr continued up to .93). adding in synth data, tr_acc (on the same model) went down to .745 too. but though it recovered val did not.\n",
    "\n",
    "With everything on (noise, dropouts, reg, pre-built synth, train-time synth), up to .82 (.96)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      " 667/1660 [===========>..................] - ETA: 12:02 - loss: 3.7232 - accuracy: 0.0519"
     ]
    }
   ],
   "source": [
    "testAttender.fit([train_proc, train_geom_tf], train_lbl_synth,\n",
    "                 validation_data=([val_img_small, val_geom], val_lbl),\n",
    "                 epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83e67fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have not yet been able to save and load a model in any other form than this; the tf native options require\n",
    "# some syntax knowledge I do not possess\n",
    "testAttender.save_weights(\"testAttender.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c90f98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why two copies of the same object? because tensorflow handles batch size in a way I don't understand, and\n",
    "# using the same one in two places raises errors\n",
    "synthesiser_val = tf.keras.Sequential([tf.keras.layers.RandomBrightness(0.2, value_range=(0,1)),\n",
    "                                    tf.keras.layers.RandomFlip(mode = 'horizontal'),\n",
    "                                    tf.keras.layers.RandomRotation(0.05, fill_mode='constant'),\n",
    "                                    tf.keras.layers.RandomZoom(height_factor=0.2, fill_mode='constant')])\n",
    "batch_val = val_img_small.shape[0]\n",
    "val_img_tf = tf.data.Dataset.from_tensor_slices(val_img_small)\n",
    "val_geom_tf = tf.data.Dataset.from_tensor_slices(val_geom_full).batch(batch_val).get_single_element()\n",
    "val_lbl_tf = tf.data.Dataset.from_tensor_slices(val_lbl_small).batch(batch_val).get_single_element()\n",
    "\n",
    "val_synth_ds = val_img_tf.map(lambda x: synthesiser_val(x),\n",
    "                             num_parallel_calls=batch_val).batch(batch_val)\n",
    "val_proc = val_synth_ds.get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9401c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model tries out test-time data augmentation; that is, given an image it generates some random synthetic frames \n",
    "# from it, gives those to the underlying trained model, and returns their averaged probabilities.\n",
    "\n",
    "# however, I was never able to get it even to run. the basic difficulty is: (1) tf tensors are usually immutable, so can't act\n",
    "# as accumulators; there is a tf.Variable object, but (2) tf does not allow the creation of tf.Variables within a function\n",
    "# that gets called more than once, while (3) a variable created at object initialisation doesn't know what shape it's supposed\n",
    "# to be, and whatever I try to tell it later it tells me I'm wrong.\n",
    "\n",
    "# After all that, I think the correct way to do this is (1) the variable must be passed to the model at execution time,\n",
    "# while (2) making a custom training loop procedure that makes explicit various stuff that tf/keras does automatically for \n",
    "# other types of models. I haven't tried yet.\n",
    "\n",
    "class testPollModel(tf.keras.Model):\n",
    "    def __init__(self, polled_model, size):\n",
    "        super(testPollModel, self).__init__()\n",
    "        self.size = size\n",
    "        self.polled_model = polled_model\n",
    "        \n",
    "        #self.vote = tf.Variable(tf.zeros_initializer(), shape=tf.TensorShape([None,50]))\n",
    "        self.vote = None\n",
    "        \n",
    "    def call(self, input_list):\n",
    "        #self.vote = self.vote*0\n",
    "        #self.vote.set_shape(tf.TensorShape((None,50)))\n",
    "        #self.vote.assign_add(-vote)\n",
    "        #if self.vote is None:\n",
    "        #    self.vote = tf.Variable(tf.zeros(shape=(1,50), dtype='float32'), trainable=False)\n",
    "            #self.vote = self.initialiser(shape=[-1,50])\n",
    "        #self.vote.set_shape(tf.TensorShape((None,50)))\n",
    "        #self.vote.assign(tf.transpose(self.polled_model(input_list=input_list, training=False))[0])\n",
    "        #tf.squeeze() ? \n",
    "        #self.vote.assign(self.polled_model(input_list=input_list,\n",
    "        #                                   training=False))\n",
    "        ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "      \n",
    "        for j in range(self.size):\n",
    "            #self.vote.assign_add\n",
    "            ta = ta.write(j, self.polled_model([synthesiser_train(input_list[0]), input_list[1]], training = False))\n",
    "            #self.vote.assign_add(self.polled_model([synthesiser_train(input_list[0]), input_list[1]], \n",
    "            #                                      training = False))\n",
    "        #ret = self.vote / self.size\n",
    "        #self.vote.assign_add(-self.vote)\n",
    "        print(ta.size)\n",
    "        return tf.keras.layers.Average()(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "394f8001",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name='random_brightness_6_input'), name='random_brightness_6_input', description=\"created by layer 'random_brightness_6_input'\"), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(64, 64, 3), dtype=tf.float32, name='random_brightness_6_input'), name='random_brightness_6_input', description=\"created by layer 'random_brightness_6_input'\"), but it was called on an input with incompatible shape (None, 64, 64, 3).\n",
      "<bound method TensorArray.size of <tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001F77C3F5AF0>>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\balin\\AppData\\Local\\Temp\\__autograph_generated_filemal1hvsh.py\", line 28, in tf__call\n        retval_ = ag__.converted_call(ag__.converted_call(ag__.ld(tf).keras.layers.Average, (), None, fscope), (ag__.ld(ta),), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"test_poll_model_42\" \"                 f\"(type testPollModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\balin\\AppData\\Local\\Temp\\ipykernel_7156\\1035348579.py\", line 34, in call  *\n            return tf.keras.layers.Average()(ta)\n        File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\layers\\merging\\base_merge.py\", line 84, in build\n            if not isinstance(input_shape[0], tuple):\n    \n        TypeError: 'NoneType' object is not subscriptable\n    \n    \n    Call arguments received by layer \"test_poll_model_42\" \"                 f\"(type testPollModel):\n       input_list=('tf.Tensor(shape=(None, 64, 64, 3), dtype=float32)', 'tf.Tensor(shape=(None, 63), dtype=float32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [245]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m testPoller \u001b[38;5;241m=\u001b[39m testPollModel(testAttender,j)\n\u001b[0;32m      3\u001b[0m testPoller\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtestPoller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_img_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_geom_full\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_lbl_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileiirr9vog.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemal1hvsh.py:28\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mAverage, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), (ag__\u001b[38;5;241m.\u001b[39mld(ta),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\balin\\AppData\\Local\\Temp\\__autograph_generated_filemal1hvsh.py\", line 28, in tf__call\n        retval_ = ag__.converted_call(ag__.converted_call(ag__.ld(tf).keras.layers.Average, (), None, fscope), (ag__.ld(ta),), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"test_poll_model_42\" \"                 f\"(type testPollModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\balin\\AppData\\Local\\Temp\\ipykernel_7156\\1035348579.py\", line 34, in call  *\n            return tf.keras.layers.Average()(ta)\n        File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\balin\\anaconda3\\lib\\site-packages\\keras\\layers\\merging\\base_merge.py\", line 84, in build\n            if not isinstance(input_shape[0], tuple):\n    \n        TypeError: 'NoneType' object is not subscriptable\n    \n    \n    Call arguments received by layer \"test_poll_model_42\" \"                 f\"(type testPollModel):\n       input_list=('tf.Tensor(shape=(None, 64, 64, 3), dtype=float32)', 'tf.Tensor(shape=(None, 63), dtype=float32)')\n"
     ]
    }
   ],
   "source": [
    "# never yet worked\n",
    "for j in range(2, 5):\n",
    "    testPoller = testPollModel(testAttender,j)\n",
    "    testPoller.compile()\n",
    "    testPoller.evaluate([val_img_small, val_geom_full], val_lbl_small, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9ddf0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras functional API style attempt at the same thing. this also did not work.\n",
    "input_img = tf.keras.Input(shape=(64,64,3))\n",
    "input_geom = tf.keras.Input(shape=(63,))\n",
    "y1 = testAttender([input_img, input_geom], training=False)\n",
    "y2 = testAttender([input_img, input_geom], training=False)\n",
    "y3 = testAttender([input_img, input_geom], training=False)\n",
    "outputs = tf.keras.layers.Average()([y1, y2, y3])\n",
    "ensemble_model = tf.keras.Model(inputs=[input_img, input_geom], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84f67fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nor could I get it to work on even a single example, because tf objects to calling testAttender on a single image rather\n",
    "# than a batch.\n",
    "vote = tf.Variable(np.zeros((1, 50), dtype='float32'), trainable=False)\n",
    "vote.set_shape(tf.TensorShape([None,50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a34d3",
   "metadata": {},
   "source": [
    "<b>Fifth pass</b>: stick in one of the pre-trained models carefully made by various google teams and see how it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae4dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd07aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first problem: these models have much larger input sizes, like 224x224. just generating our data takes a lot more available\n",
    "# memory. (I think there are ways to get python to do explicit file-swapping to cache part of the variable, but I don't\n",
    "# know how they work.) trying to put it in memory as float32, which is the standard the pre-trained image models use, is\n",
    "# four times as big a problem. the answer here is to keep it in memory as uint8 and do the rescaling dynamically; this probably\n",
    "# entails a slight performance hit, since I'm running everything on CPU, but it's negligable compared to the work it does\n",
    "# computing all those convolutional filters\n",
    "train_img_224 = np.load('datasets/train_img_224.npy')\n",
    "train_geom = np.concatenate([np.load(\"datasets/train_geom_img.npy\"), np.load(\"datasets/train_geom_wrl.npy\")], axis=1)\n",
    "train_geom = train_geom.reshape((-1, 21*6))\n",
    "train_lbl = np.load('datasets/train_lbl.npy')\n",
    "print(train_img_224.shape, train_geom.shape, train_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d735295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_224 = np.load('datasets/val_img_224.npy')\n",
    "val_geom = np.concatenate([np.load(\"datasets/val_geom_img.npy\"), np.load(\"datasets/val_geom_wrl.npy\")], axis=1)\n",
    "val_geom = val_geom.reshape((-1,21*6))\n",
    "val_lbl = np.load('datasets/val_lbl.npy')\n",
    "print(val_img_224.shape, val_geom.shape, val_lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae0f17",
   "metadata": {},
   "source": [
    "The first try was one of google's 'lightweight' models meant to work on devices with little computing power (like phones, I think). \n",
    "\n",
    "The model's page at tfhub, https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5, explains how its input\n",
    "and output work: \"The output is a batch of feature vectors. For each input image, the [output] feature vector has size num_features = 1280.... The input images are expected to have color values in the range [0,1], following the common image input conventions. For this model, the size of the input images is fixed to height x width = 224 x 224 pixels.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b036cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "830/830 [==============================] - 430s 512ms/step - loss: 2.8880 - accuracy: 0.3210 - val_loss: 2.2837 - val_accuracy: 0.4404\n",
      "Epoch 2/80\n",
      "830/830 [==============================] - 416s 501ms/step - loss: 1.5954 - accuracy: 0.5870 - val_loss: 1.7062 - val_accuracy: 0.5560\n",
      "Epoch 3/80\n",
      "830/830 [==============================] - 418s 503ms/step - loss: 1.2079 - accuracy: 0.6745 - val_loss: 1.3988 - val_accuracy: 0.6183\n",
      "Epoch 4/80\n",
      "830/830 [==============================] - 418s 504ms/step - loss: 0.9684 - accuracy: 0.7307 - val_loss: 1.1964 - val_accuracy: 0.6713\n",
      "Epoch 5/80\n",
      "830/830 [==============================] - 419s 505ms/step - loss: 0.8062 - accuracy: 0.7740 - val_loss: 1.0649 - val_accuracy: 0.7060\n",
      "Epoch 6/80\n",
      "830/830 [==============================] - 414s 499ms/step - loss: 0.6898 - accuracy: 0.8056 - val_loss: 0.9613 - val_accuracy: 0.7312\n",
      "Epoch 7/80\n",
      "830/830 [==============================] - 412s 496ms/step - loss: 0.6046 - accuracy: 0.8311 - val_loss: 0.8857 - val_accuracy: 0.7525\n",
      "Epoch 8/80\n",
      "830/830 [==============================] - 410s 494ms/step - loss: 0.5460 - accuracy: 0.8469 - val_loss: 0.8380 - val_accuracy: 0.7654\n",
      "Epoch 9/80\n",
      "830/830 [==============================] - 409s 493ms/step - loss: 0.4951 - accuracy: 0.8623 - val_loss: 0.8066 - val_accuracy: 0.7702\n",
      "Epoch 10/80\n",
      "830/830 [==============================] - 411s 495ms/step - loss: 0.4545 - accuracy: 0.8683 - val_loss: 0.7710 - val_accuracy: 0.7844\n",
      "Epoch 11/80\n",
      "830/830 [==============================] - 410s 494ms/step - loss: 0.4193 - accuracy: 0.8802 - val_loss: 0.7461 - val_accuracy: 0.7903\n",
      "Epoch 12/80\n",
      "830/830 [==============================] - 407s 490ms/step - loss: 0.3880 - accuracy: 0.8911 - val_loss: 0.7198 - val_accuracy: 0.7944\n",
      "Epoch 13/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.3634 - accuracy: 0.8967 - val_loss: 0.7152 - val_accuracy: 0.7975\n",
      "Epoch 14/80\n",
      "830/830 [==============================] - 404s 487ms/step - loss: 0.3381 - accuracy: 0.9042 - val_loss: 0.6871 - val_accuracy: 0.8067\n",
      "Epoch 15/80\n",
      "830/830 [==============================] - 402s 484ms/step - loss: 0.3208 - accuracy: 0.9074 - val_loss: 0.6748 - val_accuracy: 0.8129\n",
      "Epoch 16/80\n",
      "830/830 [==============================] - 401s 483ms/step - loss: 0.3034 - accuracy: 0.9126 - val_loss: 0.6657 - val_accuracy: 0.8106\n",
      "Epoch 17/80\n",
      "830/830 [==============================] - 397s 479ms/step - loss: 0.2870 - accuracy: 0.9188 - val_loss: 0.6462 - val_accuracy: 0.8165\n",
      "Epoch 18/80\n",
      "830/830 [==============================] - 396s 477ms/step - loss: 0.2713 - accuracy: 0.9210 - val_loss: 0.6500 - val_accuracy: 0.8148\n",
      "Epoch 19/80\n",
      "830/830 [==============================] - 396s 478ms/step - loss: 0.2601 - accuracy: 0.9242 - val_loss: 0.6368 - val_accuracy: 0.8168\n",
      "Epoch 20/80\n",
      "830/830 [==============================] - 395s 476ms/step - loss: 0.2484 - accuracy: 0.9283 - val_loss: 0.6492 - val_accuracy: 0.8167\n",
      "Epoch 21/80\n",
      "830/830 [==============================] - 395s 476ms/step - loss: 0.2346 - accuracy: 0.9332 - val_loss: 0.6361 - val_accuracy: 0.8200\n",
      "Epoch 22/80\n",
      "830/830 [==============================] - 391s 472ms/step - loss: 0.2288 - accuracy: 0.9329 - val_loss: 0.6414 - val_accuracy: 0.8201\n",
      "Epoch 23/80\n",
      "830/830 [==============================] - 393s 473ms/step - loss: 0.2195 - accuracy: 0.9372 - val_loss: 0.6349 - val_accuracy: 0.8205\n",
      "Epoch 24/80\n",
      "830/830 [==============================] - 393s 474ms/step - loss: 0.2055 - accuracy: 0.9405 - val_loss: 0.6377 - val_accuracy: 0.8208\n",
      "Epoch 25/80\n",
      "830/830 [==============================] - 393s 474ms/step - loss: 0.1964 - accuracy: 0.9438 - val_loss: 0.6290 - val_accuracy: 0.8231\n",
      "Epoch 26/80\n",
      "830/830 [==============================] - 393s 474ms/step - loss: 0.1876 - accuracy: 0.9463 - val_loss: 0.6264 - val_accuracy: 0.8262\n",
      "Epoch 27/80\n",
      "830/830 [==============================] - 394s 475ms/step - loss: 0.1834 - accuracy: 0.9468 - val_loss: 0.6316 - val_accuracy: 0.8251\n",
      "Epoch 28/80\n",
      "830/830 [==============================] - 398s 479ms/step - loss: 0.1795 - accuracy: 0.9483 - val_loss: 0.6352 - val_accuracy: 0.8269\n",
      "Epoch 29/80\n",
      "830/830 [==============================] - 397s 479ms/step - loss: 0.1718 - accuracy: 0.9498 - val_loss: 0.6303 - val_accuracy: 0.8288\n",
      "Epoch 30/80\n",
      "830/830 [==============================] - 396s 478ms/step - loss: 0.1644 - accuracy: 0.9513 - val_loss: 0.6271 - val_accuracy: 0.8289\n",
      "Epoch 31/80\n",
      "830/830 [==============================] - 397s 479ms/step - loss: 0.1605 - accuracy: 0.9527 - val_loss: 0.6177 - val_accuracy: 0.8332\n",
      "Epoch 32/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1584 - accuracy: 0.9551 - val_loss: 0.6268 - val_accuracy: 0.8324\n",
      "Epoch 33/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1481 - accuracy: 0.9584 - val_loss: 0.6412 - val_accuracy: 0.8300\n",
      "Epoch 34/80\n",
      "830/830 [==============================] - 399s 480ms/step - loss: 0.1474 - accuracy: 0.9564 - val_loss: 0.6428 - val_accuracy: 0.8355\n",
      "Epoch 35/80\n",
      "830/830 [==============================] - 399s 481ms/step - loss: 0.1440 - accuracy: 0.9575 - val_loss: 0.6302 - val_accuracy: 0.8324\n",
      "Epoch 36/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1387 - accuracy: 0.9585 - val_loss: 0.6356 - val_accuracy: 0.8367\n",
      "Epoch 37/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1367 - accuracy: 0.9599 - val_loss: 0.6390 - val_accuracy: 0.8336\n",
      "Epoch 38/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1302 - accuracy: 0.9620 - val_loss: 0.6513 - val_accuracy: 0.8329\n",
      "Epoch 39/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1247 - accuracy: 0.9634 - val_loss: 0.6484 - val_accuracy: 0.8351\n",
      "Epoch 40/80\n",
      "830/830 [==============================] - 399s 481ms/step - loss: 0.1249 - accuracy: 0.9622 - val_loss: 0.6502 - val_accuracy: 0.8353\n",
      "Epoch 41/80\n",
      "830/830 [==============================] - 399s 480ms/step - loss: 0.1227 - accuracy: 0.9626 - val_loss: 0.6394 - val_accuracy: 0.8357\n",
      "Epoch 42/80\n",
      "830/830 [==============================] - 398s 480ms/step - loss: 0.1215 - accuracy: 0.9655 - val_loss: 0.6442 - val_accuracy: 0.8374\n",
      "Epoch 43/80\n",
      "830/830 [==============================] - 399s 480ms/step - loss: 0.1148 - accuracy: 0.9666 - val_loss: 0.6558 - val_accuracy: 0.8355\n",
      "Epoch 44/80\n",
      "830/830 [==============================] - 399s 481ms/step - loss: 0.1126 - accuracy: 0.9671 - val_loss: 0.6475 - val_accuracy: 0.8351\n",
      "Epoch 45/80\n",
      "830/830 [==============================] - 399s 481ms/step - loss: 0.1130 - accuracy: 0.9665 - val_loss: 0.6593 - val_accuracy: 0.8364\n",
      "Epoch 46/80\n",
      "830/830 [==============================] - 399s 481ms/step - loss: 0.1091 - accuracy: 0.9676 - val_loss: 0.6495 - val_accuracy: 0.8376\n",
      "Epoch 47/80\n",
      "830/830 [==============================] - 400s 482ms/step - loss: 0.1040 - accuracy: 0.9699 - val_loss: 0.6713 - val_accuracy: 0.8346\n",
      "Epoch 48/80\n",
      "830/830 [==============================] - 400s 482ms/step - loss: 0.1023 - accuracy: 0.9704 - val_loss: 0.6519 - val_accuracy: 0.8389\n",
      "Epoch 49/80\n",
      "830/830 [==============================] - 401s 483ms/step - loss: 0.1032 - accuracy: 0.9690 - val_loss: 0.6580 - val_accuracy: 0.8370\n",
      "Epoch 50/80\n",
      "830/830 [==============================] - 400s 483ms/step - loss: 0.0991 - accuracy: 0.9710 - val_loss: 0.6658 - val_accuracy: 0.8360\n",
      "Epoch 51/80\n",
      "830/830 [==============================] - 403s 486ms/step - loss: 0.0959 - accuracy: 0.9726 - val_loss: 0.6615 - val_accuracy: 0.8389\n",
      "Epoch 52/80\n",
      "830/830 [==============================] - 402s 484ms/step - loss: 0.0969 - accuracy: 0.9725 - val_loss: 0.6717 - val_accuracy: 0.8355\n",
      "Epoch 53/80\n",
      "830/830 [==============================] - 402s 484ms/step - loss: 0.0960 - accuracy: 0.9716 - val_loss: 0.6708 - val_accuracy: 0.8395\n",
      "Epoch 54/80\n",
      "830/830 [==============================] - 402s 484ms/step - loss: 0.0927 - accuracy: 0.9720 - val_loss: 0.6738 - val_accuracy: 0.8384\n",
      "Epoch 55/80\n",
      "830/830 [==============================] - 403s 485ms/step - loss: 0.0895 - accuracy: 0.9739 - val_loss: 0.6751 - val_accuracy: 0.8379\n",
      "Epoch 56/80\n",
      "830/830 [==============================] - 403s 486ms/step - loss: 0.0883 - accuracy: 0.9735 - val_loss: 0.6743 - val_accuracy: 0.8386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/80\n",
      "830/830 [==============================] - 403s 485ms/step - loss: 0.0886 - accuracy: 0.9741 - val_loss: 0.6782 - val_accuracy: 0.8367\n",
      "Epoch 58/80\n",
      "830/830 [==============================] - 404s 487ms/step - loss: 0.0845 - accuracy: 0.9754 - val_loss: 0.6779 - val_accuracy: 0.8421\n",
      "Epoch 59/80\n",
      "830/830 [==============================] - 404s 486ms/step - loss: 0.0841 - accuracy: 0.9747 - val_loss: 0.6884 - val_accuracy: 0.8386\n",
      "Epoch 60/80\n",
      "830/830 [==============================] - 404s 487ms/step - loss: 0.0830 - accuracy: 0.9758 - val_loss: 0.6774 - val_accuracy: 0.8384\n",
      "Epoch 61/80\n",
      "830/830 [==============================] - 403s 486ms/step - loss: 0.0840 - accuracy: 0.9744 - val_loss: 0.6801 - val_accuracy: 0.8438\n",
      "Epoch 62/80\n",
      "830/830 [==============================] - 403s 486ms/step - loss: 0.0836 - accuracy: 0.9745 - val_loss: 0.6918 - val_accuracy: 0.8417\n",
      "Epoch 63/80\n",
      "830/830 [==============================] - 404s 487ms/step - loss: 0.0776 - accuracy: 0.9773 - val_loss: 0.6730 - val_accuracy: 0.8448\n",
      "Epoch 64/80\n",
      "830/830 [==============================] - 403s 486ms/step - loss: 0.0818 - accuracy: 0.9761 - val_loss: 0.6744 - val_accuracy: 0.8396\n",
      "Epoch 65/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0782 - accuracy: 0.9761 - val_loss: 0.6871 - val_accuracy: 0.8407\n",
      "Epoch 66/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0751 - accuracy: 0.9783 - val_loss: 0.6965 - val_accuracy: 0.8403\n",
      "Epoch 67/80\n",
      "830/830 [==============================] - 405s 488ms/step - loss: 0.0746 - accuracy: 0.9780 - val_loss: 0.6848 - val_accuracy: 0.8408\n",
      "Epoch 68/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0770 - accuracy: 0.9775 - val_loss: 0.6939 - val_accuracy: 0.8414\n",
      "Epoch 69/80\n",
      "830/830 [==============================] - 407s 490ms/step - loss: 0.0736 - accuracy: 0.9785 - val_loss: 0.6924 - val_accuracy: 0.8422\n",
      "Epoch 70/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0739 - accuracy: 0.9777 - val_loss: 0.6901 - val_accuracy: 0.8421\n",
      "Epoch 71/80\n",
      "830/830 [==============================] - 406s 490ms/step - loss: 0.0740 - accuracy: 0.9779 - val_loss: 0.6974 - val_accuracy: 0.8417\n",
      "Epoch 72/80\n",
      "830/830 [==============================] - 406s 490ms/step - loss: 0.0715 - accuracy: 0.9795 - val_loss: 0.6916 - val_accuracy: 0.8429\n",
      "Epoch 73/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0709 - accuracy: 0.9797 - val_loss: 0.7057 - val_accuracy: 0.8403\n",
      "Epoch 74/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0711 - accuracy: 0.9791 - val_loss: 0.6930 - val_accuracy: 0.8414\n",
      "Epoch 75/80\n",
      "830/830 [==============================] - 405s 488ms/step - loss: 0.0688 - accuracy: 0.9794 - val_loss: 0.6986 - val_accuracy: 0.8431\n",
      "Epoch 76/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0705 - accuracy: 0.9796 - val_loss: 0.7034 - val_accuracy: 0.8417\n",
      "Epoch 77/80\n",
      "830/830 [==============================] - 406s 489ms/step - loss: 0.0683 - accuracy: 0.9799 - val_loss: 0.6989 - val_accuracy: 0.8448\n",
      "Epoch 78/80\n",
      "830/830 [==============================] - 408s 491ms/step - loss: 0.0656 - accuracy: 0.9814 - val_loss: 0.7076 - val_accuracy: 0.8427\n",
      "Epoch 79/80\n",
      "830/830 [==============================] - 412s 497ms/step - loss: 0.0661 - accuracy: 0.9802 - val_loss: 0.7054 - val_accuracy: 0.8434\n",
      "Epoch 80/80\n",
      "830/830 [==============================] - 415s 500ms/step - loss: 0.0658 - accuracy: 0.9801 - val_loss: 0.7086 - val_accuracy: 0.8460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258e3d0c760>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the first try, an older version where the attention layer didn't take in the imported net's output.\n",
    "# we eventually got a small improvement, up to 0.84 val accuracy (compared to 0.82 from the best of my 'toy' models)\n",
    "testImport.fit([train_img_224, train_geom], train_lbl,\n",
    "                 validation_data=([val_img_224, val_geom], val_lbl),\n",
    "                 epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f25414",
   "metadata": {},
   "source": [
    "Next try is https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/5. This one has input size 299x299, \"but other input sizes are possible (within limits),\" and output size 1536. There's no chance I can store the images at that size but it turns out the 224x224 I now have is within limits. \n",
    "\n",
    "Since this one takes about 35m an epoch I had time to go back to google's <b>Colaboratory</b>, https://colab.research.google.com/, which it turns out is very easy to use. Colab takes only a couple lines to mount your google Drive, \n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "from which it can load files efficiently (compared to uploading them for an individual runtime session, which takes a long time). The root directory is \n",
    "\n",
    "    /content/drive/MyDrive/\n",
    "\n",
    "followed by whatever folder/file structure you put on Drive. After that, exactly the same code as here will run on a server far away. My first try there is https://tfhub.dev/google/imagenet/inception_v3/feature_vector/5, but it takes about 55m an epoch, so I'm waiting on results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c21041a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "830/830 [==============================] - 2170s 3s/step - loss: 2.9457 - accuracy: 0.2718 - val_loss: 2.5158 - val_accuracy: 0.3834\n",
      "Epoch 2/50\n",
      "830/830 [==============================] - 2199s 3s/step - loss: 1.8493 - accuracy: 0.5111 - val_loss: 1.9345 - val_accuracy: 0.4978\n",
      "Epoch 3/50\n",
      "830/830 [==============================] - 2227s 3s/step - loss: 1.4491 - accuracy: 0.6082 - val_loss: 1.5852 - val_accuracy: 0.5726\n",
      "Epoch 4/50\n",
      "830/830 [==============================] - 2183s 3s/step - loss: 1.2048 - accuracy: 0.6708 - val_loss: 1.3636 - val_accuracy: 0.6233\n",
      "Epoch 5/50\n",
      "830/830 [==============================] - 2174s 3s/step - loss: 1.0383 - accuracy: 0.7132 - val_loss: 1.2248 - val_accuracy: 0.6601\n",
      "Epoch 6/50\n",
      "830/830 [==============================] - 2159s 3s/step - loss: 0.9155 - accuracy: 0.7460 - val_loss: 1.1285 - val_accuracy: 0.6820\n",
      "Epoch 7/50\n",
      "830/830 [==============================] - 2170s 3s/step - loss: 0.8236 - accuracy: 0.7700 - val_loss: 1.0305 - val_accuracy: 0.7124\n",
      "Epoch 8/50\n",
      "830/830 [==============================] - 2156s 3s/step - loss: 0.7592 - accuracy: 0.7853 - val_loss: 0.9687 - val_accuracy: 0.7330\n",
      "Epoch 9/50\n",
      "830/830 [==============================] - 2319s 3s/step - loss: 0.7017 - accuracy: 0.8026 - val_loss: 0.9164 - val_accuracy: 0.7414\n",
      "Epoch 10/50\n",
      "830/830 [==============================] - 2302s 3s/step - loss: 0.6507 - accuracy: 0.8164 - val_loss: 0.8929 - val_accuracy: 0.7485\n",
      "Epoch 11/50\n",
      "830/830 [==============================] - 2512s 3s/step - loss: 0.6159 - accuracy: 0.8226 - val_loss: 0.8359 - val_accuracy: 0.7637\n",
      "Epoch 12/50\n",
      "830/830 [==============================] - 2535s 3s/step - loss: 0.5810 - accuracy: 0.8331 - val_loss: 0.8345 - val_accuracy: 0.7649\n",
      "Epoch 13/50\n",
      "830/830 [==============================] - 2293s 3s/step - loss: 0.5486 - accuracy: 0.8428 - val_loss: 0.7980 - val_accuracy: 0.7792\n",
      "Epoch 14/50\n",
      "830/830 [==============================] - 2522s 3s/step - loss: 0.5219 - accuracy: 0.8496 - val_loss: 0.7650 - val_accuracy: 0.7806\n",
      "Epoch 15/50\n",
      "830/830 [==============================] - 2175s 3s/step - loss: 0.5044 - accuracy: 0.8542 - val_loss: 0.7507 - val_accuracy: 0.7870\n",
      "Epoch 16/50\n",
      "830/830 [==============================] - 2156s 3s/step - loss: 0.4797 - accuracy: 0.8624 - val_loss: 0.7350 - val_accuracy: 0.7892\n",
      "Epoch 17/50\n",
      "830/830 [==============================] - 2158s 3s/step - loss: 0.4591 - accuracy: 0.8662 - val_loss: 0.7319 - val_accuracy: 0.7929\n",
      "Epoch 18/50\n",
      "830/830 [==============================] - 2156s 3s/step - loss: 0.4437 - accuracy: 0.8721 - val_loss: 0.7211 - val_accuracy: 0.7960\n",
      "Epoch 19/50\n",
      "572/830 [===================>..........] - ETA: 9:09 - loss: 0.4260 - accuracy: 0.8790"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtestImport2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_img_224\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_geom\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lbl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_img_224\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_geom\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_lbl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testImport.fit([train_img_224, train_geom], train_lbl,\n",
    "                 validation_data=([val_img_224, val_geom], val_lbl),\n",
    "                 epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f4906b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one thing I haven't incorporated yet is putting the CNN output as input to the attention mechanism, which is what people\n",
    "# usually do.\n",
    "\n",
    "class testImporter(tf.keras.Model):\n",
    "    def __init__(self, import_url=\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/5\",\n",
    "                       import_output = 1280,\n",
    "                       dropout_prob=0.5,\n",
    "                       reg_coef = 0.0001,\n",
    "                        use_geom_backup=True):\n",
    "        super(testImporter2, self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.reg = tf.keras.regularizers.L2(reg_coef)\n",
    "        \n",
    "        self.pretrained = hub.KerasLayer(import_url, trainable=False)\n",
    "        self.geom1 = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom_backup = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom2 = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom3 = tf.keras.layers.Dense(import_output,activation='softmax', kernel_regularizer=self.reg)\n",
    "        self.classifier = tf.keras.layers.Dense(50, activation='softmax')\n",
    "        \n",
    "        self.flattener = tf.keras.layers.Flatten()\n",
    "        self.avgpooler = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(4,4), padding='valid')\n",
    "    \n",
    "    def call(self, input_list):\n",
    "        c_out = self.pretrained(tf.image.convert_image_dtype(input_list[0], dtype=tf.float32))\n",
    "        if tf.math.reduce_max(input_list[1]) == 0 and use_geom_backup:\n",
    "            g_out = self.geom_backup(self.flattener(self.avgpooler(self.scaler(input_list[0]))))\n",
    "        else:\n",
    "            g_out = self.geom1(input_list[1])\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(g_out)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(self.geom2(g_out))\n",
    "        g_out = tf.concat([g_out, c_out], axis=-1)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(self.geom3(g_out))\n",
    "       \n",
    "        return self.classifier(tf.math.multiply(c_out, g_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d074803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testImport = testImporter()\n",
    "testImport.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31cfafea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "830/830 [==============================] - 440s 523ms/step - loss: 2.5875 - accuracy: 0.3464 - val_loss: 1.9366 - val_accuracy: 0.5417\n",
      "Epoch 2/50\n",
      "830/830 [==============================] - 459s 553ms/step - loss: 1.5393 - accuracy: 0.5896 - val_loss: 1.4894 - val_accuracy: 0.6318\n",
      "Epoch 3/50\n",
      "830/830 [==============================] - 458s 552ms/step - loss: 1.1921 - accuracy: 0.6814 - val_loss: 1.2532 - val_accuracy: 0.6879\n",
      "Epoch 4/50\n",
      "830/830 [==============================] - 458s 552ms/step - loss: 0.9898 - accuracy: 0.7442 - val_loss: 1.1379 - val_accuracy: 0.7124\n",
      "Epoch 5/50\n",
      "830/830 [==============================] - 459s 553ms/step - loss: 0.8504 - accuracy: 0.7808 - val_loss: 1.0433 - val_accuracy: 0.7409\n",
      "Epoch 6/50\n",
      "830/830 [==============================] - 459s 553ms/step - loss: 0.7439 - accuracy: 0.8174 - val_loss: 0.9713 - val_accuracy: 0.7538\n",
      "Epoch 7/50\n",
      "830/830 [==============================] - 458s 552ms/step - loss: 0.6660 - accuracy: 0.8414 - val_loss: 0.9319 - val_accuracy: 0.7673\n",
      "Epoch 8/50\n",
      "830/830 [==============================] - 460s 554ms/step - loss: 0.6002 - accuracy: 0.8614 - val_loss: 0.8810 - val_accuracy: 0.7811\n",
      "Epoch 9/50\n",
      "830/830 [==============================] - 451s 544ms/step - loss: 0.5478 - accuracy: 0.8794 - val_loss: 0.8349 - val_accuracy: 0.7925\n",
      "Epoch 10/50\n",
      "830/830 [==============================] - 458s 553ms/step - loss: 0.5061 - accuracy: 0.8925 - val_loss: 0.8209 - val_accuracy: 0.7982\n",
      "Epoch 11/50\n",
      "830/830 [==============================] - 459s 553ms/step - loss: 0.4684 - accuracy: 0.9056 - val_loss: 0.7682 - val_accuracy: 0.8082\n",
      "Epoch 12/50\n",
      "830/830 [==============================] - 459s 553ms/step - loss: 0.4385 - accuracy: 0.9132 - val_loss: 0.7639 - val_accuracy: 0.8113\n",
      "Epoch 13/50\n",
      "830/830 [==============================] - 484s 583ms/step - loss: 0.4043 - accuracy: 0.9250 - val_loss: 0.7571 - val_accuracy: 0.8143\n",
      "Epoch 14/50\n",
      "830/830 [==============================] - 508s 613ms/step - loss: 0.3835 - accuracy: 0.9311 - val_loss: 0.7520 - val_accuracy: 0.8181\n",
      "Epoch 15/50\n",
      "830/830 [==============================] - 452s 544ms/step - loss: 0.3626 - accuracy: 0.9389 - val_loss: 0.7238 - val_accuracy: 0.8272\n",
      "Epoch 16/50\n",
      "830/830 [==============================] - 426s 513ms/step - loss: 0.3429 - accuracy: 0.9422 - val_loss: 0.7046 - val_accuracy: 0.8332\n",
      "Epoch 17/50\n",
      "830/830 [==============================] - 426s 513ms/step - loss: 0.3296 - accuracy: 0.9485 - val_loss: 0.7047 - val_accuracy: 0.8312\n",
      "Epoch 18/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.3125 - accuracy: 0.9535 - val_loss: 0.6958 - val_accuracy: 0.8310\n",
      "Epoch 19/50\n",
      "830/830 [==============================] - 426s 513ms/step - loss: 0.2997 - accuracy: 0.9545 - val_loss: 0.6859 - val_accuracy: 0.8364\n",
      "Epoch 20/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2871 - accuracy: 0.9601 - val_loss: 0.6971 - val_accuracy: 0.8294\n",
      "Epoch 21/50\n",
      "830/830 [==============================] - 426s 513ms/step - loss: 0.2777 - accuracy: 0.9616 - val_loss: 0.6828 - val_accuracy: 0.8326\n",
      "Epoch 22/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2691 - accuracy: 0.9631 - val_loss: 0.6741 - val_accuracy: 0.8355\n",
      "Epoch 23/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.2616 - accuracy: 0.9653 - val_loss: 0.6804 - val_accuracy: 0.8327\n",
      "Epoch 24/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.2511 - accuracy: 0.9690 - val_loss: 0.6625 - val_accuracy: 0.8417\n",
      "Epoch 25/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2406 - accuracy: 0.9715 - val_loss: 0.6602 - val_accuracy: 0.8443\n",
      "Epoch 26/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.2332 - accuracy: 0.9731 - val_loss: 0.6373 - val_accuracy: 0.8477\n",
      "Epoch 27/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.2295 - accuracy: 0.9735 - val_loss: 0.6463 - val_accuracy: 0.8448\n",
      "Epoch 28/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2217 - accuracy: 0.9767 - val_loss: 0.6376 - val_accuracy: 0.8448\n",
      "Epoch 29/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2161 - accuracy: 0.9786 - val_loss: 0.6545 - val_accuracy: 0.8433\n",
      "Epoch 30/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2149 - accuracy: 0.9773 - val_loss: 0.6109 - val_accuracy: 0.8517\n",
      "Epoch 31/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.2073 - accuracy: 0.9797 - val_loss: 0.6368 - val_accuracy: 0.8507\n",
      "Epoch 32/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.2016 - accuracy: 0.9811 - val_loss: 0.6432 - val_accuracy: 0.8471\n",
      "Epoch 33/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1979 - accuracy: 0.9816 - val_loss: 0.6247 - val_accuracy: 0.8505\n",
      "Epoch 34/50\n",
      "830/830 [==============================] - 424s 512ms/step - loss: 0.1935 - accuracy: 0.9820 - val_loss: 0.6430 - val_accuracy: 0.8448\n",
      "Epoch 35/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1932 - accuracy: 0.9809 - val_loss: 0.6296 - val_accuracy: 0.8481\n",
      "Epoch 36/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1889 - accuracy: 0.9833 - val_loss: 0.6369 - val_accuracy: 0.8448\n",
      "Epoch 37/50\n",
      "830/830 [==============================] - 424s 511ms/step - loss: 0.1829 - accuracy: 0.9851 - val_loss: 0.6208 - val_accuracy: 0.8484\n",
      "Epoch 38/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1789 - accuracy: 0.9853 - val_loss: 0.6011 - val_accuracy: 0.8526\n",
      "Epoch 39/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1777 - accuracy: 0.9851 - val_loss: 0.6073 - val_accuracy: 0.8533\n",
      "Epoch 40/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.1759 - accuracy: 0.9851 - val_loss: 0.6069 - val_accuracy: 0.8524\n",
      "Epoch 41/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.1718 - accuracy: 0.9857 - val_loss: 0.6273 - val_accuracy: 0.8439\n",
      "Epoch 42/50\n",
      "830/830 [==============================] - 425s 513ms/step - loss: 0.1666 - accuracy: 0.9876 - val_loss: 0.6060 - val_accuracy: 0.8524\n",
      "Epoch 43/50\n",
      "830/830 [==============================] - 424s 512ms/step - loss: 0.1645 - accuracy: 0.9880 - val_loss: 0.6031 - val_accuracy: 0.8548\n",
      "Epoch 44/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1629 - accuracy: 0.9885 - val_loss: 0.6144 - val_accuracy: 0.8510\n",
      "Epoch 45/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1618 - accuracy: 0.9878 - val_loss: 0.6073 - val_accuracy: 0.8515\n",
      "Epoch 46/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1559 - accuracy: 0.9893 - val_loss: 0.6158 - val_accuracy: 0.8514\n",
      "Epoch 47/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1556 - accuracy: 0.9889 - val_loss: 0.6295 - val_accuracy: 0.8469\n",
      "Epoch 48/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1544 - accuracy: 0.9893 - val_loss: 0.6041 - val_accuracy: 0.8509\n",
      "Epoch 49/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1514 - accuracy: 0.9908 - val_loss: 0.6086 - val_accuracy: 0.8531\n",
      "Epoch 50/50\n",
      "830/830 [==============================] - 425s 512ms/step - loss: 0.1502 - accuracy: 0.9901 - val_loss: 0.6009 - val_accuracy: 0.8543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258d40c5760>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testImport.fit([train_img_224, train_geom], train_lbl,\n",
    "                 validation_data=([val_img_224, val_geom], val_lbl),\n",
    "                 epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fb2fa",
   "metadata": {},
   "source": [
    "tf hub also has models made to work on video input. I don't know anything about them, I'm just writing this down to investigate later. \n",
    "\n",
    "https://tfhub.dev/shoaib6174/swin_base_patch244_window877_kinetics600_22k/1\n",
    "\n",
    "shape_of_input = [1,3,32,224,224]   # [batch_size, channels, frames, height, width]\n",
    "\n",
    "\"output shape will be [1,768*******]\" (I don't know what that means).\n",
    "\n",
    "another example: https://tfhub.dev/deepmind/i3d-kinetics-600/1, https://github.com/deepmind/kinetics-i3d\n",
    "\n",
    "...\n",
    "\n",
    "An alternate mp.Hands version for cropped images? https://tfhub.dev/mediapipe/tfjs-model/handpose_3d/landmark/full/1\n",
    "\n",
    "\"PNAS\" https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/5\n",
    "\n",
    "The first block (2 layers) of vgg-19 https://tfhub.dev/emilutz/vgg19-block1-conv2-unpooling-encoder/1\n",
    "\n",
    "The first 3 blocks (6 layers) of vgg19. this is exactly the architecture I was using before, so the comparison here may give an idea of how much performance gain there is to get from the pre-trained models. https://tfhub.dev/emilutz/vgg19-block3-conv2-unpooling-encoder/1\n",
    "\n",
    "and the first 3.5 blocks (8 layers) of vgg19. https://tfhub.dev/emilutz/vgg19-block4-conv2-unpooling-encoder/1\n",
    "\n",
    "and the first 5 blocks (12 layers) of vgg19 https://tfhub.dev/emilutz/vgg19-block5-conv2-unpooling-encoder/1\n",
    "\n",
    "\"EfficientNet\" apparently gets top-quality results with far less computation required. https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\n",
    "\n",
    "MobileNet: 40% more features than the one we looked at previously https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/5\n",
    "\n",
    "MobileNetV3: https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a88632",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
