{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e47025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "\n",
    "data_loc = sys.argv[1]\n",
    "# basic datasets: train and val at 64x64\n",
    "train_img_small = np.load(data_loc+'/training/train_img_64.npy')\n",
    "train_img_small = np.float32(train_img_small) / 255\n",
    "\n",
    "#train_geom = np.concatenate([np.load(\"datasets/train_geom_img.npy\"), np.load(\"datasets/train_geom_wrl.npy\")], axis=1)\n",
    "train_geom = np.concatenate([\n",
    "            np.load(data_loc+'/training/train_geom_img.npy'),\n",
    "            np.load(data_loc+'/training/train_geom_wrl.npy')\n",
    "            ],\n",
    "            axis=1)\n",
    "train_geom = train_geom.reshape((-1, 21*6))\n",
    "\n",
    "train_lbl = np.load(f'{data_loc}/training/train_lbl.npy')\n",
    "print(train_img_small.shape, train_geom.shape, train_lbl.shape)\n",
    "\n",
    "\n",
    "val_img_small = np.load(f'{data_loc}/validation/val_img_64.npy')\n",
    "val_img_small = np.float32(val_img_small) / 255\n",
    "\n",
    "val_geom = np.concatenate([\n",
    "    np.load(f'{data_loc}/validation/val_geom_img.npy'),\n",
    "    np.load(f'{data_loc}/validation/val_geom_wrl.npy')\n",
    "    ],\n",
    "    axis=1)\n",
    "val_geom = val_geom.reshape((-1,21*6))\n",
    "\n",
    "val_lbl = np.load(f'{data_loc}/validation/val_lbl.npy')\n",
    "print(val_img_small.shape, val_geom.shape, val_lbl.shape)\n",
    "\n",
    "\n",
    "# synthetic data pre-generated by albumentations\n",
    "train_img_synth = np.concatenate([\n",
    "    np.load(f'{data_loc}/synthetic/train_img_64_synth.npy'),\n",
    "    np.load(f'{data_loc}/training/train_img_64.npy')\n",
    "    ],\n",
    "    axis=0)\n",
    "\n",
    "train_lbl_synth = np.concatenate([\n",
    "    np.load(f'{data_loc}/synthetic/train_lbl_64_synth.npy'),\n",
    "    np.load(f'{data_loc}/training/train_lbl.npy')\n",
    "    ],\n",
    "    axis=0)\n",
    "\n",
    "train_geom_synth = np.concatenate([\n",
    "    np.concatenate([\n",
    "        np.load(f'{data_loc}/synthetic/train_geom_img_64_synth.npy'),\n",
    "        np.load(f'{data_loc}/synthetic/train_geom_wrl_64_synth.npy')\n",
    "        ],\n",
    "        axis=1),\n",
    "    np.concatenate([        \n",
    "        np.load(f'{data_loc}/training/train_geom_img.npy'),\n",
    "        np.load(f'{data_loc}/training/train_geom_wrl.npy'),\n",
    "        ],\n",
    "        axis=1)\n",
    "    ],\n",
    "    axis=0)\n",
    "\n",
    "train_img_synth = np.float32(train_img_synth) / 255\n",
    "train_geom_synth = train_geom_synth.reshape((-1, 21*6))\n",
    "print(train_img_synth.shape, train_geom_synth.shape, train_lbl_synth.shape)\n",
    "\n",
    "synthesiser_train = tf.keras.Sequential([tf.keras.layers.RandomBrightness(0.2, value_range=(0,1)),\n",
    "                                    tf.keras.layers.RandomFlip(mode = 'horizontal'),\n",
    "                                    tf.keras.layers.RandomRotation(0.05, fill_mode='constant'),\n",
    "                                    tf.keras.layers.RandomZoom(height_factor=0.2, fill_mode='constant')])\n",
    "batch_size = train_img_synth.shape[0]\n",
    "train_img_tf = tf.data.Dataset.from_tensor_slices(train_img_synth)\n",
    "train_geom_tf = tf.data.Dataset.from_tensor_slices(train_geom_synth).batch(batch_size).get_single_element()\n",
    "\n",
    "\n",
    "# this makes a dataset object with an attached function, rather than just applying a function once to its tensors\n",
    "train_synth = train_img_tf.map(lambda x: synthesiser_train(x),\n",
    "                                 num_parallel_calls=batch_size).batch(batch_size)\n",
    "train_proc = train_synth.get_single_element()\n",
    "\n",
    "\n",
    "# had to add @tf.function to make models saveable\n",
    "class testAttentionModel(tf.keras.Model):\n",
    "    def __init__(self, conv_filters, reg_coef=0, labels=50, use_geom_backup=True):\n",
    "        super(testAttentionModel, self).__init__()\n",
    "        filters_1, filters_2, filters_3 = conv_filters\n",
    "        conv_out_size = filters_3\n",
    "        self.reg = tf.keras.regularizers.L2(reg_coef)\n",
    "        self.spatial_dropout_prob = 0.0\n",
    "        self.dropout_prob = 0.1\n",
    "        self.use_geom_backup = use_geom_backup\n",
    "        \n",
    "        \n",
    "        # 64x64xch\n",
    "        self.conv_1a = tf.keras.layers.Convolution2D(filters_1, 5, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        self.conv_1b = tf.keras.layers.Convolution2D(filters_1, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        \n",
    "        self.batch = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # 32x32xch\n",
    "        self.conv_2a = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        self.conv_2b = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True,\n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        # 16x16xch\n",
    "        self.conv_3a = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                    kernel_regularizer=self.reg)\n",
    "        self.conv_3b = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, \n",
    "                                                     activation='relu',\n",
    "                                                     kernel_regularizer=self.reg)\n",
    "        # 8x8xch\n",
    "        #self.conv_4a = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "                                                    # activation='tanh', kernel_regularizer=regulator)\n",
    "        #self.conv_4b = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "                                                     #activation='tanh', kernel_regularizer=regulator)\n",
    "        # out: 4x4xch\n",
    "        \n",
    "        self.geom1 = tf.keras.layers.Dense(64, use_bias=True, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom_backup = tf.keras.layers.Dense(64, use_bias=True, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.geom2 = tf.keras.layers.Dense(64, use_bias=True, activation='relu', kernel_regularizer=self.reg)\n",
    "        self.attention = tf.keras.layers.Dense(conv_out_size, use_bias=True, \n",
    "                                               activation='softmax', kernel_regularizer=self.reg)\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(labels, activation='softmax')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, input_list, training=True):\n",
    "        c_out = tf.keras.layers.GaussianNoise(0.03)(input_list[0], #start 64x64x3\n",
    "                                                    training=training)\n",
    "        c_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_1a(c_out),\n",
    "                                                                            training=training)\n",
    "        \n",
    "        c_out = self.batch(c_out)\n",
    "        \n",
    "        c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_1b(c_out)) # to 32x32xch\n",
    "        c_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_2a(c_out),\n",
    "                                                                            training=training)\n",
    "        \n",
    "        c_out = self.batch(c_out)\n",
    "        \n",
    "        c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_2b(c_out))\n",
    "        c_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob) (self.conv_3a(c_out),\n",
    "                                                                            training=training)\n",
    "        \n",
    "        c_out = self.batch(c_out)\n",
    "        \n",
    "        c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_3b(c_out)) # to 16x16xch\n",
    "        #c_out = self.conv_4a(c_out)\n",
    "        #c_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_4b(c_out))\n",
    "       \n",
    "        if tf.math.reduce_max(input_list[1]) == 0 and self.use_geom_backup:\n",
    "            g_out = self.geom_backup(tf.keras.layers.Flatten()(tf.keras.layers.AveragePooling2D(pool_size=(4, 4),\n",
    "                                                                                                strides=(4,4),\n",
    "                                                                                                padding='valid')(input_list[0])))\n",
    "        else:\n",
    "            g_out = self.geom1(input_list[1]) # if use_geom_backup is off this will output max(0, bias)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(g_out, training=training)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(self.geom2(g_out),training=training)\n",
    "        g_out = tf.keras.layers.Dropout(self.dropout_prob)(self.attention(g_out),training=training)\n",
    "        g_out = tf.expand_dims(tf.expand_dims(g_out, axis=-2), axis=-2)\n",
    "       \n",
    "        return self.classifier(tf.keras.layers.Flatten()(tf.math.multiply(c_out, g_out)))\n",
    "    \n",
    "\n",
    "testAttender = testAttentionModel((64,128,256), reg_coef=0.0001)\n",
    "\n",
    "learning_rate=0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    learning_rate,\n",
    "                    decay_steps=5000,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "\n",
    "testAttender.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "NUM_EPOCHS = 40\n",
    "history = testAttender.fit([train_proc, train_geom_tf], train_lbl_synth,\n",
    "                 validation_data=([val_img_small, val_geom], val_lbl),\n",
    "                 epochs=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "# ## Saving the model ##\n",
    "save_loc = sys.argv[2]\n",
    "\n",
    "# This code cell saves the model's architecture \n",
    "# to a folder name save_loc + 'static_80_ep'\n",
    "# see https://www.tensorflow.org/guide/keras/save_and_serialize#keras_h5_format\n",
    "testAttender.save(f'{save_loc}/static_40_ep')\n",
    "\n",
    "\n",
    "# ## Plotting ##\n",
    "# This code cell plots the training and val accuracy by epoch\n",
    "history_dict = history.history\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.scatter(range(1,NUM_EPOCHS+1), \n",
    "            history_dict['accuracy'], \n",
    "            label=\"Training Data\")\n",
    "plt.scatter(range(1,NUM_EPOCHS+1), \n",
    "            history_dict['val_accuracy'], \n",
    "            marker='v',\n",
    "            label=\"Validation Data\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Accuracy\", fontsize=18)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "print(f\"saving to {save_loc}/train_val_accuracy_by_epoch.png\")\n",
    "plt.savefig(f'{save_loc}/train_val_accuracy_by_epoch.png', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
