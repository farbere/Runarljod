{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ad9a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from time import time\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c79628",
   "metadata": {},
   "source": [
    "<h3>Kinetics-i3d video action classifier</h3>\n",
    "\n",
    "See https://github.com/deepmind/kinetics-i3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6723a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after i3d.py\n",
    "\n",
    "pad_same = \"SAME\"\n",
    "pad_valid = \"VALID\"\n",
    "\n",
    "class Unit3D(tf.Module):\n",
    "    #Basic unit containing Conv3D + BatchNorm + non-linearity.\n",
    "\n",
    "    def __init__(self, output_channels,\n",
    "                   kernel_shape = [1,1,1],\n",
    "                   stride=[1, 1, 1],\n",
    "                   activation_fn=tf.nn.relu,\n",
    "                   use_batch_norm=True,\n",
    "                   use_bias=False,\n",
    "                   name='unit_3d'):\n",
    "        super(Unit3D, self).__init__()\n",
    "        self._output_channels = output_channels\n",
    "        self._kernel_shape = kernel_shape\n",
    "        self._padding = pad_same\n",
    "        self._stride = [1] + stride + [1]\n",
    "        self._use_batch_norm = use_batch_norm\n",
    "        self._activation_fn = activation_fn\n",
    "        self._use_bias = use_bias\n",
    "        self._name = name\n",
    "\n",
    "        # vardict refers to a global dictionary of tf.Variables loaded from the file containing the weights\n",
    "        if self._use_batch_norm:\n",
    "            self.bn_beta = vardict[self._name + \"/batch_norm/beta\"]\n",
    "            self.bn_moving_mean = vardict[self._name + \"/batch_norm/moving_mean\"]\n",
    "            self.bn_moving_variance = vardict[self._name + \"/batch_norm/moving_variance\"]\n",
    "        self.conv_w = vardict[self._name + \"/conv_3d/w\"]\n",
    "        if self._use_bias:\n",
    "            self.conv_b = vardict[self._name+\"/conv_3d/b\"]\n",
    "        \n",
    "    def __call__(self, inputs, is_training):\n",
    "         # input shape is [batch, depth, height, width, channels]\n",
    "        net = tf.nn.conv3d(inputs, filters=self.conv_w, strides=self._stride, padding=self._padding)\n",
    "        if self._use_bias:\n",
    "            net = tf.nn.bias_add(net, self.conv_b)\n",
    "        if self._use_batch_norm:\n",
    "            net = tf.nn.batch_normalization(net, \n",
    "                                            self.bn_moving_mean, \n",
    "                                            self.bn_moving_variance, \n",
    "                                            self.bn_beta, \n",
    "                                            scale=1, \n",
    "                                            variance_epsilon=0.01)\n",
    "        if self._activation_fn is not None:\n",
    "            net = self._activation_fn(net)\n",
    "        return net\n",
    "\n",
    "class InceptionI3d(tf.Module):\n",
    "#  \"\"\"Inception-v1 I3D architecture.\n",
    "\n",
    "#  The model is introduced in:\n",
    "\n",
    "#    Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
    "#    Joao Carreira, Andrew Zisserman\n",
    "#    https://arxiv.org/pdf/1705.07750v1.pdf.\n",
    "\n",
    "#  See also the Inception architecture, introduced in:\n",
    "\n",
    "#    Going deeper with convolutions\n",
    "#    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
    "#    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
    "#    http://arxiv.org/pdf/1409.4842v1.pdf.\n",
    "#  \"\"\"\n",
    "\n",
    "  # Endpoints of the model in order. During construction, all the endpoints up\n",
    "  # to a designated `final_endpoint` are returned in a dictionary as the\n",
    "  # second return value.\n",
    "    VALID_ENDPOINTS = (\n",
    "      'Conv3d_1a_7x7',\n",
    "      'MaxPool3d_2a_3x3',\n",
    "      'Conv3d_2b_1x1',\n",
    "      'Conv3d_2c_3x3',\n",
    "      'MaxPool3d_3a_3x3',\n",
    "      'Mixed_3b',\n",
    "      'Mixed_3c',\n",
    "      'MaxPool3d_4a_3x3',\n",
    "      'Mixed_4b',\n",
    "      'Mixed_4c',\n",
    "      'Mixed_4d',\n",
    "      'Mixed_4e',\n",
    "      'Mixed_4f',\n",
    "      'MaxPool3d_5a_2x2',\n",
    "      'Mixed_5b',\n",
    "      'Mixed_5c',\n",
    "      'Logits',\n",
    "      'Predictions',\n",
    "  )\n",
    "    \n",
    "    # In the paper referenced above, notations are made of the receptive field after each pooling layer, i.e. the\n",
    "    # size of the input data that each of its outputs depends on. They are (time is the first dim listed):\n",
    "    #   MaxPool3d_2a_3x3   7x11x11\n",
    "    #   MaxPool3d_3a_3x3   11x27x27\n",
    "    #   MaxPool3d_4a_3x3   23x75x75\n",
    "    #   MaxPool3d_5a_2x2   59x219x219\n",
    "    #   AvgPool3d_2x7x7    99x539x539\n",
    "    # The AvgPool layer is immediately prior to the logits (which are linear combinations of its outputs).\n",
    "    # Since the net uses only convolutional layers the dimensions of its input are not fixed (they can even vary\n",
    "    # between calls, since the net calls tf.nn.conv3d directly rather than keras.layers.Conv3D).\n",
    "    \n",
    "\n",
    "    def __init__(self, var_prefix='RGB', num_classes=400, spatial_squeeze=True,\n",
    "               final_endpoint='Logits', name='inception_i3d'):\n",
    "#    \"\"\"Initializes I3D model instance.\n",
    "\n",
    "#    Args:\n",
    "#      num_classes: The number of outputs in the logit layer (default 400, which\n",
    "#          matches the Kinetics dataset).\n",
    "#      spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
    "#          before returning (default True).\n",
    "#      final_endpoint: The model contains many possible endpoints.\n",
    "#          `final_endpoint` specifies the last endpoint for the model to be built\n",
    "#          up to. In addition to the output at `final_endpoint`, all the outputs\n",
    "#          at endpoints up to `final_endpoint` will also be returned, in a\n",
    "#          dictionary. `final_endpoint` must be one of\n",
    "#          InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
    "#      name: A string (optional). The name of this module.\n",
    "#    Raises:\n",
    "#      ValueError: if `final_endpoint` is not recognized.\n",
    "#    \"\"\"\n",
    "\n",
    "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
    "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
    "\n",
    "        super(InceptionI3d, self).__init__(name=name)\n",
    "        self._num_classes = num_classes\n",
    "        self._spatial_squeeze = spatial_squeeze\n",
    "        self._final_endpoint = final_endpoint\n",
    "        self._var_prefix = var_prefix\n",
    "\n",
    "        # except for the first and last entries here all this (output channels and kernel shape) is already implicit \n",
    "        # in the weights passed to the modules. the important part is the correspondence between modules and the \n",
    "        # names of variables contained in the checkpoint data\n",
    "        arg_dict = {'Conv3d_1a_7x7' : {'output_channels': 64, 'kernel_shape' : [7,7,7], 'stride' : [2,2,2]}, \n",
    "                     'Conv3d_2b_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Conv3d_2c_3x3' : {'output_channels' : 192, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_3b/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3b/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 96, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3b/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 128, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_3b/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 16, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3b/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 32, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_3b/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 32, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3c/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3c/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3c/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 192, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_3c/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 32, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_3c/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 96, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_3c/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4b/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 192, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4b/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 96, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4b/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 208, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4b/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 16, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4b/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 48, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4b/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4c/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 160, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4c/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 112, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4c/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 224, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4c/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 24, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4c/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 64, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4c/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4d/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4d/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4d/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 256, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4d/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 24, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4d/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 64, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4d/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4e/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 112, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4e/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 144, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4e/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 288, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4e/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 32, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4e/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 64, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4e/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 64, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4f/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 256, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4f/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 160, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4f/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 320, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4f/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 32, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_4f/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 128, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_4f/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5b/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 256, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5b/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 160, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5b/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 320, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_5b/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 32, 'kernel_shape' : [1, 1, 1]},\n",
    "                    # typo here: in other modules the name is Branch2/Conv3d_0b_3x3 !\n",
    "                     'Mixed_5b/Branch_2/Conv3d_0a_3x3' : {'output_channels' : 128, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_5b/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5c/Branch_0/Conv3d_0a_1x1' : {'output_channels' : 384, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5c/Branch_1/Conv3d_0a_1x1' : {'output_channels' : 192, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5c/Branch_1/Conv3d_0b_3x3' : {'output_channels' : 384, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_5c/Branch_2/Conv3d_0a_1x1' : {'output_channels' : 48, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Mixed_5c/Branch_2/Conv3d_0b_3x3' : {'output_channels' : 128, 'kernel_shape' : [3, 3, 3]},\n",
    "                     'Mixed_5c/Branch_3/Conv3d_0b_1x1' : {'output_channels' : 128, 'kernel_shape' : [1, 1, 1]},\n",
    "                     'Logits/Conv3d_0c_1x1' : {'output_channels' : self._num_classes, 'kernel_shape' : [1, 1, 1],\n",
    "                                              'activation_fn' : None, 'use_batch_norm' : False, 'use_bias' : True}\n",
    "                    }\n",
    "        \n",
    "        self.module_dict = {}\n",
    "        model_prefix = self._var_prefix + \"/\" + self.name + \"/\" \n",
    "        for module_name in list(arg_dict.keys()):\n",
    "            self.module_dict[module_name] = Unit3D(**arg_dict[module_name],\n",
    "                                              name = model_prefix+module_name)\n",
    "\n",
    "    def __call__(self, inputs, is_training = False, dropout_prob=0.0):\n",
    "#    \"\"\"Connects the model to inputs.\n",
    "\n",
    "#    Args:\n",
    "#      inputs: Inputs to the model, which should have dimensions\n",
    "#          `batch_size` x `num_frames` x 224 x 224 x `num_channels`.\n",
    "#      is_training: whether to use training mode for snt.BatchNorm (boolean).\n",
    "#      dropout_prob: Probability for the tf.nn.dropout layer (float in\n",
    "#          [0, 1)).\n",
    "\n",
    "#    Returns:\n",
    "#      A tuple consisting of:\n",
    "#        1. Network output at location `self._final_endpoint`.\n",
    "#        2. Dictionary containing all endpoints up to `self._final_endpoint`,\n",
    "#           indexed by endpoint name.\n",
    "\n",
    "#    Raises:\n",
    "#      ValueError: if `self._final_endpoint` is not recognized.\n",
    "#    \"\"\"\n",
    "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
    "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
    "\n",
    "        net = inputs\n",
    "        end_points = {}\n",
    "        end_point = 'Conv3d_1a_7x7'        \n",
    "        net = self.module_dict[end_point](net, is_training=is_training)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "        \n",
    "        end_point = 'MaxPool3d_2a_3x3'\n",
    "        net = tf.nn.max_pool3d(net, ksize=[1, 1, 3, 3, 1], strides=[1, 1, 2, 2, 1],\n",
    "                               padding=pad_same, name=end_point)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "        end_point = 'Conv3d_2b_1x1'\n",
    "        net = self.module_dict[end_point](net, is_training=is_training)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "        end_point = 'Conv3d_2c_3x3'\n",
    "        net = self.module_dict[end_point](net, is_training=is_training)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "        end_point = 'MaxPool3d_3a_3x3'\n",
    "        net = tf.nn.max_pool3d(net, ksize=[1, 1, 3, 3, 1], strides=[1, 1, 2, 2, 1],\n",
    "                               padding=pad_same, name=end_point)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_3b'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_3c'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'MaxPool3d_4a_3x3'\n",
    "        # modified: was time stride = 2\n",
    "        net = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1], strides=[1, 1, 2, 2, 1],\n",
    "                               padding=pad_same, name=end_point)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_4b'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_4c'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_4d'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_4e'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_4f'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'MaxPool3d_5a_2x2'\n",
    "        net = tf.nn.max_pool3d(net, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 2, 2, 1],\n",
    "                               padding=pad_same, name=end_point)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_5b'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        # typo here: in other modules the name is Branch2/Conv3d_0b_3x3 !\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Mixed_5c'\n",
    "        branch_0 = self.module_dict[end_point+'/Branch_0/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_1 = self.module_dict[end_point+'/Branch_1/Conv3d_0b_3x3'](branch_1, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0a_1x1'](net, is_training=is_training)\n",
    "        branch_2 = self.module_dict[end_point+'/Branch_2/Conv3d_0b_3x3'](branch_2, is_training=is_training)\n",
    "        branch_3 = tf.nn.max_pool3d(net, ksize=[1, 3, 3, 3, 1],\n",
    "                                            strides=[1, 1, 1, 1, 1], padding=pad_same)\n",
    "        branch_3 = self.module_dict[end_point+'/Branch_3/Conv3d_0b_1x1'](branch_3, is_training=is_training)\n",
    "        net = tf.concat([branch_0, branch_1, branch_2, branch_3], 4)\n",
    "        end_points[end_point] = net\n",
    "        if self._final_endpoint == end_point: return net, end_points\n",
    "\n",
    "        end_point = 'Logits'\n",
    "        net = tf.nn.avg_pool3d(net, ksize=[1, 2, 7, 7, 1],\n",
    "                                 strides=[1, 1, 1, 1, 1], padding=pad_valid)\n",
    "        net = tf.nn.dropout(net, dropout_prob)\n",
    "        logits = self.module_dict[end_point+'/Conv3d_0c_1x1'](net, is_training=is_training)\n",
    "        if self._spatial_squeeze:\n",
    "            logits = tf.squeeze(logits, [2, 3], name='SpatialSqueeze')\n",
    "        averaged_logits = tf.reduce_mean(logits, axis=1)\n",
    "        end_points[end_point] = averaged_logits\n",
    "        if self._final_endpoint == end_point: return averaged_logits, end_points\n",
    "\n",
    "        end_point = 'Predictions'\n",
    "        predictions = tf.nn.softmax(averaged_logits)\n",
    "        end_points[end_point] = predictions\n",
    "        return predictions, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211a4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n",
      "INFO:tensorflow:Restoring parameters from i3d/data/checkpoints/flow_imagenet/model.ckpt\n",
      "WARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n",
      "INFO:tensorflow:Restoring parameters from i3d/data/checkpoints/rgb_imagenet/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "fine_tuning=False\n",
    "\n",
    "# paths to pre-trained i3d models provided by the paper's authors\n",
    "_CHECKPOINT_PATHS = {\n",
    "    'rgb': 'i3d/data/checkpoints/rgb_scratch/model.ckpt',\n",
    "    'rgb600': 'i3d/data/checkpoints/rgb_scratch_kin600/model.ckpt',\n",
    "    'flow': 'i3d/data/checkpoints/flow_scratch/model.ckpt',\n",
    "    'rgb_imagenet': 'i3d/data/checkpoints/rgb_imagenet/model.ckpt',\n",
    "    'flow_imagenet': 'i3d/data/checkpoints/flow_imagenet/model.ckpt',\n",
    "}\n",
    "\n",
    "# this list has the names of all the weights in the Flow net and their shapes\n",
    "flow_varlist = tf.train.list_variables(_CHECKPOINT_PATHS['flow_imagenet'])\n",
    "flow_vardict = {}\n",
    "# make variables to load the saved weights into\n",
    "for variable in flow_varlist:\n",
    "    flow_vardict[variable[0]] = tf.Variable(initial_value = np.zeros(variable[1], dtype=np.float32),\n",
    "                                            shape=tf.TensorShape(variable[1]),\n",
    "                                            trainable=fine_tuning,\n",
    "                                            name=variable[0])\n",
    "    \n",
    "flow_saver = tf.compat.v1.train.Saver(var_list=flow_vardict)\n",
    "flow_saver.restore(sess=None, save_path=_CHECKPOINT_PATHS['flow_imagenet'])\n",
    "\n",
    "rgb_varlist = tf.train.list_variables(_CHECKPOINT_PATHS['rgb_imagenet'])\n",
    "rgb_vardict = {}\n",
    "for variable in rgb_varlist:\n",
    "    rgb_vardict[variable[0]] = tf.Variable(initial_value = np.zeros(variable[1], dtype=np.float32),\n",
    "                                           shape=tf.TensorShape(variable[1]), \n",
    "                                           trainable=fine_tuning,\n",
    "                                           name=variable[0])\n",
    "rgb_saver = tf.compat.v1.train.Saver(var_list=rgb_vardict, reshape=True)\n",
    "rgb_saver.restore(sess=None, save_path=_CHECKPOINT_PATHS['rgb_imagenet'])\n",
    "\n",
    "# now vardict will contain all the weights\n",
    "vardict = {}\n",
    "vardict.update(rgb_vardict)\n",
    "vardict.update(flow_vardict)\n",
    "\n",
    "# some warning messages about deprecated functions appear here, they're irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8a6ab",
   "metadata": {},
   "source": [
    "<h3>Video FS recog</h3>\n",
    "\n",
    "The Encoder module runs the i3d network on image (RGB) frames of fingerspelling sequences together with optical flow frames computed from them, up to a specified endpoint within the net architecture. All the i3d layers use padding to keep shape, so the reductions in time depth come only at the layers with time stride larger than 1. These are the first layer (conv 7x7x7 stride 2x2x2), and the pooling layers after the second and seventh inception modules,\n",
    "\n",
    "    MaxPool3d_4a_3x3, MaxPool3d_5a_3x3\n",
    "    \n",
    "Consequently after layer 4a the time depth has been reduced by a factor of 4, and after layer 5a the time depth has been reduced by a factor of 8. This means that the network can output potentially one character per 4 or 8 frames, respectively. In our data set the average number of frames per character is about 5.7, but a quarter of sequences have fewer than 4 frames per character. Since we are using the i3d net for its feature abstraction capabilities (rather than the classification task for which it was trained) it appears reasonable to decrease the time strides of the pooling layers, in which case the network can emit up to one character per 2 frames.\n",
    "\n",
    "The output of the Encoder module is fed into the Cogitator module, a sequence of LSTM cells. The time depth is unchanged here. Finally, the Decoder module assigns a probability distribution (over the alphabet) to each output frame. (More precisely, these modules are run on two streams of hand-detected inputs from the frame sequence. The output depths of the two streams are the same.) The alphabet includes the special 'blank' symbol which is meant to indicate a break in the input sequence or just a lack of result. Most often in this dataset only one hand in the frame sequence is actually signing, and the output of the other stream should tend to be just a sequence of 'blanks'. From the decoder output a CTC loss is computed, and the network weights are updated. (At least for now, the Encoder module doesn't contain any trainable weights.) \n",
    "\n",
    "Unfortunately for our purposes, in the basic CTC algorithm the 'blank' character performs two functions. Generally the network's output characters are emitted faster than letters are formed. The intended behaviour in this case is that the network should continue emitting the character it sees in the frame sequence until something changes; a later step in the calculation of the CTC loss collapses these repeats, but repeated characters separated by a blank are not merged. This means that interleaving the outputs of the two input streams is not a neutral operation: the stream watching a hand that does not sign may emit only blanks, but putting these into the output sequence of the stream watching a signing hand causes the repetitions it emits not to be merged as intended. Elaborations of the CTC set-up have long allowed for multiple 'blank' symbols and other special punctuation, which would remove this problem, but TensorFlow does not implement this. (The change is not trivial: CTC computes the probability of a given label from the probabilities of all the output sequences which reduce to that label, of which there are many, by a dynamic programming 'forward-backward' method. Adding another 'blank' symbol requires updating this calculation to recognise the new outputs which collapse to a given label. Changing the processing to remove the 'separator' functionality of blanks, and merging repeated characters even across them, presents the same difficulty.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3282dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_coef = 1/2048\n",
    "\n",
    "def random_flip(seq):\n",
    "    if tf.random.categorical(tf.math.log([[0.5, 0.5]]), 1).numpy()[0][0]:\n",
    "        #return tf.raw_ops.Reverse(tensor=seq, dims=[False,False,True,False])\n",
    "        return tf.raw_ops.Reverse(tensor=seq, dims=[False,False,False,True,False])\n",
    "    return seq\n",
    "\n",
    "def random_augment(seq):\n",
    "    seq = random_flip(seq)\n",
    "    seq = tf.image.random_brightness(seq, 0.15)\n",
    "    seq = tf.image.random_saturation(seq, 0.85, 1.15)\n",
    "    seq = tf.image.random_contrast(seq, 0.85, 1.15)\n",
    "    #seq = tf.image.random_hue(seq, 0.01)\n",
    "    return tf.raw_ops.ClipByValue(t=seq, clip_value_min=0, clip_value_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f51a6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softsign_shift(x):\n",
    "    return tf.nn.softsign(x) + 0.5\n",
    "\n",
    "class testSpeller3dEncoder(tf.Module):\n",
    "    def __init__(self, i3d_endpoint='MaxPool3d_4a_3x3', fine_tuning=False):\n",
    "        super(testSpeller3dEncoder, self).__init__()\n",
    "        self.fine_tuning = fine_tuning\n",
    "        self.rgb = InceptionI3d(var_prefix='RGB', final_endpoint=i3d_endpoint)\n",
    "        self.flow = InceptionI3d(var_prefix='Flow',final_endpoint=i3d_endpoint)\n",
    "        \n",
    "    def __call__(self, inputs, is_training=False, return_dict=False):\n",
    "        # input shape [batch, time, height, width, channels]\n",
    "        rgb_scaled = random_augment(tf.image.convert_image_dtype(inputs[0].to_tensor(), dtype=tf.float32))\n",
    "        rgb_results = self.rgb(rgb_scaled, self.fine_tuning)\n",
    "        # i3d expects flow in range [-1,1]\n",
    "        flow_results = self.flow(2*random_flip(inputs[1].to_tensor())-1, self.fine_tuning)\n",
    "        if return_dict:\n",
    "            return rgb_results[1], flow_results[1]\n",
    "        return rgb_results[0], flow_results[0]\n",
    "    \n",
    "class testSpellerAttender(tf.Module):\n",
    "    def __init__(self, units=256, encoder_size = 480):\n",
    "        self.dense1 = tf.keras.layers.Dense(units, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units, activation='relu')\n",
    "        self.dense_rgb = tf.keras.layers.Dense(encoder_size, activation='softmax')\n",
    "        self.dense_flow = tf.keras.layers.Dense(encoder_size, activation='softmax')\n",
    "        \n",
    "    def __call__(self, geom, is_training=True):\n",
    "        # pool w/ stride 2 in time dim to agree with depth reduction in Encoder\n",
    "        out = self.dense2(self.dense1(geom.to_tensor()))\n",
    "        return tf.nn.avg_pool(self.dense_rgb(out), ksize=[1,2,1], strides=[1,2,1], padding='SAME'), tf.nn.avg_pool(self.dense_flow(out), ksize=[1,2,1], strides=[1,2,1], padding='SAME')\n",
    "    \n",
    "class testSpellerCogitator(tf.Module):\n",
    "    def __init__(self, units=256):\n",
    "        super(testSpellerCogitator, self).__init__()\n",
    "        # i/o shape [batch, timesteps, channels] with return_sequences on\n",
    "        # out shape [batch, units] with return_sequences off\n",
    "        self.lstm1 = tf.keras.layers.LSTM(units, return_sequences=True, recurrent_activation=softsign_shift)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(units, return_sequences=True, recurrent_activation=softsign_shift)\n",
    "    \n",
    "    def __call__(self, inputs, is_training=True):\n",
    "        # an option here is to propagate the LSTM state as well; this amounts to making it a recurrent layer\n",
    "        return self.lstm2(self.lstm1(inputs))\n",
    "    \n",
    "class testSpellerDecoder(tf.Module):\n",
    "    def __init__(self, units=256, labels=32):\n",
    "        super(testSpellerDecoder,self).__init__()\n",
    "        self.class1 = tf.keras.layers.Dense(units, activation = 'relu')\n",
    "        self.class2 = tf.keras.layers.Dense(units, activation = 'relu')\n",
    "        self.class3 = tf.keras.layers.Dense(labels)\n",
    "        \n",
    "    def __call__(self, input_, is_training=True):\n",
    "        return self.class3(self.class2(self.class1(input_)))\n",
    "    \n",
    "learning_rate=0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    learning_rate,\n",
    "                    decay_steps=500,\n",
    "                    decay_rate=0.9,\n",
    "                    staircase=True)\n",
    "\n",
    "#opt=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "opt=tf.keras.optimizers.Nadam(learning_rate=0.0001)\n",
    "# Nadam, adam with nesterov momentum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "769c5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # or 32, 64, 128, 256; we should increase this depending on how many processors/threads will be running\n",
    "buffer_size = 32 # should be at least batch_size; only used for shuffling the dataset, so not a critical parameter\n",
    "\n",
    "alphabet_size = 32 # 26 letters, 5 punctuation symbols, and the 'blank' symbol\n",
    "\n",
    "attender_units = 64\n",
    "cogitator_units = 128\n",
    "decoder_units = 128\n",
    "\n",
    "i3d_endpoint='MaxPool3d_4a_3x3'\n",
    "encoder_output = 480\n",
    "# later endpoint options include:\n",
    "#i3d_endpoint = 'MaxPool3d_5a_2x2'\n",
    "#i3d_endpoint = 'Mixed_5c'\n",
    "timings = np.zeros(6)\n",
    "\n",
    "#testSpell2dEncode = testSpeller2dEncoder((64,64,128,128))\n",
    "testSpell3dEncode = testSpeller3dEncoder(i3d_endpoint = i3d_endpoint)\n",
    "testSpellAttend = testSpellerAttender(attender_units, encoder_output)\n",
    "testSpellCogitate = testSpellerCogitator(units = cogitator_units)\n",
    "testSpellDecode = testSpellerDecoder(units = decoder_units, labels=alphabet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0a2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cfsw = \"h:/project/cfsw/\"\n",
    "base_cfswp = \"h:/project/cfswp/\"\n",
    "\n",
    "# this makes a small difference to the data shuffling; it should be at least batch_size\n",
    "\n",
    "#buffer_size = batch_size\n",
    "\n",
    "def load_fs_batches(start_batch, endpoint, \n",
    "                   image_in, geom_in, label_in, image_tag,\n",
    "                   batch_size = batch_size, image_size=128, file_size=256, buffer_size=32):\n",
    "    for batch_number in range(start_batch, endpoint):\n",
    "        suffix = str(batch_number) + \".npy\"\n",
    "        image_suffix = str(batch_number) + image_tag + \".npy\"\n",
    "        \n",
    "        rgb_a = np.load(image_in + \"train_img_a_\" + image_suffix)\n",
    "        flow_a = np.load(image_in + \"train_flow_a_\" + image_suffix)\n",
    "        geom_a = np.concatenate([np.load(geom_in + \"train_geom_img_a_\" + suffix), # 21 * 3 floats per frame per file\n",
    "                                 np.load(geom_in + \"train_geom_wrl_a_\" + suffix)], axis=1)\n",
    "        rgb_b = np.load(image_in + \"train_img_b_\" + image_suffix)\n",
    "        flow_b = np.load(image_in + \"train_flow_b_\" + image_suffix)\n",
    "        geom_b = np.concatenate([np.load(geom_in + \"train_geom_img_b_\" + suffix),\n",
    "                                 np.load(geom_in + \"train_geom_wrl_b_\" + suffix)], axis=1)\n",
    "        label_seqs = np.load(label_in + \"train_label_seqs_\" + suffix)\n",
    "        label_lengths = np.load(label_in + \"train_seq_lengths_\" + suffix)\n",
    "        frame_counts = np.load(label_in + \"train_frame_counts_\" + suffix)\n",
    "\n",
    "        seq_count = frame_counts.shape[0]\n",
    "        seq_heights=[image_size]*file_size\n",
    "        seq_widths = [image_size]*file_size\n",
    "        # for input that hasn't been resized to a fixed section, these lines compute dimensions\n",
    "        #seq_heights = [crnrs[1][1] - crnrs[0][1] for crnrs in corners[0]]\n",
    "        #seq_widths = [crnrs[1][0] - crnrs[0][0] for crnrs in corners[0]]\n",
    "\n",
    "        total_frames = frame_counts.sum()\n",
    "        frame_heights = list(chain(*[[seq_heights[j]]*frame_counts[j] for j in range(seq_count)]))\n",
    "        frame_widths = list(chain(*[[seq_widths[j]]*frame_counts[j] for j in range(seq_count)]))\n",
    "        by_widths = list(chain(*[[frame_widths[j]]*frame_heights[j] for j in range(total_frames)]))\n",
    "        geom_lengths = list(chain(*[[21]*frame_counts[j] for j in range(seq_count)]))\n",
    "\n",
    "        rgb_a_tensor = tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(rgb_a, \n",
    "                                                                                                                      by_widths),\n",
    "                                                                                        frame_heights),\n",
    "                                                        frame_counts)\n",
    "        del rgb_a \n",
    "        flow_a_tensor = tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(flow_a, \n",
    "                                                                                                                        by_widths),\n",
    "                                                                                          frame_heights),\n",
    "                                                          frame_counts)\n",
    "        del flow_a\n",
    "        flow_a_tensor = tf.cast(flow_a_tensor, tf.float32)\n",
    "\n",
    "        geom_a_tensor = tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(geom_a, geom_lengths),\n",
    "                                                          frame_counts).merge_dims(-2,-1)\n",
    "        del geom_a\n",
    "\n",
    "\n",
    "        rgb_b_tensor = tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(rgb_b, \n",
    "                                                                                                                        by_widths),\n",
    "                                                                                          frame_heights),\n",
    "                                                          frame_counts)\n",
    "        del rgb_b\n",
    "\n",
    "        flow_b_tensor = tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(flow_b, \n",
    "                                                                                                                        by_widths),\n",
    "                                                                                          frame_heights),\n",
    "                                                          frame_counts)\n",
    "        del flow_b\n",
    "        flow_b_tensor = tf.cast(flow_b_tensor, tf.float32)\n",
    "\n",
    "        geom_b_tensor = tf.RaggedTensor.from_row_lengths(tf.RaggedTensor.from_row_lengths(geom_b, geom_lengths),\n",
    "                                                          frame_counts).merge_dims(-2,-1)\n",
    "        del geom_b\n",
    "\n",
    "        # bundle the data into a tf Dataset with set batch size\n",
    "        if batch_number==start_batch:\n",
    "            dataset_ = tf.data.Dataset.from_tensor_slices((((rgb_a_tensor, flow_a_tensor, geom_a_tensor), \n",
    "                                                    (rgb_b_tensor, flow_b_tensor, geom_b_tensor)), \n",
    "                                                   (label_seqs, label_lengths)))\n",
    "        else:\n",
    "            dataset_ = dataset_.concatenate(tf.data.Dataset.from_tensor_slices((((rgb_a_tensor, flow_a_tensor, geom_a_tensor), \n",
    "                                                    (rgb_b_tensor, flow_b_tensor, geom_b_tensor)), \n",
    "                                                   (label_seqs, label_lengths))))\n",
    "        del rgb_a_tensor, flow_a_tensor, geom_a_tensor\n",
    "        del rgb_b_tensor, flow_b_tensor, geom_b_tensor\n",
    "    return dataset_.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True).batch(batch_size)\n",
    "\n",
    "def load_csfw_batches(start_batch, endpoint, batch_size = batch_size):\n",
    "    return load_fs_batches(start_batch, endpoint,\n",
    "                           image_in = base_cfsw + 'cfsw_128/',\n",
    "                           geom_in = base_cfsw + 'cfsw_geom/',\n",
    "                           label_in = base_cfsw + 'cfsw_labels/',\n",
    "                           batch_size = batch_size, image_tag = \"_128\", \n",
    "                           file_size=256, buffer_size = buffer_size)\n",
    "\n",
    "def load_csfwp_batches(start_batch, endpoint, batch_size = batch_size):\n",
    "    return load_fs_batches(start_batch, endpoint,\n",
    "                          image_in = base_cfswp + 'cfswp_128/',\n",
    "                          geom_in = base_cfswp + 'cfswp_geom/',\n",
    "                          label_in = base_cfswp + 'cfswp_labels/',\n",
    "                          batch_size = batch_size, image_tag = '',\n",
    "                          file_size=512, buffer_size=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "783ce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fs_batch(input_batch, label_batch, return_lists=False, batch_size=batch_size, is_training=True):\n",
    "    # input_batch has frame sequences [batch_size, frame_count, height, width, channels]\n",
    "    #   organised as ((rgb_a, flow_a), (rgb_b, flow_b))\n",
    "    # label_batch has seq labels plus their lengths\n",
    "    out_lengths = np.zeros(batch_size, dtype=np.int32) # records time depth of encoder output\n",
    "    timings_ = np.zeros(5) # informational statistics on how long various computations take\n",
    "    \n",
    "    timings_[0]=time()    \n",
    "    decoder_results_list_a = []\n",
    "    decoder_results_list_b = []\n",
    "    for k in range(batch_size):\n",
    "        # since this length depends only on the depth of the input and the i3d net endpoint,\n",
    "        # both A and B have the same output length, even if the frame sizes are different\n",
    "        out_lengths[k] = np.int32(np.ceil(input_batch[0][0][k].get_shape()[0]/2))\n",
    "    encoder_results_a = testSpell3dEncode(input_batch[0], is_training=is_training)\n",
    "    encoder_results_b = testSpell3dEncode(input_batch[1], is_training=is_training)\n",
    "    results_depth = encoder_results_a[0].shape[1]\n",
    "    timings_[1] = time()\n",
    "    \n",
    "    # attender gives weights to the filters of the encoder; spatial axes of size 1x1 are added\n",
    "    # to get tf to broadcast multiplication correctly\n",
    "    attender_results_a = tf.expand_dims(tf.expand_dims(testSpellAttend(input_batch[0][2], is_training=is_training), -2), -2)\n",
    "    attender_results_b = tf.expand_dims(tf.expand_dims(testSpellAttend(input_batch[1][2], is_training=is_training), -2), -2)\n",
    "    \n",
    "    encoder_reshaped_a = tf.concat([tf.reshape(tf.math.multiply(encoder_results_a[0],\n",
    "                                                                attender_results_a[0]), \n",
    "                                               (batch_size, results_depth, -1)),\n",
    "                                    tf.reshape(tf.math.multiply(encoder_results_a[1],\n",
    "                                                                attender_results_a[1]),\n",
    "                                               (batch_size, results_depth, -1))],\n",
    "                                   axis=2)\n",
    "    encoder_reshaped_b = tf.concat([tf.reshape(tf.math.multiply(encoder_results_b[0],\n",
    "                                                                attender_results_b[0]), \n",
    "                                               (batch_size, results_depth, -1)),\n",
    "                                    tf.reshape(tf.math.multiply(encoder_results_b[1],\n",
    "                                                                attender_results_b[1]),\n",
    "                                               (batch_size, results_depth, -1))],\n",
    "                                   axis=2)\n",
    "    timings_[2] = time()\n",
    "\n",
    "    cogitator_results_a = testSpellCogitate(encoder_reshaped_a, is_training=is_training)\n",
    "    cogitator_results_b = testSpellCogitate(encoder_reshaped_b, is_training=is_training)\n",
    "    timings_[3] = time()\n",
    "\n",
    "    for j in range(cogitator_results_a.get_shape()[1]):\n",
    "        decoder_results_list_a.append(testSpellDecode(cogitator_results_a[:,j], is_training=is_training))\n",
    "        decoder_results_list_b.append(testSpellDecode(cogitator_results_b[:,j], is_training=is_training))\n",
    "    decoder_results_list_a = tf.convert_to_tensor(decoder_results_list_a)\n",
    "    decoder_results_list_b = tf.convert_to_tensor(decoder_results_list_b)\n",
    "\n",
    "    if return_lists:\n",
    "        timings_[4] = time()\n",
    "        timings[0:4] = np.diff(timings_)\n",
    "        return decoder_results_list_a, decoder_results_list_b, out_lengths\n",
    "    \n",
    "    loss_a = tf.nn.ctc_loss(label_batch[0], # tensor of shape [batch_size, max_label_seq_length] or SparseTensor\n",
    "                         decoder_results_list_a, # tensor of shape [frames, batch_size, num_labels] : prob. of each character at each time step \n",
    "                         label_batch[1],  # tensor of shape [batch_size], or None if labels=SparseTensor\n",
    "                         out_lengths,  # tensor of shape [batch_size] \"Length of input sequence in logits.\"\n",
    "                         logits_time_major=True) # flip to swap first two axes of logits tensor\n",
    "    loss_b = tf.nn.ctc_loss(label_batch[0],\n",
    "                         decoder_results_list_b, \n",
    "                         label_batch[1],  \n",
    "                         out_lengths,  \n",
    "                         logits_time_major=True)\n",
    "    timings_[4] = time()\n",
    "    timings[0:4] = np.diff(timings_)\n",
    "    # tf.nn.l2_loss(tensor) # regulariser\n",
    "    \n",
    "    return tf.math.minimum(loss_a, loss_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "43229452",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSpell3dEncode2 = testSpeller3dEncoder(i3d_endpoint = i3d_endpoint)\n",
    "test_batch = test_dataset.take(1)\n",
    "for elt in test_batch:\n",
    "    input_batch, label_batch = elt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a8bae6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_results_a = testSpell3dEncode2(input_batch[0], return_dict=True)\n",
    "encoder_results_a = encoder_results_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1fc9302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 31, 32, 32, 64)\n",
      "(8, 31, 16, 16, 192)\n",
      "(8, 31, 8, 8, 480)\n"
     ]
    }
   ],
   "source": [
    "pool_args = {'ksize' : [1,1,2,2,1], 'strides' : [1,1,2,2,1], 'padding' : pad_same}\n",
    "\n",
    "def pool_to_4x4(filter_tensor):\n",
    "    filter_space = filter_tensor.get_shape()[-2]\n",
    "    parity = True\n",
    "    while filter_space > 4:\n",
    "        if parity:\n",
    "            filter_tensor = tf.nn.avg_pool3d(filter_tensor, **pool_args)\n",
    "            filter_space = filter_space // 2\n",
    "            parity = not parity\n",
    "        else:\n",
    "            filter_tensor = tf.nn.max_pool3d(filter_tensor, **pool_args)\n",
    "            filter_space == filter_space // 2\n",
    "            parity = not parity\n",
    "    return filter_tensor\n",
    "\n",
    "def pool_and_concatenate(tensor_dict, keys):\n",
    "    return tf.concat([pool_to_4x4(t[key])] for key in keys], axis=-1)\n",
    "  \n",
    "i3d_endpoints = ['MaxPool3d_5a_2x2', 'MaxPool3d_4a_3x3', 'MaxPool3d_3a_3x3', 'MaxPool3d_2a_3x3']\n",
    "encoder_results_a[0]['conv_2dim'] = testSpell2dEncode(input_batch[0][0])\n",
    "encoder_results_b[0]['conv_2dim'] = testSpell2dEncode(input_batch[1][0])\n",
    "encoder_out_a = tf.concat([pool_and_concatenate(encoder_results_a[0], i3d_endpoints + ['conv_2dim']),\n",
    "                           pool_and_concatenate(encoder_results_a[1], i3d_endpoints)]).reshape(batch_size, \n",
    "                                                                                               results_depth, -1)\n",
    "encoder_out_b = encoder_out_a = tf.concat([pool_and_concatenate(encoder_results_a[0], i3d_endpoints + ['conv_2dim']),\n",
    "                           pool_and_concatenate(encoder_results_a[1], i3d_endpoints)]).reshape(batch_size, \n",
    "                                                                                               results_depth, -1)\n",
    "        \n",
    "\n",
    "print(encoder_results_a(['MaxPool3d_2a_3x3'].shape) # 32x32x64\n",
    "print(encoder_results_a(['MaxPool3d_3a_3x3'].shape) # 16x16x192\n",
    "print(encoder_results_a(['MaxPool3d_4a_3x3'].shape) # 8x8x480\n",
    "#encoder_results_a[0]['MaxPool3d_5a_2x2'].shape # 4x4x832\n",
    "\n",
    "encoder_results_a[0]['MaxPool3d_4a_3x3'],\n",
    "tf.nn.avg_pool3d(encoder_results_a[0]['MaxPool3d_4a_3x3'], **pool_args)\n",
    "tf.nn.avg_pool3d(encoder_results_a[1]['MaxPool3d_4a_3x3'], **pool_args)\n",
    "tf.nn.avg_pool3d(encoder_results_b[0]['MaxPool3d_4a_3x3'], **pool_args)\n",
    "tf.nn.max_pool3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "505cd88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop version loads the entire dataset into memory\n",
    "epochs = 1\n",
    "timings_temp = time()\n",
    "cfsw_train = load_cfsw_batches(0, 21, batch_size = batch_size)\n",
    "timings[5] = time()-timings_temp\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch \" + str(epoch))\n",
    "    epoch_loss = 0       \n",
    "    for input_batch, label_batch in cfsw_train:\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            loss_ = process_fs_batch(input_batch, label_batch)\n",
    "            print(loss_.numpy())\n",
    "            loss_sum = tf.math.reduce_sum(loss_).numpy()\n",
    "            epoch_loss += loss_sum\n",
    "            \n",
    "        timings_temp = time()\n",
    "        grad_decode = tape.gradient(loss_, testSpellDecode.trainable_variables)\n",
    "        grad_cogitate = tape.gradient(loss_, testSpellCogitate.trainable_variables)\n",
    "        grad_attend = tape.gradient(loss_, testSpellAttend.trainable_variables)\n",
    "        timings[4] = time() - timings_temp\n",
    "\n",
    "        opt.apply_gradients(zip(grad_decode, testSpellDecode.trainable_variables))\n",
    "        opt.apply_gradients(zip(grad_cogitate, testSpellCogitate.trainable_variables))\n",
    "        opt.apply_gradients(zip(grad_attend, testSpellAttend.trainable_variables))\n",
    "    print('\\nAverage loss this epoch: ', epoch_loss/(21*256)) \n",
    "print('Timings: ', timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5a11d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_list_stats(var_list):\n",
    "    norm_acc, number_acc = 0, 0\n",
    "    for j in range(len(var_list)):\n",
    "        number_acc += tf.math.reduce_prod(var_list[j].shape)\n",
    "        norm_acc += tf.math.reduce_euclidean_norm(var_list[j])\n",
    "    return norm_acc.numpy(), number_acc.numpy(), norm_acc.numpy()/number_acc.numpy()\n",
    "\n",
    "def var_list_decay(var_list, coef):\n",
    "    for j in range(len(var_list)):\n",
    "        var_list[j].assign_sub(var_list[j]*coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "939be99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "[ 9.796211   8.473819  15.416117  43.322746   6.1899147 17.462254\n",
      " 17.666729  27.439209 ]\n",
      "[19.32058263  0.0626924   0.67225838  4.71258354  9.34029102 15.78667021]\n",
      "(191.85931, 37152, 0.0051641718875119445)\n",
      "(38.530037, 31654912, 1.2171898290625333e-06)\n",
      "(11.735307, 74688, 0.00015712439400984265)\n",
      "[ 6.6294107  9.096701   7.926929   6.747944   6.7425213 12.820393\n",
      " 17.470823  26.094831 ]\n",
      "[19.06116343  0.05864835  0.68686438  0.42482233  9.44190145 15.78667021]\n",
      "(137.9225, 37152, 0.0037123842756877573)\n",
      "(25.09863, 31654912, 7.928826624174904e-07)\n",
      "(15.16831, 74688, 0.00020308898571932938)\n",
      "[ 9.199769 16.891727 10.904272 13.731448 14.006483 25.982197 20.295616\n",
      " 19.662216]\n",
      "[12.8787353   0.05043149  0.47526312  0.33152246  6.14788961 15.78667021]\n",
      "(174.96635, 37152, 0.00470947336267542)\n",
      "(37.756226, 31654912, 1.192744607406822e-06)\n",
      "(30.541744, 74688, 0.0004089243818575639)\n",
      "[ 6.310892  26.204674   6.0927315 12.650826  12.319907   9.847191\n",
      "  8.310272  24.978683 ]\n",
      "[18.45156169  0.04929256  0.66486669  0.39726019  8.82873154 15.78667021]\n",
      "(118.080696, 37152, 0.0031783133103455274)\n",
      "(27.617655, 31654912, 8.724603246540391e-07)\n",
      "(2.2435534, 74688, 3.0039007605501188e-05)\n",
      "[39.905296 25.165325 20.192509 25.327984 24.47052  24.287643 22.48758\n",
      " 11.693472]\n",
      "[19.83583212  0.06255269  0.70526958  0.44171143  9.81343722 15.78667021]\n",
      "(169.38483, 37152, 0.00455923844369499)\n",
      "(31.526247, 31654912, 9.959353867272205e-07)\n",
      "(7.811592, 74688, 0.00010458965432265935)\n",
      "[16.205421   9.219912   6.5992327  6.2474318  8.928417  11.806672\n",
      " 14.638687  11.379839 ]\n",
      "[12.47703886  0.04840541  0.47516847  0.32140231  6.16049361 15.78667021]\n",
      "(92.147804, 37152, 0.002480291889003389)\n",
      "(22.525442, 31654912, 7.115938949194705e-07)\n",
      "(1.6084566, 74688, 2.1535676569640383e-05)\n",
      "[25.272778 22.740685 15.117618 16.02459  13.264573 16.159548 10.908187\n",
      " 14.59314 ]\n",
      "[15.21150899  0.0484488   0.53559899  0.3514967   7.32025051 15.78667021]\n",
      "(225.76962, 37152, 0.0060769170651037464)\n",
      "(69.52274, 31654912, 2.1962703047507337e-06)\n",
      "(31.313818, 74688, 0.0004192617017178834)\n",
      "[10.357077   6.9171095 26.003826  21.707237   9.266264  18.15382\n",
      " 13.810244   7.3275175]\n",
      "[18.85215664  0.06061935  0.67430377  0.46225619  8.74925852 15.78667021]\n",
      "(265.5912, 37152, 0.007148772247077883)\n",
      "(170.84389, 31654912, 5.397073519872732e-06)\n",
      "(2.4986656, 74688, 3.3454712553727084e-05)\n",
      "[50.349033 25.89957  11.853642 21.1968   18.40105  11.619412 17.976578\n",
      " 25.549324]\n",
      "[25.68468857  0.06903028  0.88426375  0.56233358 12.68148875 15.78667021]\n",
      "(246.16869, 37152, 0.00662598745459426)\n",
      "(58.130104, 31654912, 1.8363691570187088e-06)\n",
      "(7.4010954, 74688, 9.90935008344021e-05)\n",
      "[46.043465 20.573635  8.321109 14.463516  8.836469 16.064117 15.624755\n",
      " 12.29623 ]\n",
      "[23.25890064  0.0786128   0.84822369  0.51285982 11.51701832 15.78667021]\n",
      "(299.242, 37152, 0.00805453284869001)\n",
      "(60.725765, 31654912, 1.918367842193701e-06)\n",
      "(38.921085, 74688, 0.0005211156458556396)\n",
      "[ 29.05928   21.87036    8.861074  18.323723  12.157343 711.283\n",
      "  29.129086  13.457714]\n",
      "[16.90241694  0.05852294  0.59579873  0.381495    8.04258204 15.78667021]\n",
      "(131.15541, 37152, 0.0035302382312285088)\n",
      "(52.311066, 31654912, 1.652541813220903e-06)\n",
      "(6.9035463, 74688, 9.243180073523174e-05)\n",
      "[15.503967  11.3734455  6.0200944  5.949118   8.541312  13.059528\n",
      " 15.957302  24.859049 ]\n",
      "[16.81441522  0.05049896  0.61403251  0.38184905  8.14423275 15.78667021]\n",
      "(121.658325, 37152, 0.0032746103896240447)\n",
      "(69.617874, 31654912, 2.19927555462823e-06)\n",
      "(12.864038, 74688, 0.0001722370188973761)\n",
      "[ 7.6128364 24.637619  29.9943     7.5055847 23.110048  15.619916\n",
      "  8.5385895 21.440823 ]\n",
      "[25.03632402  0.0638628   0.86133766  0.53209305 11.85745215 15.78667021]\n",
      "(179.49019, 37152, 0.004831238926535121)\n",
      "(37.489933, 31654912, 1.1843322456216595e-06)\n",
      "(22.58836, 74688, 0.0003024362659699506)\n",
      "[10.11821  18.28236  51.4304    7.511345 10.634293 20.213318 11.64162\n",
      " 29.401665]\n",
      "[39.99412012  0.09113598  1.39074063  0.88064742 19.6364336  15.78667021]\n",
      "(425.66113, 37152, 0.011457287166572459)\n",
      "(82.49774, 31654912, 2.6061592494466185e-06)\n",
      "(43.157757, 74688, 0.000577840574194247)\n",
      "[16.01749   18.927887  22.401167  28.40216   10.215891  15.765963\n",
      "  8.604996   7.9854574]\n",
      "[20.74707222  0.05861568  0.68768668  0.48170137 10.18661809 15.78667021]\n",
      "(103.005104, 37152, 0.0027725318708263728)\n",
      "(32.725426, 31654912, 1.0338182497621488e-06)\n",
      "(10.779386, 74688, 0.00014432553511556644)\n",
      "[11.255984   6.724485  14.9584055  9.84948    4.6317225 16.007925\n",
      " 22.288494  12.164302 ]\n",
      "[18.8495214   0.05247355  0.69519925  0.41012692  9.08898115 15.78667021]\n",
      "(206.11176, 37152, 0.005547797033029009)\n",
      "(38.340485, 31654912, 1.2112017439549547e-06)\n",
      "(2.2782645, 74688, 3.0503755925349323e-05)\n",
      "[ 6.237996 13.049817 21.362469  6.993863 23.855818 20.261414 37.26525\n",
      " 17.69562 ]\n",
      "[18.72796845  0.05805206  0.73967195  0.43868327  9.02273774 15.78667021]\n",
      "(144.65378, 37152, 0.003893566378019269)\n",
      "(27.826527, 31654912, 8.790587268682252e-07)\n",
      "(2.1390798, 74688, 2.8640207385240505e-05)\n",
      "[ 5.834887 35.23428  15.520151 21.896328  8.62666   8.144136 24.263756\n",
      " 45.458893]\n",
      "[25.19000244  0.07070112  0.85159278  0.56309557 12.6305635  15.78667021]\n",
      "(163.87428, 37152, 0.004410914159046998)\n",
      "(36.55401, 31654912, 1.154765759067241e-06)\n",
      "(16.920013, 74688, 0.00022654259623680343)\n",
      "[ 7.7691097  5.676626   5.554012  15.615216   7.912323   5.4163904\n",
      "  6.7367706 21.20449  ]\n",
      "[15.59173703  0.04024506  0.56389213  0.35215688  7.74677372 15.78667021]\n",
      "(88.62422, 37152, 0.002385449553234222)\n",
      "(21.817917, 31654912, 6.892426954185558e-07)\n",
      "(7.0795283, 74688, 9.478802929194237e-05)\n",
      "[21.805471  28.115536   6.597286   8.969643   7.7062726 18.810537\n",
      "  5.0542693  8.782738 ]\n",
      "[20.37406588  0.04840326  0.73026228  0.44793844 10.23201823 15.78667021]\n",
      "(134.77876, 37152, 0.003627766010373138)\n",
      "(30.637917, 31654912, 9.678724289279782e-07)\n",
      "(4.5227194, 74688, 6.0554833216042016e-05)\n",
      "[34.487087  21.882801  51.65137    4.7740493 16.444183  22.463572\n",
      "  7.6981473  4.747119 ]\n",
      "[37.37807417  0.09337592  1.2493186   0.77247572 18.47106719 15.78667021]\n",
      "(160.53235, 37152, 0.004320961149677339)\n",
      "(44.193897, 31654912, 1.3961149930637764e-06)\n",
      "(5.1449957, 74688, 6.888651040852734e-05)\n",
      "[14.963179 21.488758 12.119682 16.932884 11.158133 12.110333 14.544912\n",
      " 13.330954]\n",
      "[13.152107    0.04841161  0.47551918  0.2957921   6.57647109 15.78667021]\n",
      "(239.38385, 37152, 0.006443363751551902)\n",
      "(47.422775, 31654912, 1.4981174254584783e-06)\n",
      "(12.560798, 74688, 0.00016817691853236415)\n",
      "[31.487576   8.777208  28.63154    5.989215   5.611764  21.12463\n",
      " 29.836094   6.9947033]\n",
      "[17.33118963  0.05053687  0.61076999  0.39814544  8.78498435 15.78667021]\n",
      "(123.18146, 37152, 0.003315607706705729)\n",
      "(38.736126, 31654912, 1.22370032006549e-06)\n",
      "(46.61868, 74688, 0.0006241789718111459)\n",
      "[ 15.629159  41.287533  16.25195   29.421808  15.53406    9.095074\n",
      "  70.60729  710.579   ]\n",
      "[30.4953959   0.06491756  1.12576103  0.71751213 15.65041327 15.78667021]\n",
      "(175.02292, 37152, 0.004710995873739553)\n",
      "(34.415543, 31654912, 1.0872101809203911e-06)\n",
      "(6.9797645, 74688, 9.345228767027279e-05)\n",
      "[ 6.9481   39.31349  25.869413 11.866354  7.34011  20.488247 41.633755\n",
      " 17.56335 ]\n",
      "[38.59643292  0.101197    1.30998015  0.81142688 19.22988939 15.78667021]\n",
      "(257.0326, 37152, 0.0069184052749094935)\n",
      "(52.308704, 31654912, 1.6524672182383796e-06)\n",
      "(43.10949, 74688, 0.0005771943209206026)\n",
      "[16.831459 13.201961 16.019627 15.902849 17.512697 16.166912 20.853897\n",
      "  7.028741]\n",
      "[14.5747726   0.04232621  0.53177881  0.34207463  7.31676841 15.78667021]\n",
      "(125.9426, 37152, 0.003389927767967993)\n",
      "(18.557741, 31654912, 5.862515481060611e-07)\n",
      "(4.4608784, 74688, 5.9726841958445574e-05)\n",
      "[23.483856   7.3526907  5.95576    6.5611844 16.677069   7.4262495\n",
      " 15.055332   9.939573 ]\n",
      "[17.21793962  0.05258155  0.64525652  0.40008259  8.70372176 15.78667021]\n",
      "(120.84261, 37152, 0.003252654317942906)\n",
      "(19.208504, 31654912, 6.068095758138431e-07)\n",
      "(3.9548376, 74688, 5.295144548861513e-05)\n",
      "[ 5.805995  17.548134   5.237481  16.362522  22.324959   6.9789906\n",
      " 10.003495  14.89065  ]\n",
      "[15.48409176  0.04929972  0.64859366  0.43365812  7.81925488 15.78667021]\n",
      "(82.54319, 37152, 0.002221769756740994)\n",
      "(62.05143, 31654912, 1.9602464776567743e-06)\n",
      "(70.44767, 74688, 0.000943226086960558)\n",
      "[ 7.7559834 15.048105   9.6525135  6.6507406 15.888353   8.532668\n",
      " 11.058334  13.731194 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.39529967  0.04029703  0.41394711  0.29171443  5.55115962 15.78667021]\n",
      "(89.71384, 37152, 0.0024147781188071133)\n",
      "(15.513202, 31654912, 4.900724953385438e-07)\n",
      "(2.044028, 74688, 2.736755628410117e-05)\n",
      "[ 4.7671175 30.332767   5.2156305  7.1049824 21.338531   8.341793\n",
      " 21.964289  14.425127 ]\n",
      "[24.48677611  0.07066035  0.87712955  0.50647902 12.65562797 15.78667021]\n",
      "(162.56107, 37152, 0.004375567013184435)\n",
      "(37.12052, 31654912, 1.1726622884123058e-06)\n",
      "(19.24214, 74688, 0.00025763362007664124)\n",
      "[  5.2342625   4.495678  709.584       4.5542274  12.60906    10.731551\n",
      "  13.322425   13.420117 ]\n",
      "[16.20144868  0.05041909  0.60287571  0.34342933  8.11578465 15.78667021]\n",
      "(187.64091, 37152, 0.005050627554828601)\n",
      "(41.651386, 31654912, 1.3157953577942763e-06)\n",
      "(23.366919, 74688, 0.00031286041350475006)\n",
      "[10.154652  25.88444    7.9416957 39.37274    9.152397  10.118042\n",
      "  5.744207  11.838406 ]\n",
      "[20.39018512  0.05131483  0.74883938  0.43888712 10.35318661 15.78667021]\n",
      "(265.33127, 37152, 0.007141776171149517)\n",
      "(58.896072, 31654912, 1.8605666124642935e-06)\n",
      "(24.046717, 74688, 0.000321962252169873)\n",
      "\n",
      "Average loss for batch 0/9:  24.22042688727379\n",
      "[28.310497  22.173565   5.996522   4.634331   3.7888448 16.420277\n",
      " 22.82465    9.791779 ]\n",
      "[16.28027177  0.04577374  0.61731744  0.39293122  8.33633327 19.14966393]\n",
      "(156.84132, 37152, 0.004221611860802624)\n",
      "(43.35697, 31654912, 1.3696759523679187e-06)\n",
      "(23.393457, 74688, 0.00031321574299378384)\n",
      "[ 6.7514725 13.642591  21.99482   25.325893  40.78755    7.4382095\n",
      " 13.719479   9.737709 ]\n",
      "[19.15204954  0.06072783  0.71608043  0.42199922  9.66937947 19.14966393]\n",
      "(190.80348, 37152, 0.00513575263931051)\n",
      "(82.590355, 31654912, 2.6090849634784516e-06)\n",
      "(140.60606, 74688, 0.0018825790467380762)\n",
      "[ 7.2482634 21.153404  29.379162  10.124058  32.167664   7.4191804\n",
      " 20.15601   28.394596 ]\n",
      "[26.5275178   0.06943107  0.92934871  0.55499673 13.23993206 19.14966393]\n",
      "(255.80482, 37152, 0.006885358118785033)\n",
      "(55.876915, 31654912, 1.7651893955044748e-06)\n",
      "(20.424175, 74688, 0.0002734599301420733)\n",
      "[ 8.186489 27.424404  6.946596 39.27591  32.34742  37.513096 22.211231\n",
      " 28.656252]\n",
      "[35.04641318  0.09370589  1.40732884  0.86809373 18.91768599 19.14966393]\n",
      "(456.68134, 37152, 0.012292240941247274)\n",
      "(82.77841, 31654912, 2.61502580911406e-06)\n",
      "(10.417081, 74688, 0.00013947462616767655)\n",
      "[27.051414 20.814976 21.820906  9.712687 12.87222  33.854485 20.487106\n",
      " 17.33596 ]\n",
      "[24.13541722  0.07305574  0.96784711  0.617347   12.68101645 19.14966393]\n",
      "(216.1707, 37152, 0.005818548128586407)\n",
      "(56.899597, 31654912, 1.7974966149951309e-06)\n",
      "(18.071548, 74688, 0.000241960535319115)\n",
      "[74.63902   30.144035  38.553444  25.230843  11.2227335  9.656556\n",
      " 11.8746195 92.71748  ]\n",
      "[46.63370943  0.13198543  1.76595044  1.00139809 25.11937976 19.14966393]\n",
      "(320.46637, 37152, 0.008625817442638519)\n",
      "(61.246365, 31654912, 1.9348139269351265e-06)\n",
      "(27.050947, 74688, 0.0003621859895743768)\n",
      "[42.27973  18.469393 30.191711  8.858765 13.800341 25.67629  28.994146\n",
      " 25.24151 ]\n",
      "[28.88912225  0.0811758   1.16525078  0.78662324 14.61760879 19.14966393]\n",
      "(352.68103, 37152, 0.009492921788152387)\n",
      "(158.91672, 31654912, 5.020286189053278e-06)\n",
      "(11.052916, 74688, 0.00014798783704370337)\n",
      "[ 7.611558 15.958324 17.994814 20.767414 18.763496 11.465649  8.149147\n",
      " 60.04286 ]\n",
      "[42.73911262  0.10113478  1.61227345  0.95787334 21.46612692 19.14966393]\n",
      "(288.14417, 37152, 0.007755818395754266)\n",
      "(40.575787, 31654912, 1.2818164394384092e-06)\n",
      "(10.989518, 74688, 0.00014713900714423173)\n",
      "[21.21724  21.202864 12.288629 23.85837  37.424744  6.23595  12.841209\n",
      "  8.755714]\n",
      "[17.80040097  0.05377364  0.6329174   0.39582515  9.12136412 19.14966393]\n",
      "(150.88239, 37152, 0.004061218380003936)\n",
      "(28.472092, 31654912, 8.994525612582555e-07)\n",
      "(5.163638, 74688, 6.913611443510603e-05)\n",
      "[17.517097  6.864524 21.215956 16.074516 48.135387 10.847495  8.570799\n",
      " 33.175163]\n",
      "[44.06687546  0.11593819  1.73057008  0.9191618  22.45183277 19.14966393]\n",
      "(316.77008, 37152, 0.008526326457967437)\n",
      "(59.37607, 31654912, 1.8757300009311154e-06)\n",
      "(21.131054, 74688, 0.00028292435096080425)\n",
      "[47.028824  13.497648  30.895327  17.708824  13.2729435 23.094633\n",
      " 24.66265   36.70858  ]\n",
      "[25.31300473  0.08911848  0.95873618  0.5636909  13.01405263 19.14966393]\n",
      "(368.9864, 37152, 0.009931804187127375)\n",
      "(51.75847, 31654912, 1.635084900186413e-06)\n",
      "(4.159902, 74688, 5.569706105123551e-05)\n",
      "[46.323822   6.250197   5.985558  12.082384   9.243834   6.4637327\n",
      "  8.909697  16.175156 ]\n",
      "[25.29378748  0.05379653  0.84910011  0.51696014 12.54722977 19.14966393]\n",
      "(155.14146, 37152, 0.004175857672087598)\n",
      "(26.243946, 31654912, 8.290639403906557e-07)\n",
      "(2.9830124, 74688, 3.9939648107064515e-05)\n",
      "[19.336212  13.048339   7.3725853 34.457634  13.463473   8.157524\n",
      " 22.883875  16.881336 ]\n",
      "[17.97085929  0.06083345  0.76788235  0.50802207  9.24639821 19.14966393]\n",
      "(89.683205, 37152, 0.0024139536135572488)\n",
      "(14.564417, 31654912, 4.600997432997437e-07)\n",
      "(5.3587008, 74688, 7.174781427081058e-05)\n",
      "[12.181725 20.114172 16.478909  7.782251 17.56952   7.424524 20.380095\n",
      " 28.580185]\n",
      "[20.83122754  0.06141615  0.80275416  0.54375434 10.9422648  19.14966393]\n",
      "(140.88965, 37152, 0.0037922493657811156)\n",
      "(19.289795, 31654912, 6.093776195579063e-07)\n",
      "(6.702081, 74688, 8.973437772414167e-05)\n",
      "[30.89869    6.0853405  6.534807   7.3303976 26.636642   7.100999\n",
      "  7.77621    8.274432 ]\n",
      "[21.89520335  0.07270312  0.8966372   0.47026777 11.21420312 19.14966393]\n",
      "(267.22745, 37152, 0.007192814586287835)\n",
      "(33.783527, 31654912, 1.067244393990657e-06)\n",
      "(19.557503, 74688, 0.0002618560243490525)\n",
      "[34.014038  14.181957   6.7573986  6.234479   9.334758  24.288868\n",
      "  6.8203707 13.577868 ]\n",
      "[16.71636534  0.06105661  0.66523147  0.38735056  8.40709805 19.14966393]\n",
      "(162.48885, 37152, 0.004373623111143285)\n",
      "(23.927086, 31654912, 7.558727655431436e-07)\n",
      "(5.5506787, 74688, 7.431821350164666e-05)\n",
      "[34.611843   8.676874   7.701148   6.466706   7.1820893 10.637137\n",
      "  6.3775935  8.556669 ]\n",
      "[15.79348826  0.04228234  0.53866267  0.33206415  7.51532936 19.14966393]\n",
      "(125.30976, 37152, 0.003372894085038848)\n",
      "(16.128948, 31654912, 5.095243421201083e-07)\n",
      "(9.3118925, 74688, 0.0001246772240448325)\n",
      "[ 6.174103   5.38246    7.352312   7.2153144  6.5089073 27.382233\n",
      " 10.170549   9.889546 ]\n",
      "[17.12347794  0.05689216  0.64787817  0.40759754  8.11647558 19.14966393]\n",
      "(112.815704, 37152, 0.003036598415851182)\n",
      "(11.839774, 31654912, 3.740264427768715e-07)\n",
      "(5.6238875, 74688, 7.52984085650963e-05)\n",
      "[22.355343  10.825983  13.377991  10.907794  27.947704   7.0976954\n",
      " 10.056118   6.8170385]\n",
      "[17.0854094   0.05936265  0.64856768  0.38546515  8.28259492 19.14966393]\n",
      "(168.56458, 37152, 0.004537160185059014)\n",
      "(20.249565, 31654912, 6.396974069778402e-07)\n",
      "(14.919595, 74688, 0.00019975892733383507)\n",
      "[ 6.308053   7.8362846 10.662165  10.435645   9.672519  18.026512\n",
      " 64.30752    5.262555 ]\n",
      "[34.25782704  0.09118438  1.25724459  0.78377175 18.10189652 19.14966393]\n",
      "(168.97424, 37152, 0.004548186993003405)\n",
      "(22.793558, 31654912, 7.200638599383104e-07)\n",
      "(1.8648242, 74688, 2.496819001492416e-05)\n",
      "[44.31105   13.043946   7.2094026  8.001496  15.709061   7.7365017\n",
      " 12.99597   25.173386 ]\n",
      "[35.50107861  0.10028076  1.07186794  0.68058228 18.44392085 19.14966393]\n",
      "(139.09406, 37152, 0.0037439183671345083)\n",
      "(16.493925, 31654912, 5.210542077831236e-07)\n",
      "(2.3181362, 74688, 3.1037599282481266e-05)\n",
      "[ 8.149432 51.475174 15.556164 47.581825 17.573196 10.127793  7.220847\n",
      "  5.994672]\n",
      "[30.61160398  0.07506108  1.14996886  0.63614488 15.90665984 19.14966393]\n",
      "(127.57438, 37152, 0.003433849563072921)\n",
      "(18.660965, 31654912, 5.895124575238217e-07)\n",
      "(1.5707794, 74688, 2.1031215761396348e-05)\n",
      "[20.041245   6.245273  12.286204   6.3411756 10.132068  38.20414\n",
      " 25.831028  42.106297 ]\n",
      "[27.9733386   0.07499242  0.94774985  0.57240081 14.23142791 19.14966393]\n",
      "(131.67496, 37152, 0.003544222579548628)\n",
      "(17.53019, 31654912, 5.537904990593611e-07)\n",
      "(7.747101, 74688, 0.00010372617863750702)\n",
      "[ 9.53056  57.555656  9.88047  51.60932  10.881464 19.913246 21.149298\n",
      " 23.214947]\n",
      "[32.27394366  0.09360552  1.27434111  0.79135418 17.22165442 19.14966393]\n",
      "(249.80197, 37152, 0.0067237826075459434)\n",
      "(40.758907, 31654912, 1.2876013466129755e-06)\n",
      "(11.402132, 74688, 0.00015266350731445155)\n",
      "[12.574933 10.317556 12.762584 15.231967 17.374107 10.39662  13.899244\n",
      " 27.703321]\n",
      "[23.10495663  0.07897615  0.97141433  0.5988884  12.22559834 19.14966393]\n",
      "(247.02895, 37152, 0.006649142601282611)\n",
      "(46.49114, 31654912, 1.4686863908594013e-06)\n",
      "(39.81732, 74688, 0.000533115383694084)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.903958  24.67627   35.188717   6.5146546  9.743471  21.686518\n",
      " 33.19203   14.773273 ]\n",
      "[18.10789704  0.05943871  0.77101588  0.50407767  9.36475563 19.14966393]\n",
      "(213.20892, 37152, 0.005738827609276587)\n",
      "(29.526693, 31654912, 9.327681386104062e-07)\n",
      "(3.561154, 74688, 4.7680402323029036e-05)\n",
      "[13.29659    9.643696  10.831479  64.7794    28.45706    9.3889675\n",
      "  8.39043   25.280788 ]\n",
      "[43.37036848  0.1218853   1.68970585  1.04150891 22.95608497 19.14966393]\n",
      "(141.41093, 37152, 0.003806280535320903)\n",
      "(22.973694, 31654912, 7.257544689322229e-07)\n",
      "(4.8948026, 74688, 6.553666680514966e-05)\n",
      "[23.253944  11.187419  23.090137  26.295534  50.399536  31.284601\n",
      " 10.816544   7.0026345]\n",
      "[28.59828401  0.06939864  1.01843262  0.55608058 14.48482251 19.14966393]\n",
      "(143.83136, 37152, 0.0038714297982149347)\n",
      "(42.96406, 31654912, 1.3572636605990422e-06)\n",
      "(18.904274, 74688, 0.00025310992377378436)\n",
      "[20.094215  38.589325  20.361782   8.948833  10.643777   6.6752253\n",
      " 36.619293  25.041903 ]\n",
      "[24.61879468  0.07500005  0.93368483  0.55474877 12.50468755 19.14966393]\n",
      "(161.2594, 37152, 0.0043405307766489695)\n",
      "(22.310305, 31654912, 7.047975569075545e-07)\n",
      "(8.861035, 74688, 0.00011864068320191815)\n",
      "[26.496311 10.781003 10.974825 44.933453 30.873104 15.208594 26.082085\n",
      " 11.725632]\n",
      "[28.10084248  0.07077861  0.94524598  0.55980134 13.93980932 19.14966393]\n",
      "(206.382, 37152, 0.005555071161288213)\n",
      "(34.592533, 31654912, 1.0928014303616535e-06)\n",
      "(22.189451, 74688, 0.00029709526587472375)\n",
      "[ 9.915928   4.723382  21.473225  24.863003   7.7473035 26.706722\n",
      " 20.008806  23.817184 ]\n",
      "[20.96562099  0.05840087  0.74555516  0.44234681 10.46980166 19.14966393]\n",
      "(143.22804, 37152, 0.003855190638526568)\n",
      "(30.452618, 31654912, 9.620187111960276e-07)\n",
      "(32.71417, 74688, 0.00043801103990713346)\n",
      "[17.94092  10.258757 18.887611  9.478748  6.891061 28.778828 18.629196\n",
      " 17.056269]\n",
      "[17.58846378  0.05868578  0.61881781  0.38289452  9.18421698 19.14966393]\n",
      "(94.06894, 37152, 0.0025320020243589677)\n",
      "(16.264275, 31654912, 5.137993938245025e-07)\n",
      "(4.1721025, 74688, 5.5860411998238847e-05)\n",
      "\n",
      "Average loss for batch 0/10:  18.9148972928524\n",
      "[ 9.480478  21.573689  12.04158   39.314133   6.0846324 18.482224\n",
      "  5.8111773  6.7374125]\n",
      "[18.13100553  0.04930711  0.73325062  0.43254352  8.87773585 24.69034958]\n",
      "(192.12892, 37152, 0.005171428765848112)\n",
      "(25.988615, 31654912, 8.209978608062705e-07)\n",
      "(21.075655, 74688, 0.00028218261278278314)\n",
      "[10.279039  23.69533   17.424583  17.704868  30.806175  21.137682\n",
      "  9.4327755  9.23596  ]\n",
      "[23.12199545  0.06902766  0.87124896  0.53262115 11.19602275 24.69034958]\n",
      "(164.36893, 37152, 0.0044242282246434415)\n",
      "(47.920876, 31654912, 1.5138527489609323e-06)\n",
      "(2.1252663, 74688, 2.8455258054210268e-05)\n",
      "[ 7.5199513 18.930737  15.751056   8.4543    19.123856  19.868675\n",
      " 12.702001   6.0621057]\n",
      "[17.38688278  0.05046844  0.59405208  0.30447841  7.89676881 24.69034958]\n",
      "(148.0847, 37152, 0.00398591466241618)\n",
      "(15.959654, 31654912, 5.041762192979755e-07)\n",
      "(1.7441969, 74688, 2.335310748426344e-05)\n",
      "[39.48314   26.100382   7.0948715 22.189388   7.399272  15.209717\n",
      " 21.068079   7.723361 ]\n",
      "[26.74624228  0.07867074  0.95045376  0.55348277 13.7653749  24.69034958]\n",
      "(164.54341, 37152, 0.004428924721546157)\n",
      "(17.302134, 31654912, 5.465860578029932e-07)\n",
      "(3.1716332, 74688, 4.246509805538763e-05)\n",
      "[10.532824   8.875101  19.235771  20.443645  26.730349  29.4226\n",
      " 17.714113   6.1367826]\n",
      "[25.98403978  0.07072949  0.95089769  0.56639886 12.8156743  24.69034958]\n",
      "(107.97807, 37152, 0.002906386550390751)\n",
      "(14.97754, 31654912, 4.731505813749953e-07)\n",
      "(7.8966084, 74688, 0.00010572793959754087)\n",
      "[26.51632  38.190655 21.105125 26.570549 16.466867  7.522564  8.088884\n",
      "  6.689613]\n",
      "[26.03489208  0.06296158  0.9070971   0.51186562 12.99337912 24.69034958]\n",
      "(128.64342, 37152, 0.003462624282902628)\n",
      "(12.64378, 31654912, 3.9942552216346934e-07)\n",
      "(5.76427, 74688, 7.717799149523868e-05)\n",
      "[ 7.7198772 14.700031   6.982663   7.6122203  7.8262033  7.6279407\n",
      " 16.468649  14.840157 ]\n",
      "[13.41467071  0.04244447  0.48385811  0.30930448  6.51403213 24.69034958]\n",
      "(108.08908, 37152, 0.002909374483488019)\n",
      "(16.416674, 31654912, 5.186137829186927e-07)\n",
      "(2.232499, 74688, 2.989099834245193e-05)\n",
      "[ 6.833786  36.21229   20.127405   7.073051  27.179207   9.8729515\n",
      "  7.0602956  6.7179527]\n",
      "[22.94932389  0.06057048  0.7169776   0.41152692 11.72262955 24.69034958]\n",
      "(142.68263, 37152, 0.00384051013259822)\n",
      "(15.236579, 31654912, 4.813337955684481e-07)\n",
      "(10.12339, 74688, 0.00013554239232211208)\n",
      "[10.714695  21.118828  20.144255   6.5157757 16.559431   6.1214786\n",
      " 13.374908  13.489389 ]\n",
      "[15.84639692  0.04862809  0.58242488  0.36332059  7.9301641  24.69034958]\n",
      "(108.64817, 37152, 0.0029244231931306905)\n",
      "(16.749458, 31654912, 5.291266743369333e-07)\n",
      "(6.212656, 74688, 8.3181448440421e-05)\n",
      "[12.073582  36.10447    7.631899  16.462952  11.45568    8.095114\n",
      " 21.382055   5.6075068]\n",
      "[27.37774706  0.07064152  0.94541192  0.5599587  13.87382317 24.69034958]\n",
      "(155.32707, 37152, 0.00418085357836872)\n",
      "(17.708326, 31654912, 5.594179614121713e-07)\n",
      "(7.9070177, 74688, 0.00010586731078385694)\n",
      "[42.027027   6.920286  47.189835   5.377187  21.099945  28.18729\n",
      "  6.1266346 38.556496 ]\n",
      "[33.85522676  0.08092618  1.17936754  0.67574954 17.07079387 24.69034958]\n",
      "(186.14563, 37152, 0.005010379787974066)\n",
      "(31.37946, 31654912, 9.912982661617733e-07)\n",
      "(3.99639, 74688, 5.3507793812845746e-05)\n",
      "[16.477676   5.327465  13.024824   7.1852694  7.0423245 27.964249\n",
      " 21.505066  16.456179 ]\n",
      "[20.75824642  0.06067705  0.65588403  0.3648448  10.46376562 24.69034958]\n",
      "(151.29172, 37152, 0.004072236152274356)\n",
      "(23.594797, 31654912, 7.453755402763215e-07)\n",
      "(6.0487647, 74688, 8.098710242151295e-05)\n",
      "[ 6.329831   5.5902305 13.79657    5.2322807 21.133698   3.627295\n",
      " 15.131018   9.452533 ]\n",
      "[18.66259098  0.04531908  0.65475178  0.41154432  8.74009848 24.69034958]\n",
      "(163.58783, 37152, 0.004403203854162461)\n",
      "(19.181828, 31654912, 6.05966857376385e-07)\n",
      "(2.2385569, 74688, 2.9972108797630695e-05)\n",
      "[ 7.4458017  6.653828  29.072937  11.77538    7.4289865 84.83414\n",
      " 28.773151  49.601738 ]\n",
      "[47.95133209  0.11153436  1.68223381  0.81891251 23.7676034  24.69034958]\n",
      "(334.1016, 37152, 0.008992829269422322)\n",
      "(53.7764, 31654912, 1.6988327599765683e-06)\n",
      "(7.254539, 74688, 9.713125285064449e-05)\n",
      "[ 6.697932 12.867358 16.835365 19.542553 46.08489  14.796143 37.128178\n",
      " 29.720554]\n",
      "[23.79393172  0.06495142  0.88691878  0.50136447 11.99325538 24.69034958]\n",
      "(280.21265, 37152, 0.007542330062563927)\n",
      "(38.63201, 31654912, 1.2204112718296047e-06)\n",
      "(15.546317, 74688, 0.0002081501325584418)\n",
      "[13.223132   7.517882  18.456621   7.4866514 13.262402  26.53267\n",
      " 15.588307  42.65972  ]\n",
      "[40.37989163  0.09086204  1.36835432  0.81386232 20.38750052 24.69034958]\n",
      "(134.07407, 37152, 0.0036087980771454934)\n",
      "(16.934889, 31654912, 5.349845496244526e-07)\n",
      "(5.1677175, 74688, 6.919073287298666e-05)\n",
      "[22.396873 17.617798 37.080276 36.827522  9.509032  8.430021 17.68463\n",
      "  6.906255]\n",
      "[39.05497193  0.10625839  1.41225266  0.87792873 19.48190999 24.69034958]\n",
      "(163.80978, 37152, 0.0044091780775071835)\n",
      "(15.206819, 31654912, 4.803936457200526e-07)\n",
      "(6.7412395, 74688, 9.025867003708082e-05)\n",
      "[20.962286   5.919845   5.8751793  8.804541  28.521294  27.647038\n",
      " 21.296349  20.777382 ]\n",
      "[26.41391706  0.07068634  0.88619661  0.5421989  12.6754725  24.69034958]\n",
      "(91.27442, 37152, 0.002456783529605258)\n",
      "(20.888344, 31654912, 6.598768561111513e-07)\n",
      "(5.3754663, 74688, 7.197228934689271e-05)\n",
      "[22.468918  22.564804  20.48347   12.676447   5.4624195 14.593058\n",
      " 22.80053   25.221199 ]\n",
      "[28.52095222  0.07063317  0.94452477  0.56164384 13.56012058 24.69034958]\n",
      "(152.5779, 37152, 0.00410685551566979)\n",
      "(65.4948, 31654912, 2.0690247615576907e-06)\n",
      "(13.328672, 74688, 0.0001784580174734578)\n",
      "[67.45756   35.04346   21.45943    6.0581703 15.961969  19.048538\n",
      " 29.58476   17.403236 ]\n",
      "[47.15685129  0.11126041  1.62269926  0.95084071 23.56330037 24.69034958]\n",
      "(202.46706, 37152, 0.005449694667162308)\n",
      "(68.59402, 31654912, 2.1669312184095975e-06)\n",
      "(8.872582, 74688, 0.00011879528753759519)\n",
      "[ 7.4723954  5.5888066 15.669868  10.421732  26.840403   7.983549\n",
      "  7.331303  51.66712  ]\n",
      "[30.65995455  0.05935216  0.94872904  0.4857316  15.4366293  24.69034958]\n",
      "(270.4851, 37152, 0.007280499230778289)\n",
      "(130.19263, 31654912, 4.1128728126972835e-06)\n",
      "(13.493959, 74688, 0.00018067105059554257)\n",
      "[18.934948 16.273958 19.869953 12.468466  5.63175  12.551695  9.879894\n",
      " 13.728304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.05817127  0.06492019  0.81700253  0.50412607 11.46232677 24.69034958]\n",
      "(225.89236, 37152, 0.00608022083607755)\n",
      "(45.615593, 31654912, 1.4410273184946137e-06)\n",
      "(5.6449313, 74688, 7.55801643687839e-05)\n",
      "[13.952521  21.338547  30.810425  15.9802475 44.626152  48.11917\n",
      "  5.40077   18.883888 ]\n",
      "[60.88966751  0.13372111  2.02498102  1.31369638 32.25278115 24.69034958]\n",
      "(543.1516, 37152, 0.01461971391387072)\n",
      "(353.33896, 31654912, 1.1162215795773951e-05)\n",
      "(16.018112, 74688, 0.00021446701187094564)\n",
      "[37.26317   26.735891   4.1092644 12.928933   8.823449   6.603446\n",
      " 26.393204   5.7706757]\n",
      "[41.12660503  0.09060001  1.23466969  0.76433778 20.28187895 24.69034958]\n",
      "(358.73828, 37152, 0.009655961489287252)\n",
      "(57.795948, 31654912, 1.825812942666353e-06)\n",
      "(58.65199, 74688, 0.0007852933400700822)\n",
      "[ 4.784301  40.841698   7.3750267 10.952774   5.1428213  5.31944\n",
      " 18.31645    5.2132215]\n",
      "[26.27144885  0.0707171   0.92642641  0.60258365 12.83506751 24.69034958]\n",
      "(268.41345, 37152, 0.007224737622427797)\n",
      "(39.738895, 31654912, 1.255378483322265e-06)\n",
      "(5.1428747, 74688, 6.885811265146212e-05)\n",
      "[17.557999  29.417965  11.13997   19.832731  40.56443    4.3442674\n",
      "  6.4581304 17.815567 ]\n",
      "[41.83412433  0.09090519  1.37947869  0.86363816 20.62588668 24.69034958]\n",
      "(289.15894, 37152, 0.0077831324167440515)\n",
      "(62.029488, 31654912, 1.9595533107109343e-06)\n",
      "(5.285828, 74688, 7.077212020078069e-05)\n",
      "[ 4.7384167 41.270752   4.6943626  4.84846    9.58106   15.17548\n",
      " 18.344204  15.461375 ]\n",
      "[40.00410557  0.09115171  1.36091495  0.80405927 19.96433902 24.69034958]\n",
      "(253.18127, 37152, 0.0068147414517135685)\n",
      "(39.970608, 31654912, 1.26269843231813e-06)\n",
      "(25.906801, 74688, 0.0003468669829658698)\n",
      "[25.854921   3.900034   4.0248756 38.6671    13.192486  21.126854\n",
      "  8.723851  25.55778  ]\n",
      "[36.53005576  0.1036756   1.37554312  0.84639549 19.37333846 24.69034958]\n",
      "(326.4607, 37152, 0.008787163365616251)\n",
      "(41.903984, 31654912, 1.3237750927825742e-06)\n",
      "(9.025, 74688, 0.00012083600603216411)\n",
      "[11.107923  15.273169  57.484245   4.1369543 28.37349    7.8927145\n",
      "  7.0761147 18.418423 ]\n",
      "[59.26698995  0.24564838  2.39119911  2.27462006 28.6299789  24.69034958]\n",
      "(177.88144, 37152, 0.004787937101878348)\n",
      "(31.803877, 31654912, 1.0047059008355814e-06)\n",
      "(5.5874195, 74688, 7.48101369682907e-05)\n",
      "[15.2505245  4.7936187 29.10985   39.31641   29.375599   8.518871\n",
      " 18.406406   4.7633095]\n",
      "[46.2014432   0.10117579  1.48094058  0.95841718 23.02051592 24.69034958]\n",
      "(134.72148, 37152, 0.0036262241958237022)\n",
      "(16.805676, 31654912, 5.309026133635057e-07)\n",
      "(3.3084896, 74688, 4.4297471629725426e-05)\n",
      "[33.316315   6.6385746  9.829189  14.709235  15.651637  14.662186\n",
      "  3.992069   6.9231467]\n",
      "[28.72758126  0.06050944  0.91778803  0.54868388 14.29030871 24.69034958]\n",
      "(124.41522, 37152, 0.0033488162728243097)\n",
      "(17.26819, 31654912, 5.455137699928255e-07)\n",
      "(10.402693, 74688, 0.00013928198364931188)\n",
      "[29.316883  11.316013  17.368671  17.19335   17.904331   4.0108347\n",
      " 15.559653  20.515827 ]\n",
      "[32.0092473   0.08128762  1.16742659  0.66700172 16.56352592 24.69034958]\n",
      "(177.94714, 37152, 0.004789705629701968)\n",
      "(17.768093, 31654912, 5.613060339302431e-07)\n",
      "(6.5360513, 74688, 8.75113977258187e-05)\n",
      "\n",
      "Average loss for batch 0/0:  17.35805132985115\n",
      "[ 9.543491 15.370214 12.873558 15.238676  8.228424 19.956802 14.177106\n",
      " 16.003042]\n",
      "[10.81557846  0.04029918  0.42307782  0.30176783  5.14381433 18.43240762]\n",
      "(241.39743, 37152, 0.0064975621883053905)\n",
      "(35.34501, 31654912, 1.116572645979798e-06)\n",
      "(17.1477, 74688, 0.00022959109035024504)\n",
      "[19.618807 16.662909 21.728899  9.221638 11.942337 16.358738  7.472277\n",
      " 39.40322 ]\n",
      "[17.41622877  0.06871843  0.61673808  0.38042259  8.72087836 18.43240762]\n",
      "(179.56567, 37152, 0.004833270721041263)\n",
      "(48.8775, 31654912, 1.5440731165737874e-06)\n",
      "(12.095309, 74688, 0.00016194447913329214)\n",
      "[12.078131  35.095745  24.212547   9.552753  20.044296   7.8375077\n",
      " 49.121567  28.24243  ]\n",
      "[23.92666554  0.07083654  0.75347877  0.51697516 11.98569322 18.43240762]\n",
      "(192.95563, 37152, 0.005193680755851805)\n",
      "(22.197472, 31654912, 7.012330856788464e-07)\n",
      "(2.0207422, 74688, 2.7055781088839666e-05)\n",
      "[15.563236   8.806454  17.909634   7.3040934 24.29544   11.884401\n",
      " 48.787025  15.328991 ]\n",
      "[23.88662767  0.06864333  0.75614762  0.40108657 12.01949596 18.43240762]\n",
      "(120.72901, 37152, 0.003249596563728589)\n",
      "(17.925644, 31654912, 5.662831702359001e-07)\n",
      "(4.6083536, 74688, 6.170139265755046e-05)\n",
      "[ 6.798854 24.339945 35.98377  25.755072 21.575003 12.400622 18.133356\n",
      " 14.786003]\n",
      "[21.05809736  0.05854774  0.73734474  0.45194602 10.47012663 18.43240762]\n",
      "(106.25498, 37152, 0.002860007051965679)\n",
      "(15.059288, 31654912, 4.75733056054692e-07)\n",
      "(5.6275005, 74688, 7.534678307167974e-05)\n",
      "[19.674171 24.701805 25.82759  41.913086  7.483859 14.128454 26.422853\n",
      " 15.533868]\n",
      "[28.76387262  0.07108212  1.08885741  0.61710715 14.47929502 18.43240762]\n",
      "(141.39056, 37152, 0.0038057322341958372)\n",
      "(24.802303, 31654912, 7.835214741462237e-07)\n",
      "(3.5471437, 74688, 4.7492819432019166e-05)\n",
      "[10.4068165 16.949022  10.389223  13.972488  13.5838375 13.087388\n",
      " 62.345837  12.78389  ]\n",
      "[36.64825678  0.12242818  1.44709086  0.80151081 18.98338246 18.43240762]\n",
      "(165.9773, 37152, 0.004467519781488884)\n",
      "(19.281199, 31654912, 6.091060528485094e-07)\n",
      "(1.1691102, 74688, 1.5653253252831094e-05)\n",
      "[16.788742  17.934496   6.8575907 13.292742   9.247483  14.680998\n",
      " 12.847932  18.652365 ]\n",
      "[14.96706533  0.04576969  0.53265381  0.31643653  7.24683166 18.43240762]\n",
      "(195.9585, 37152, 0.0052745073237981804)\n",
      "(21.234024, 31654912, 6.707971277207013e-07)\n",
      "(1.6960993, 74688, 2.270912705268631e-05)\n",
      "[12.826431  35.980392  14.814275  18.856617  20.353004   9.2572155\n",
      " 12.581184  17.156273 ]\n",
      "[18.16230774  0.0605824   0.66393995  0.38072491  8.86295247 18.43240762]\n",
      "(235.77824, 37152, 0.0063463136309903825)\n",
      "(33.94814, 31654912, 1.0724445921907422e-06)\n",
      "(3.2094007, 74688, 4.2970767109028375e-05)\n",
      "[26.531712   8.6762495 68.196465  10.712687  33.42545   10.286514\n",
      " 22.56644   20.709198 ]\n",
      "[36.45345163  0.09912634  1.35175037  0.74717307 17.83795857 18.43240762]\n",
      "(127.86086, 37152, 0.0034415606893823643)\n",
      "(17.738668, 31654912, 5.603764888612694e-07)\n",
      "(0.6272383, 74688, 8.398113132238183e-06)\n",
      "[ 6.709777 19.986795 50.432674 15.65192  25.380163  6.985585 32.509193\n",
      " 12.188176]\n",
      "[47.88591552  0.11164212  1.64211965  0.79387283 23.57793379 18.43240762]\n",
      "(180.30554, 37152, 0.004853185346473609)\n",
      "(31.510271, 31654912, 9.954306956338306e-07)\n",
      "(2.8376312, 74688, 3.799313444711249e-05)\n",
      "[13.546511  19.903824  29.256262  11.854738   6.5683312 13.86953\n",
      " 28.206388   9.56387  ]\n",
      "[25.00871015  0.04931712  0.87612653  0.53214979 12.01203442 18.43240762]\n",
      "(95.56319, 37152, 0.002572221862766683)\n",
      "(29.98677, 31654912, 9.473022900800613e-07)\n",
      "(4.5889163, 74688, 6.144114585646014e-05)\n",
      "[36.855167 18.947865 17.927223 19.096752 34.13537  34.599724  6.683988\n",
      " 36.045727]\n",
      "[22.38420177  0.06863546  5.54440618  0.46814942 10.92305231 18.43240762]\n",
      "(171.94315, 37152, 0.004628099315028885)\n",
      "(23.123373, 31654912, 7.304829352113255e-07)\n",
      "(2.9450567, 74688, 3.943145722023115e-05)\n",
      "[18.97022  23.7765    9.063456 13.280317 26.476093 23.145771  8.894751\n",
      "  9.194002]\n",
      "[18.95020247  0.06860018  0.63374448  0.33207202  8.93667531 18.43240762]\n",
      "(153.39209, 37152, 0.0041287707214618325)\n",
      "(23.949207, 31654912, 7.565715964052657e-07)\n",
      "(7.7181187, 74688, 0.00010333813554523536)\n",
      "[23.08045  14.964189 60.6846    9.852968  6.954466  6.415073 15.978451\n",
      " 23.126196]\n",
      "[42.89800072  0.10120535  1.43899083  0.86524224 20.87575364 18.43240762]\n",
      "(111.54871, 37152, 0.0030024955233656876)\n",
      "(12.580398, 31654912, 3.974232373761139e-07)\n",
      "(0.7350218, 74688, 9.841230580616733e-06)\n",
      "[18.57156  34.026962 10.411644 11.55318  44.89856   8.027269 27.132532\n",
      " 14.5912  ]\n",
      "[39.57500172  0.09085536  1.22909021  0.77370501 19.46466899 18.43240762]\n",
      "(102.27234, 37152, 0.0027528084320410074)\n",
      "(19.262175, 31654912, 6.085050751783243e-07)\n",
      "(0.8183445, 74688, 1.0956840106025146e-05)\n",
      "[15.96286  22.211285 13.903988  9.981998 53.65454   6.876742 55.080925\n",
      " 20.258512]\n",
      "[36.32609296  0.10917544  1.18021154  0.57081461 17.66383672 18.43240762]\n",
      "(164.19035, 37152, 0.004419421656803259)\n",
      "(31.194534, 31654912, 9.854563582978153e-07)\n",
      "(1.2510868, 74688, 1.6750841247494036e-05)\n",
      "[31.859743  8.207618 46.024586  9.516148 10.374913 12.296049 35.799217\n",
      "  9.49496 ]\n",
      "[37.67603898  0.08891034  1.35165143  0.8033874  18.59217072 18.43240762]\n",
      "(189.19406, 37152, 0.0050924327433057946)\n",
      "(36.154755, 31654912, 1.1421530610690649e-06)\n",
      "(5.9424276, 74688, 7.956335201361492e-05)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.90473   18.43777   15.439573  22.506157  16.7518    15.0858135\n",
      " 22.269163  23.189175 ]\n",
      "[16.93858147  0.05049586  0.5674181   0.30081296  8.05937719 18.43240762]\n",
      "(149.8244, 37152, 0.004032741221346597)\n",
      "(39.85401, 31654912, 1.2590150791019268e-06)\n",
      "(8.904934, 74688, 0.00011922844271426949)\n",
      "[25.422583 15.186783 11.860341 27.048359 31.924894  7.862547 17.425945\n",
      " 50.922287]\n",
      "[28.00999904  0.07071924  0.93607187  0.581141   13.76459479 18.43240762]\n",
      "(154.45316, 37152, 0.004157330844034726)\n",
      "(52.183884, 31654912, 1.6485240479263435e-06)\n",
      "(10.222727, 74688, 0.0001368724135322865)\n",
      "[25.921692  28.812794  13.164911  25.124147   9.48151    8.490883\n",
      "  7.9666986  8.016422 ]\n",
      "[27.52539539  0.07057095  0.9484148   0.57236362 13.13229489 18.43240762]\n",
      "(181.62727, 37152, 0.0048887616698850755)\n",
      "(30.295643, 31654912, 9.570597717277876e-07)\n",
      "(1.3752387, 74688, 1.8413113980795852e-05)\n",
      "[31.47601   11.454204  38.72441   22.896767  20.227074   6.4823713\n",
      " 22.321598   8.806608 ]\n",
      "[26.13938498  0.07064533  0.84381557  0.47390842 12.11571908 18.43240762]\n",
      "(281.37967, 37152, 0.007573742172412067)\n",
      "(40.88157, 31654912, 1.2914763089119126e-06)\n",
      "(4.220894, 74688, 5.651368171410777e-05)\n",
      "[15.6821375 11.944708   6.7006645  8.161604   7.2753406 24.5611\n",
      "  9.871967  15.864595 ]\n",
      "[14.9523468   0.05049324  0.55421257  0.34965634  7.40643001 18.43240762]\n",
      "(169.56425, 37152, 0.004564068011432552)\n",
      "(24.698124, 31654912, 7.802303772597683e-07)\n",
      "(7.8144298, 74688, 0.00010462764781463218)\n",
      "[11.356684   6.2514296 14.492035  10.625987  34.758278   7.6367774\n",
      " 21.181839  46.22445  ]\n",
      "[39.56433606  0.10306573  1.43106651  0.82149124 19.83038378 18.43240762]\n",
      "(96.701225, 37152, 0.0026028538243099086)\n",
      "(18.473902, 31654912, 5.836030044454783e-07)\n",
      "(1.1932058, 74688, 1.59758707347239e-05)\n",
      "[16.555124 20.830116 17.39344  21.694895 29.131783 35.975742 27.095688\n",
      " 42.825974]\n",
      "[25.52596259  0.070719    0.82662034  0.62307334 12.67549109 18.43240762]\n",
      "(336.07037, 37152, 0.009045821854413121)\n",
      "(44.107616, 31654912, 1.3933893237346717e-06)\n",
      "(2.7495666, 74688, 3.6814033780837526e-05)\n",
      "[11.225971   6.3103447 22.066618  37.36566   11.226834  26.998798\n",
      "  9.781338  27.516758 ]\n",
      "[32.47783279  0.08104324  1.03059244  0.58076549 16.4302423  18.43240762]\n",
      "(376.11926, 37152, 0.010123795830515517)\n",
      "(54.159264, 31654912, 1.7109276314159346e-06)\n",
      "(1.3208861, 74688, 1.7685386341866408e-05)\n",
      "[ 6.0881557 30.36108   15.87434   26.392464   5.5089035 10.794672\n",
      "  7.7712846 32.55731  ]\n",
      "[39.9229238   0.09093618  1.230685    0.66548419 19.6423943  18.43240762]\n",
      "(180.9011, 37152, 0.004869215453523279)\n",
      "(25.271803, 31654912, 7.983532824928301e-07)\n",
      "(3.3584738, 74688, 4.496671189174963e-05)\n",
      "[17.131824   7.7225385 56.217358  19.736876  19.745628   7.483587\n",
      " 36.225445  33.6251   ]\n",
      "[25.67983627  0.07071447  0.8763597   0.5000093  12.03826571 18.43240762]\n",
      "(345.4206, 37152, 0.00929749658865522)\n",
      "(52.671284, 31654912, 1.6639213440847293e-06)\n",
      "(3.6256702, 74688, 4.8544213188542396e-05)\n",
      "[ 6.898283  11.380787   6.2641907 19.289757   8.400001  17.279228\n",
      "  8.004255  27.923077 ]\n",
      "[13.58699489  0.04371262  0.46363449  0.26954651  6.80571342 18.43240762]\n",
      "(208.52272, 37152, 0.005612691654202037)\n",
      "(57.46915, 31654912, 1.8154891898992765e-06)\n",
      "(14.419066, 74688, 0.00019305733757950654)\n",
      "[22.710571  27.70002   15.130796  16.304642   7.3060665  7.254008\n",
      " 17.749178  15.00223  ]\n",
      "[13.8293519   0.03365731  0.44800377  0.26954484  7.02289104 18.43240762]\n",
      "(90.94331, 37152, 0.0024478712747263353)\n",
      "(23.038944, 31654912, 7.278157729323262e-07)\n",
      "(6.851494, 74688, 9.173486819099746e-05)\n",
      "[ 7.121353  46.065826  11.384338  15.375158  12.102201   5.6677237\n",
      " 29.536602  43.111584 ]\n",
      "[67.15225577  0.29227018  2.21088696  1.38807845 29.01787496 18.43240762]\n",
      "(137.66208, 37152, 0.0037053746462484354)\n",
      "(27.507408, 31654912, 8.689775584304212e-07)\n",
      "(1.2045882, 74688, 1.612826926440588e-05)\n",
      "[30.26595  28.35145  13.131527 26.363514  7.791704 17.024925  5.864487\n",
      "  7.869632]\n",
      "[20.56691742  0.06044412  0.78345227  0.53259087 10.2892251  18.43240762]\n",
      "(121.92041, 37152, 0.003281664786720769)\n",
      "(15.957377, 31654912, 5.041043056375218e-07)\n",
      "(8.252026, 74688, 0.00011048663244762274)\n",
      "\n",
      "Average loss for batch 0/13:  19.435960918664932\n",
      "[20.168447   6.6060004 12.520514  19.904484   6.732451   6.672209\n",
      "  7.0009074 20.197716 ]\n",
      "[19.21295691  0.06059551  0.69722867  0.41669464  9.29438281 18.22829747]\n",
      "(85.4605, 37152, 0.002300293459962094)\n",
      "(26.015911, 31654912, 8.218601619330018e-07)\n",
      "(2.563614, 74688, 3.4324307674613894e-05)\n",
      "[ 6.6798754 13.546321  48.48523   10.994802   6.714908  15.424963\n",
      " 14.863783  10.012499 ]\n",
      "[28.07191563  0.08109045  0.97683668  0.58037996 13.63533092 18.22829747]\n",
      "(157.84123, 37152, 0.00424852584786296)\n",
      "(19.484825, 31654912, 6.155387553842305e-07)\n",
      "(6.8827996, 74688, 9.215402240516186e-05)\n",
      "[13.6928005 22.78904   19.739305   7.024224   6.56637   10.67882\n",
      "  4.9512997 13.332388 ]\n",
      "[16.3893702   0.05039215  0.58374763  5.26650071  7.7462852  18.22829747]\n",
      "(90.851974, 37152, 0.0024454127499812846)\n",
      "(8.968686, 31654912, 2.833268373584738e-07)\n",
      "(0.82952696, 74688, 1.1106562779158124e-05)\n",
      "[13.608002 11.174998 11.586503  9.243187 25.286716 31.528355 26.659552\n",
      " 10.575272]\n",
      "[20.58096027  0.05391502  0.64856911  0.37860894  9.90079641 18.22829747]\n",
      "(157.84534, 37152, 0.00424863632951288)\n",
      "(19.039448, 31654912, 6.014689847952769e-07)\n",
      "(5.530831, 74688, 7.405246974263523e-05)\n",
      "[25.924929   4.658248   6.55812    7.4217324  8.124687   6.677662\n",
      " 12.546967   9.088234 ]\n",
      "[20.20676613  0.06062818  0.69498491  0.40742803  9.53829503 18.22829747]\n",
      "(247.26099, 37152, 0.00665538830555892)\n",
      "(28.390583, 31654912, 8.968776485093396e-07)\n",
      "(0.8740155, 74688, 1.1702221375351394e-05)\n",
      "[ 6.2707586  7.3155384  6.5254135  5.4459314 48.153656  15.933285\n",
      "  5.217455   9.074525 ]\n",
      "[24.23579788  0.07075715  0.77901793  0.46059966 11.78564262 18.22829747]\n",
      "(99.35174, 37152, 0.0026741962202862356)\n",
      "(28.648571, 31654912, 9.050276625126709e-07)\n",
      "(1.7499295, 74688, 2.342986219084559e-05)\n",
      "[37.66496   15.028176   7.959753  16.889679  14.203026  11.194721\n",
      "  8.106302   6.8096776]\n",
      "[18.13194942  0.05848837  0.67686105  0.45230627  8.41019583 18.22829747]\n",
      "(99.461525, 37152, 0.0026771512963872446)\n",
      "(13.88316, 31654912, 4.3857836778858117e-07)\n",
      "(1.6175251, 74688, 2.165709485738014e-05)\n",
      "[42.36124   16.474464   7.869593   7.9215364 10.39923    9.971767\n",
      " 11.788521   4.958415 ]\n",
      "[21.17286777  0.05252123  0.74549413  0.48208475  9.55973363 18.22829747]\n",
      "(132.13171, 37152, 0.0035565168461236946)\n",
      "(31.090973, 31654912, 9.821847838462044e-07)\n",
      "(27.769394, 74688, 0.0003718052956418493)\n",
      "[ 23.746487  10.200237  10.195122   7.051824 708.6074    14.928874\n",
      "  16.223068   8.110723]\n",
      "[17.59325671  0.06045532  0.60419488  0.42233086  8.03576517 18.22829747]\n",
      "(105.702515, 37152, 0.0028451365915277103)\n",
      "(72.3415, 31654912, 2.28531670941348e-06)\n",
      "(44.918503, 74688, 0.0006014152582425181)\n",
      "[ 6.6555657 12.648452  35.19168    8.869179  19.110231  37.41452\n",
      " 18.641092  29.344736 ]\n",
      "[28.58212686  0.08085585  1.00483394  0.60271645 13.61344981 18.22829747]\n",
      "(214.32791, 37152, 0.005768946796321951)\n",
      "(55.951065, 31654912, 1.7675318466681116e-06)\n",
      "(28.180105, 74688, 0.0003773043221046297)\n",
      "[14.930433  17.080967  19.509819   6.9825854  7.1033487  7.0631733\n",
      " 45.41972    4.4202747]\n",
      "[26.83689451  0.07274461  0.96661544  0.53203607 13.0760808  18.22829747]\n",
      "(169.04544, 37152, 0.004550103377310189)\n",
      "(24.90009, 31654912, 7.866105981882374e-07)\n",
      "(1.8460283, 74688, 2.4716531811561357e-05)\n",
      "[ 9.064813  26.379326  22.260458  21.844122  32.999825  16.417446\n",
      "  7.4012585 16.317364 ]\n",
      "[34.29712582  0.08097482  1.16825891  0.78365946 16.34780288 18.22829747]\n",
      "(123.8021, 37152, 0.003332313230384741)\n",
      "(28.781492, 31654912, 9.092267333826854e-07)\n",
      "(19.286243, 74688, 0.0002582241248757592)\n",
      "[41.19313   17.402912  24.436699  34.353817   9.181137  16.32741\n",
      " 11.4385395 13.883933 ]\n",
      "[35.23664832  0.07054043  1.05812979  0.617697   16.31425619 18.22829747]\n",
      "(152.76396, 37152, 0.00411186374332451)\n",
      "(22.27341, 31654912, 7.036320554964468e-07)\n",
      "(2.785165, 74688, 3.72906634464362e-05)\n",
      "[19.828138 23.227493  6.100899 49.88319  26.24159  13.554606  9.457171\n",
      "  9.479016]\n",
      "[21.20876312  0.06851959  0.66738057  0.416049   10.10775566 18.22829747]\n",
      "(180.13956, 37152, 0.004848717616407343)\n",
      "(39.475426, 31654912, 1.2470552980913308e-06)\n",
      "(15.01336, 74688, 0.00020101435335661064)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 19.30627    17.226875  710.47577     6.8518558   8.494934   48.951912\n",
      "  32.21505    11.95833  ]\n",
      "[30.02944136  0.06492376  0.93377519  0.56044364 14.67345119 18.22829747]\n",
      "(186.57555, 37152, 0.005021951611343897)\n",
      "(31.055801, 31654912, 9.810736921840618e-07)\n",
      "(13.07141, 74688, 0.0001750135253205091)\n",
      "[10.140709  15.833483  12.702584  11.262122  15.72148   11.210411\n",
      "  7.7037835 18.566324 ]\n",
      "[13.39638925  0.04032803  0.50333285  0.32061267  6.60397267 18.22829747]\n",
      "(110.374306, 37152, 0.0029708846286901824)\n",
      "(23.191113, 31654912, 7.326228712406639e-07)\n",
      "(21.283619, 74688, 0.00028496704861560027)\n",
      "[ 9.294263 27.810467 19.887009 23.610058  8.999241 66.81937  25.725727\n",
      "  6.767634]\n",
      "[29.93950772  0.08094478  1.09622216  0.55767131 14.82489014 18.22829747]\n",
      "(191.44333, 37152, 0.0051529750446119155)\n",
      "(24.924133, 31654912, 7.873701655143205e-07)\n",
      "(6.6531777, 74688, 8.907960767713283e-05)\n",
      "[ 6.9150705 24.557585   8.620681  13.143844  30.46113    8.729795\n",
      "  6.986642   9.540995 ]\n",
      "[39.68931651  0.08505177  1.41471672  0.83303642 19.90940857 18.22829747]\n",
      "(307.7016, 37152, 0.008282235118461826)\n",
      "(239.97565, 31654912, 7.5809923898274065e-06)\n",
      "(4.3077784, 74688, 5.767698102050494e-05)\n",
      "[10.317347  20.033144   8.876685   7.8081617 15.59705   35.322292\n",
      " 15.897801   5.4243336]\n",
      "[51.01518559  0.12118959  1.92766094  1.00687242 26.21469498 18.22829747]\n",
      "(105.99216, 37152, 0.002852932735314973)\n",
      "(17.369925, 31654912, 5.48727620701902e-07)\n",
      "(12.2516365, 74688, 0.00016403754960806226)\n",
      "[ 5.391721  9.0968    8.404276 10.223869 24.989511  5.213372 15.255248\n",
      " 43.281723]\n",
      "[41.84867287  0.09113932  1.45179844  0.75624418 21.66197658 18.22829747]\n",
      "(83.50017, 37152, 0.0022475282043141606)\n",
      "(17.716516, 31654912, 5.596766939282907e-07)\n",
      "(14.255342, 74688, 0.00019086521971195094)\n",
      "[14.190499  15.389743  72.25433    8.856194  10.2969055  9.231609\n",
      " 24.98928   11.715298 ]\n",
      "[48.81621075  0.11138415  1.72170281  0.9447546  23.83585596 18.22829747]\n",
      "(257.4937, 37152, 0.006930817005246184)\n",
      "(51.70715, 31654912, 1.6334636945323125e-06)\n",
      "(42.565006, 74688, 0.0005699042182961589)\n",
      "[ 27.41       26.486872  709.595       7.2605805  27.27093    17.933228\n",
      "   8.125013   28.506786 ]\n",
      "[40.61709762  0.09127235  1.46407413  0.8642683  20.97626066 18.22829747]\n",
      "(327.30472, 37152, 0.008809881514254363)\n",
      "(78.912926, 31654912, 2.4929124971257177e-06)\n",
      "(39.011448, 74688, 0.0005223255128868645)\n",
      "[11.493098 16.583668 19.291714  9.036043 19.018059 15.000383 15.472351\n",
      " 16.062677]\n",
      "[20.93477082  0.06050801  0.70095825  0.35897756  9.61513853 18.22829747]\n",
      "(114.636284, 37152, 0.003085601956139958)\n",
      "(19.031464, 31654912, 6.012167597574391e-07)\n",
      "(2.5257938, 74688, 3.381793314611799e-05)\n",
      "[ 9.113138  22.196367   6.966689  21.254627   8.193989   9.143702\n",
      "  6.7315865 38.306038 ]\n",
      "[23.95431781  0.05860257  0.77671909  0.51128006 11.1834898  18.22829747]\n",
      "(86.61225, 37152, 0.002331294446644549)\n",
      "(18.774853, 31654912, 5.931102494515084e-07)\n",
      "(3.246739, 74688, 4.3470690213622654e-05)\n",
      "[11.902623  14.76504    7.4783278  7.177261  14.974091  18.877125\n",
      " 25.423553  14.654247 ]\n",
      "[30.24991703  0.08093429  0.99896812  0.61339355 14.12093544 18.22829747]\n",
      "(94.175606, 37152, 0.002534873109763291)\n",
      "(21.667229, 31654912, 6.844823545467594e-07)\n",
      "(17.936333, 74688, 0.00024015012723110432)\n",
      "[15.90362  21.260504  9.446293 17.548285 11.988299 28.722317 19.336567\n",
      "  8.731284]\n",
      "[57.34490061  0.12953401  1.65552115  0.86308956 25.20420504 18.22829747]\n",
      "(97.5537, 37152, 0.002625799507647111)\n",
      "(14.154158, 31654912, 4.471393772489339e-07)\n",
      "(5.4001346, 74688, 7.230257288247169e-05)\n",
      "[ 9.441608 41.92545  19.081501 22.031796 14.405403 14.087789 13.218753\n",
      " 22.118347]\n",
      "[42.0231297   0.08685112  1.35388422  0.87652111 20.05103612 18.22829747]\n",
      "(261.93222, 37152, 0.007050285865067409)\n",
      "(44.466297, 31654912, 1.4047202895291007e-06)\n",
      "(29.315111, 74688, 0.0003925009527672226)\n",
      "[ 9.166683 17.78241  15.572001 10.503902 17.562479 18.301622 11.81355\n",
      " 18.066639]\n",
      "[25.73500705  0.06858134  0.83932567  0.45490694 12.47234917 18.22829747]\n",
      "(158.70346, 37152, 0.004271733976457778)\n",
      "(22.143814, 31654912, 6.995380080953648e-07)\n",
      "(8.919687, 74688, 0.00011942597567371149)\n",
      "[25.940271 27.49907   6.189005  7.338763 10.330015  8.690159 19.85912\n",
      "  6.323909]\n",
      "[29.74959064  0.07277083  1.19841504  0.78444862 14.34157825 18.22829747]\n",
      "(118.16201, 37152, 0.003180501997008804)\n",
      "(20.439217, 31654912, 6.456886253157024e-07)\n",
      "(6.56513, 74688, 8.790073684881973e-05)\n",
      "[45.172672  16.617744  33.909096  48.46889   18.174429  12.989965\n",
      "  7.7590895  7.4299245]\n",
      "[28.69423842  0.06791329  0.96815586  0.52184296 13.98735976 18.22829747]\n",
      "(186.7583, 37152, 0.005026870714396264)\n",
      "(33.882885, 31654912, 1.0703831676817819e-06)\n",
      "(12.085962, 74688, 0.00016181933236306)\n",
      "[19.093143  11.034588   9.356      7.8182206 55.416306  14.1153755\n",
      " 19.775528  13.285592 ]\n",
      "[22.09691048  0.0810709   0.77070093  0.52547741 10.61187005 18.22829747]\n",
      "(294.73795, 37152, 0.007933299568169698)\n",
      "(74.07601, 31654912, 2.340111122650249e-06)\n",
      "(48.697098, 74688, 0.0006520069861064737)\n",
      "[ 6.6033363  9.019351  12.403112  15.581781   6.1183343  6.0198116\n",
      " 88.96608   28.008785 ]\n",
      "[91.85236859  0.2008543   3.18784642  1.85546017 46.29440594 18.22829747]\n",
      "(822.1087, 37152, 0.02212824891293285)\n",
      "(134.8394, 31654912, 4.2596675437011855e-06)\n",
      "(66.83744, 74688, 0.0008948886098265138)\n",
      "\n",
      "Average loss for batch 0/20:  24.83281970024109\n",
      "[25.42895  10.48517  26.282784  9.768192 10.435292 30.683928 10.567882\n",
      " 10.43594 ]\n",
      "[13.58241916  0.05058455  0.51338196  0.34159303  6.82914138 15.78768158]\n",
      "(393.8795, 37152, 0.010601838019056427)\n",
      "(62.040405, 31654912, 1.959898207059855e-06)\n",
      "(29.432085, 74688, 0.0003940671197144313)\n",
      "[36.32756  17.532715  9.302113 10.043495  9.441063 67.07499  20.07741\n",
      "  9.811284]\n",
      "[41.31260514  0.09901047  1.44131756  0.8572948  20.54087448 15.78768158]\n",
      "(156.74263, 37152, 0.004218955372655114)\n",
      "(99.5034, 31654912, 3.1433795396417764e-06)\n",
      "(11.996269, 74688, 0.0001606184290123476)\n",
      "[17.942028 11.601577 12.664673  9.454911 24.258087 15.058823 33.56788\n",
      " 10.288497]\n",
      "[18.02283549  0.0437541   0.6618731   0.44045305  8.61616421 15.78768158]\n",
      "(111.214485, 37152, 0.0029934992777900793)\n",
      "(31.200897, 31654912, 9.856573670714004e-07)\n",
      "(14.950761, 74688, 0.00020017621092236542)\n",
      "[25.0727    44.86155    5.9243765 17.072699   9.087486  19.007622\n",
      " 19.551466  10.160526 ]\n",
      "[23.61542749  0.07070422  0.82644153  0.51035833 11.30490661 15.78768158]\n",
      "(165.8613, 37152, 0.004464397545419409)\n",
      "(21.140863, 31654912, 6.678541206678793e-07)\n",
      "(1.7324727, 74688, 2.3196131348507774e-05)\n",
      "[13.673989  9.62813  24.047462  9.746159 16.78188   9.422497  7.190466\n",
      " 18.359285]\n",
      "[14.31904149  0.04026818  0.53264189  0.3008101   7.43562269 15.78768158]\n",
      "(86.10557, 37152, 0.002317656328922505)\n",
      "(25.028954, 31654912, 7.906815078887628e-07)\n",
      "(3.2722135, 74688, 4.381176974902116e-05)\n",
      "[23.337093  32.684162  23.914165  23.041933  21.270117  17.84967\n",
      "  6.9155064  9.003679 ]\n",
      "[29.31948733  0.07696176  1.00275421  0.60189247 14.25022388 15.78768158]\n",
      "(116.543976, 37152, 0.003136950253824239)\n",
      "(30.646767, 31654912, 9.681520094763698e-07)\n",
      "(1.8893365, 74688, 2.5296385855682915e-05)\n",
      "[25.708174  14.744848  37.554348  16.02506    8.008053   7.496218\n",
      " 45.868267   6.7917714]\n",
      "[32.34046984  0.08071041  1.15582681  0.62424755 16.11926293 15.78768158]\n",
      "(138.27022, 37152, 0.0037217435910720234)\n",
      "(25.544966, 31654912, 8.069826807295675e-07)\n",
      "(4.394206, 74688, 5.883416408336152e-05)\n",
      "[12.48045    9.229471  18.218151  11.219826   6.8718123  8.290405\n",
      "  7.6065187  8.985244 ]\n",
      "[20.81108522  0.04930449  0.63264513  0.36944127  9.89898014 15.78768158]\n",
      "(217.84407, 37152, 0.00586358932048262)\n",
      "(35.96814, 31654912, 1.1362577677814267e-06)\n",
      "(2.5496464, 74688, 3.4137296186314755e-05)\n",
      "[ 7.4871516 15.33125   15.363375  16.618345  14.286781  10.381324\n",
      " 68.60667   24.398817 ]\n",
      "[42.66649342  0.0889318   1.23482537  0.65064359 19.67409444 15.78768158]\n",
      "(241.89992, 37152, 0.006511087360102795)\n",
      "(34.325317, 31654912, 1.084359905433081e-06)\n",
      "(9.171811, 74688, 0.00012280166966341046)\n",
      "[32.39058   33.08357   15.903916   8.011253   9.437942  17.367683\n",
      " 23.318504   7.5170374]\n",
      "[32.56137252  0.07754636  1.17889833  0.66326308 15.38720679 15.78768158]\n",
      "(189.42421, 37152, 0.005098627519237902)\n",
      "(49.614136, 31654912, 1.5673439794173966e-06)\n",
      "(3.260738, 74688, 4.365812307151853e-05)\n",
      "[96.99332   30.21317   32.17186    6.813464  14.933933  12.48176\n",
      " 38.729263  11.8905325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137.76787567   0.40756392   4.63434315   2.74955797  61.27723503\n",
      "  15.78768158]\n",
      "(237.06874, 37152, 0.006381049226010082)\n",
      "(62.65947, 31654912, 1.9794548664198525e-06)\n",
      "(96.59891, 74688, 0.001293365834815541)\n",
      "[ 32.50599   10.709186   7.96778   35.452736  39.242973  11.437173\n",
      "  11.976629 709.00977 ]\n",
      "[36.82563996  0.08357239  5.51655078  0.74968219 17.2286644  15.78768158]\n",
      "(227.19733, 37152, 0.0061153457865029135)\n",
      "(42.680534, 31654912, 1.3483068397976581e-06)\n",
      "(7.9777884, 74688, 0.00010681486247233479)\n",
      "[ 10.013348  10.582167  50.949593  29.783215   7.587714 709.69604\n",
      "  11.528336  24.84115 ]\n",
      "[31.2634275   0.07506704  1.00143719  0.57652497 15.25194216 15.78768158]\n",
      "(132.07034, 37152, 0.003554864960636793)\n",
      "(37.832897, 31654912, 1.195166714924979e-06)\n",
      "(26.6622, 74688, 0.00035698105355257034)\n",
      "[41.182377  9.834907  9.966293 10.240526  9.48793  22.397526 23.30185\n",
      " 11.964304]\n",
      "[23.70387173  0.06406283  0.81518626  0.51194096 11.12368822 15.78768158]\n",
      "(115.44814, 37152, 0.003107454322926655)\n",
      "(137.35551, 31654912, 4.339153257679793e-06)\n",
      "(97.63255, 74688, 0.0013072053489260114)\n",
      "[ 18.336662  20.012794   9.818467  45.757378 709.8255    18.442924\n",
      "  84.71889   18.864183]\n",
      "[39.84066296  0.09311438  1.3287499   0.83375669 18.94732738 15.78768158]\n",
      "(200.64523, 37152, 0.005400657653808594)\n",
      "(37.35204, 31654912, 1.1799760914564602e-06)\n",
      "(3.0694723, 74688, 4.109726211609959e-05)\n",
      "[ 23.579592   12.683968  710.512       6.4686933  30.616371   14.108434\n",
      "   6.2838964   6.5892887]\n",
      "[15.74030066  0.04929924  0.50817418  0.30930614  7.70610809 15.78768158]\n",
      "(116.88575, 37152, 0.003146149596707971)\n",
      "(18.176044, 31654912, 5.741934921225283e-07)\n",
      "(11.527181, 74688, 0.00015433778748516353)\n",
      "[ 8.101534 25.688814 20.362026 17.899624 51.62217   9.235106  8.623541\n",
      " 12.687637]\n",
      "[21.79748917  0.06055212  0.75719595  0.4698782  10.47749519 15.78768158]\n",
      "(131.80383, 37152, 0.003547691456928631)\n",
      "(22.1774, 31654912, 7.00599028327397e-07)\n",
      "(3.3516347, 74688, 4.487514380930629e-05)\n",
      "[ 45.287445  18.320179  19.691277  19.826788  33.759293 713.983\n",
      "  30.959814  26.65456 ]\n",
      "[24.0123775   0.07079005  0.85382271  0.55500913 11.26168942 15.78768158]\n",
      "(150.71146, 37152, 0.004056617579102824)\n",
      "(68.68854, 31654912, 2.1699171868699635e-06)\n",
      "(21.698639, 74688, 0.0002905237644068073)\n",
      "[15.99155  51.247868 39.879692  9.677182 19.438393 40.62276  17.108105\n",
      " 13.922958]\n",
      "[36.88456225  0.09894228  1.23817229  0.73749852 17.39390993 15.78768158]\n",
      "(242.00613, 37152, 0.0065139463294897485)\n",
      "(199.72626, 31654912, 6.309487049726082e-06)\n",
      "(37.55547, 74688, 0.0005028313720134353)\n",
      "[  9.469122  13.138931  18.28299  714.71124   13.172907  45.820747\n",
      "  33.081482  16.070595]\n",
      "[20.31980443  0.0627284   0.65137625  0.34548569  9.87730265 15.78768158]\n",
      "(151.75156, 37152, 0.004084613382764976)\n",
      "(29.690548, 31654912, 9.379444158023637e-07)\n",
      "(5.722356, 74688, 7.661680380503337e-05)\n",
      "[10.234533  7.993068 15.267618 31.891575 15.85334   8.224646 16.253159\n",
      " 25.369888]\n",
      "[15.13406658  0.04842162  0.48934221  0.29098177  7.33681774 15.78768158]\n",
      "(97.75284, 37152, 0.002631159510518024)\n",
      "(24.241165, 31654912, 7.657947417807642e-07)\n",
      "(16.26635, 74688, 0.0002177906730998349)\n",
      "[16.376783 11.611916  5.708687 48.407494 13.127619 11.976081 11.103412\n",
      "  7.602871]\n",
      "[29.90365052  0.08642936  1.03218269  0.56558776 14.66905189 15.78768158]\n",
      "(128.33331, 37152, 0.0034542773737155806)\n",
      "(27.701565, 31654912, 8.751110977284777e-07)\n",
      "(3.2852278, 74688, 4.39860188460493e-05)\n",
      "[  7.2627997  16.953886   15.438825    6.7197046   7.332558   22.135782\n",
      " 712.52057    13.731375 ]\n",
      "[17.39074659  0.05052352  0.59214973  0.40107179  8.26953554 15.78768158]\n",
      "(110.96585, 37152, 0.0029868069237208796)\n",
      "(20.052013, 31654912, 6.334566147969958e-07)\n",
      "(10.306101, 74688, 0.00013798871097548353)\n",
      "[12.835262  18.245876  31.185186   6.6451488 22.978266  16.374804\n",
      " 20.207668  40.045876 ]\n",
      "[31.9632082   0.08306646  1.14636421  0.67018414 15.58032513 15.78768158]\n",
      "(120.18467, 37152, 0.0032349448076719666)\n",
      "(85.929436, 31654912, 2.714568776244915e-06)\n",
      "(2.7728734, 74688, 3.712608988916353e-05)\n",
      "[13.860086   5.5064034 16.177212  25.959837  12.914734  16.491663\n",
      " 10.729113  22.473331 ]\n",
      "[ 9.55056572  0.03819585  0.3622973   0.21740985  4.60874724 15.78768158]\n",
      "(114.66626, 37152, 0.0030864088007543336)\n",
      "(50.115322, 31654912, 1.5831767945852166e-06)\n",
      "(7.342516, 74688, 9.830917878955884e-05)\n",
      "[10.11151    6.243478  38.291008  10.736988  24.256401   7.2531605\n",
      "  6.2422967  6.3033032]\n",
      "[16.3380878   0.05847716  0.6154387   0.39429116  8.09463406 15.78768158]\n",
      "(104.81901, 37152, 0.002821355724416859)\n",
      "(23.93924, 31654912, 7.562567067617539e-07)\n",
      "(1.2678473, 74688, 1.6975247691406996e-05)\n",
      "[23.012344 19.571978  5.754329 23.172749  6.655588 12.066532 12.088141\n",
      "  6.017728]\n",
      "[13.6202662   0.04837513  0.50545549  0.3315959   6.69827271 15.78768158]\n",
      "(94.514336, 37152, 0.002543990515512603)\n",
      "(25.393166, 31654912, 8.021872115259365e-07)\n",
      "(3.8818789, 74688, 5.1974599036582024e-05)\n",
      "[21.42448    5.467307  30.68594    5.4517846 21.402401  16.792643\n",
      "  5.4466624 12.760115 ]\n",
      "[25.87157393  0.06241083  0.8785193   0.51965523 12.56066942 15.78768158]\n",
      "(137.31775, 37152, 0.0036961065090287872)\n",
      "(27.038467, 31654912, 8.541634046313748e-07)\n",
      "(3.2361863, 74688, 4.332940051876385e-05)\n",
      "[ 7.881695  46.072823   4.9042215 10.636898   4.9051437 10.668545\n",
      " 20.11813    4.9652886]\n",
      "[28.51740766  0.06630731  0.94661045  0.59621286 13.47615528 15.78768158]\n",
      "(121.5024, 37152, 0.0032704135244206864)\n",
      "(27.794102, 31654912, 8.780344015831694e-07)\n",
      "(1.6905564, 74688, 2.2634913332460267e-05)\n",
      "[ 22.944351    4.001251   16.070545    3.9584236 710.0981     11.216045\n",
      "   3.9581487  25.729332 ]\n",
      "[20.54153228  0.06057262  0.70178509  0.44617414  9.83095598 15.78768158]\n",
      "(108.780846, 37152, 0.0029279943379115483)\n",
      "(27.81648, 31654912, 8.787413667931435e-07)\n",
      "(9.683678, 74688, 0.00012965506739154675)\n",
      "[19.015682  29.875895  30.322445   9.957643   3.4233317  4.700316\n",
      " 17.424423  17.922224 ]\n",
      "[19.95880294  0.05047083  0.6946044   0.3793335   9.75226331 15.78768158]\n",
      "(101.36642, 37152, 0.0027284242283363573)\n",
      "(26.884768, 31654912, 8.493079220169253e-07)\n",
      "(21.658957, 74688, 0.0002899924556516436)\n",
      "[ 6.3489237 21.956978  12.407295   3.0167801 10.254761   7.4207153\n",
      " 34.061787  14.764154 ]\n",
      "[18.29887462  0.05877399  0.65451837  0.38274503  8.96523809 15.78768158]\n",
      "(114.51785, 37152, 0.0030824142114341925)\n",
      "(39.0393, 31654912, 1.2332777614807607e-06)\n",
      "(36.3218, 74688, 0.00048631373489628314)\n",
      "\n",
      "Average loss for batch 0/16:  39.99034237861633\n",
      "[12.897191  16.237194  23.880283  13.264254  13.439907  32.348602\n",
      " 13.705415   6.9113235]\n",
      "[15.50377989  0.05291057  0.57409334  0.34940791  7.39299297 18.46889377]\n",
      "(121.247345, 37152, 0.0032635482604086758)\n",
      "(18.648468, 31654912, 5.891176705080755e-07)\n",
      "(6.098509, 74688, 8.165312814426504e-05)\n",
      "[60.502747 28.562164 13.669871 24.232693 19.061958 26.796766 47.097393\n",
      " 39.762924]\n",
      "[53.86251259  0.14189577  1.87354946  1.19509244 26.27277637 18.46889377]\n",
      "(208.03944, 37152, 0.005599683569383252)\n",
      "(39.946045, 31654912, 1.261922475787486e-06)\n",
      "(9.352601, 74688, 0.00012522227200260507)\n",
      "[17.642601 23.456287 23.09994   6.606143 12.640642 11.688648 22.970833\n",
      " 35.78378 ]\n",
      "[22.99286294  0.07106495  0.8437674   0.54249239 10.64668059 18.46889377]\n",
      "(172.06886, 37152, 0.004631483174931068)\n",
      "(26.760729, 31654912, 8.453894560205876e-07)\n",
      "(1.8738444, 74688, 2.508896188339619e-05)\n",
      "[18.082973   6.4242296  9.474721  18.322332  14.66413   14.083885\n",
      " 15.905443   9.512651 ]\n",
      "[12.0349431   0.0440402   0.47943568  0.27024603  5.7252059  18.46889377]\n",
      "(126.19872, 37152, 0.00339682178185173)\n",
      "(22.880943, 31654912, 7.228244165815354e-07)\n",
      "(3.6278796, 74688, 4.857379524954998e-05)\n",
      "[14.121911  18.727821  13.649486  27.193506  14.17159   17.504766\n",
      " 12.759476   7.8047576]\n",
      "[21.49757576  0.06251645  0.74954653  0.46802926 10.05088329 18.46889377]\n",
      "(115.52078, 37152, 0.0031094095195602694)\n",
      "(17.49429, 31654912, 5.526563902055188e-07)\n",
      "(5.8973083, 74688, 7.895924846842029e-05)\n",
      "[27.02092    7.0019264 18.397606  13.396286  17.291664  19.351656\n",
      " 14.3284645 37.018585 ]\n",
      "[20.46682167  0.0568943   0.72176433  0.41985083  9.36912179 18.46889377]\n",
      "(126.0232, 37152, 0.0033920973565021947)\n",
      "(30.849136, 31654912, 9.74545004343688e-07)\n",
      "(9.4418125, 74688, 0.00012641672712160974)\n",
      "[12.637676 13.159457  7.174156 14.413723 26.622322 28.280506 26.339308\n",
      " 19.171839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.49155855  0.05246377  0.67255998  0.40091705  8.92896581 18.46889377]\n",
      "(128.0685, 37152, 0.0034471494590897277)\n",
      "(18.730366, 31654912, 5.91704875160412e-07)\n",
      "(4.4579124, 74688, 5.968713106614663e-05)\n",
      "[ 7.347961 12.288617  9.419021  6.041459 21.384033 11.755468 11.672304\n",
      " 12.222061]\n",
      "[16.63149643  0.04238439  0.56363654  0.35571742  8.03808355 18.46889377]\n",
      "(146.7171, 37152, 0.003949103737370297)\n",
      "(39.906105, 31654912, 1.2606607480540115e-06)\n",
      "(6.1796274, 74688, 8.273922743302895e-05)\n",
      "[32.549812   9.687704   9.997201  13.198854   9.7294855 58.893806\n",
      " 17.490993  16.614304 ]\n",
      "[48.50975418  0.15657353  1.7082634   1.94102383 24.049227   18.46889377]\n",
      "(246.52979, 37152, 0.006635706964800011)\n",
      "(36.504627, 31654912, 1.1532057719125299e-06)\n",
      "(1.2568104, 74688, 1.6827474650707424e-05)\n",
      "[15.174288  15.126748   7.8383484 16.496807  20.184029   8.504979\n",
      " 38.281044  16.35395  ]\n",
      "[20.75635433  0.06065774  0.82936454  0.49075007 10.2320087  18.46889377]\n",
      "(152.80327, 37152, 0.004112921738604037)\n",
      "(21.510712, 31654912, 6.795378761413671e-07)\n",
      "(4.2234616, 74688, 5.6548061642569156e-05)\n",
      "[17.331057  9.441336 38.923477 19.338728  9.217583  9.575635 34.186306\n",
      " 16.282284]\n",
      "[26.18886852  0.07075453  0.91461205  0.5726254  12.80661798 18.46889377]\n",
      "(122.32956, 37152, 0.0032926776304417495)\n",
      "(23.774193, 31654912, 7.510427705519634e-07)\n",
      "(13.575572, 74688, 0.00018176376411009775)\n",
      "[11.938509 14.524949 31.847023 27.214382 32.62455  19.273842 22.0743\n",
      " 14.507547]\n",
      "[23.75848937  0.05934525  0.81783223  0.48571157 11.67236471 18.46889377]\n",
      "(104.99509, 37152, 0.0028260951407709377)\n",
      "(13.541231, 31654912, 4.277766166399549e-07)\n",
      "(1.5456027, 74688, 2.0694123276197307e-05)\n",
      "[ 7.266201  38.97222   37.66839    6.4624395 17.4824    14.389845\n",
      " 26.455753  23.29867  ]\n",
      "[26.9284718   0.06717253  0.93653035  0.46911573 13.16598272 18.46889377]\n",
      "(123.91646, 37152, 0.0033353913148654935)\n",
      "(23.949245, 31654912, 7.565728014938364e-07)\n",
      "(5.316686, 74688, 7.118527947477326e-05)\n",
      "[ 6.541668 36.8268   12.121959 11.066668 33.858074 21.2491   17.22921\n",
      " 32.74507 ]\n",
      "[23.00173044  0.06077409  0.8083148   0.47877645 11.13252258 18.46889377]\n",
      "(104.95663, 37152, 0.002825059940032565)\n",
      "(26.924278, 31654912, 8.505560925039799e-07)\n",
      "(1.5304204, 74688, 2.0490847559902743e-05)\n",
      "[26.537563  30.765408  11.118475  25.497208  13.46785    5.3274145\n",
      " 12.076136  13.625336 ]\n",
      "[19.70537925  0.05271029  0.71179843  0.41313052  9.43678093 18.46889377]\n",
      "(129.40617, 37152, 0.0034831549770148224)\n",
      "(23.84493, 31654912, 7.532774265429552e-07)\n",
      "(5.309722, 74688, 7.109203549052469e-05)\n",
      "[30.404526 14.329851  7.126321 34.156696 23.645151 22.695177  9.657496\n",
      " 10.024022]\n",
      "[26.84749722  0.07064652  0.95240092  0.58715177 12.90137148 18.46889377]\n",
      "(83.19559, 37152, 0.002239329973035183)\n",
      "(15.294664, 31654912, 4.831687538077683e-07)\n",
      "(13.576197, 74688, 0.00018177212765815428)\n",
      "[13.562965 32.23204  13.28135  27.921219 10.872545 13.52398   9.135555\n",
      " 37.80798 ]\n",
      "[28.84201288  0.06859136  0.97675729  0.62492204 13.80261612 18.46889377]\n",
      "(109.359436, 37152, 0.0029435679380694513)\n",
      "(12.3238, 31654912, 3.8931714885118296e-07)\n",
      "(2.9932766, 74688, 4.007707524728653e-05)\n",
      "[40.523193  9.601757 47.02974  31.399887 38.85103  37.388256 10.267012\n",
      " 21.527216]\n",
      "[34.81352425  0.09294724  1.19414902  0.67951822 16.49096918 18.46889377]\n",
      "(166.37326, 37152, 0.004478177769650271)\n",
      "(19.89185, 31654912, 6.283969299242489e-07)\n",
      "(6.8531213, 74688, 9.175665810665925e-05)\n",
      "[19.723705 12.063926 15.433912 49.446644 26.502613 21.088512 10.33754\n",
      " 40.855843]\n",
      "[30.89963031  0.08713198  1.11772418  0.65959382 15.39272571 18.46889377]\n",
      "(170.35452, 37152, 0.004585339220097925)\n",
      "(82.24181, 31654912, 2.5980743102261023e-06)\n",
      "(8.969734, 74688, 0.00012009605548273526)\n",
      "[30.75214  18.871206  9.202941 62.078224  6.744135  8.272984 17.076761\n",
      " 26.211754]\n",
      "[38.39991117  0.0906291   1.34185219  0.70050693 19.41665435 18.46889377]\n",
      "(192.15794, 37152, 0.005172209940934161)\n",
      "(37.418987, 31654912, 1.1820910218979576e-06)\n",
      "(14.264466, 74688, 0.00019098739135745457)\n",
      "[ 5.799398  24.796652  24.136675   9.340172   6.9911985  5.8348083\n",
      " 36.21987   14.081804 ]\n",
      "[26.51357937  0.06059456  0.80321836  0.44795132 12.8330121  18.46889377]\n",
      "(132.26866, 37152, 0.003560202990391458)\n",
      "(18.914602, 31654912, 5.975250311756698e-07)\n",
      "(7.7060676, 74688, 0.00010317678291162263)\n",
      "[15.433243  9.724752 24.053854 17.136395 21.70456  30.487682 32.068367\n",
      " 13.772019]\n",
      "[23.56444526  0.06105638  0.89516521  0.48169088 11.53584743 18.46889377]\n",
      "(127.31203, 37152, 0.003426787978508265)\n",
      "(33.781445, 31654912, 1.0671785961546993e-06)\n",
      "(10.166243, 74688, 0.00013611614448756567)\n",
      "[18.4307    40.85733    6.67153   13.008949   7.8353004 19.384972\n",
      " 14.372363   6.5258923]\n",
      "[31.22505283  0.07289505  1.0876708   0.59866261 15.76349306 18.46889377]\n",
      "(148.04294, 37152, 0.003984790542431683)\n",
      "(27.81972, 31654912, 8.78843678812792e-07)\n",
      "(6.1924725, 74688, 8.291121007237766e-05)\n",
      "[25.018553 11.024157 25.787806  8.33661  14.41086  20.530972 14.39681\n",
      " 37.2931  ]\n",
      "[20.2870295   0.0576055   0.75896645  0.45323277  9.41858077 18.46889377]\n",
      "(216.44345, 37152, 0.005825889613687941)\n",
      "(31.930922, 31654912, 1.0087193278112866e-06)\n",
      "(1.5601287, 74688, 2.0888612478741644e-05)\n",
      "[31.739758 15.974786 25.721571  9.314842 10.874481 13.710047 16.147537\n",
      "  9.933161]\n",
      "[18.81962657  0.0560534   0.69595385  0.38949728  9.24025178 18.46889377]\n",
      "(147.89119, 37152, 0.003980706007084284)\n",
      "(15.485377, 31654912, 4.891935037351089e-07)\n",
      "(3.772406, 74688, 5.0508864894317784e-05)\n",
      "[ 7.843256 27.269472 39.441284 50.108162  7.202986 37.836834 31.047855\n",
      " 15.083035]\n",
      "[38.50943375  0.09627366  1.1548593   0.62165475 18.94594097 18.46889377]\n",
      "(147.3801, 37152, 0.003966949193463256)\n",
      "(30.383818, 31654912, 9.598452737044251e-07)\n",
      "(4.3194017, 74688, 5.783260685823468e-05)\n",
      "[10.779646 25.194458 38.61502  10.596806  9.261953 10.632346 11.742844\n",
      "  8.850561]\n",
      "[24.35135341  0.06918979  0.92296243  0.53377247 11.22858286 18.46889377]\n",
      "(105.88202, 37152, 0.0028499682128275395)\n",
      "(17.554995, 31654912, 5.545741079024286e-07)\n",
      "(2.6410246, 74688, 3.53607619636163e-05)\n",
      "[ 9.941958 14.427465 10.279321 30.848362  8.942312 23.451157  8.77677\n",
      " 16.089607]\n",
      "[22.2620461   0.0660603   0.81833601  0.47613668 11.07048154 18.46889377]\n",
      "(126.259544, 37152, 0.0033984588816903154)\n",
      "(20.837301, 31654912, 6.582643873491881e-07)\n",
      "(0.7999254, 74688, 1.0710226367096055e-05)\n",
      "[ 8.235068 15.793753  8.000636 16.955551 10.18404   6.594681 15.750776\n",
      "  9.74366 ]\n",
      "[15.20888805  0.05264544  0.5840714   0.4071455   7.68389297 18.46889377]\n",
      "(104.14434, 37152, 0.002803196073297177)\n",
      "(15.414805, 31654912, 4.869640898793995e-07)\n",
      "(2.7427208, 74688, 3.6722376316964574e-05)\n",
      "[25.483053  27.978302   7.9127383  7.914412   5.8775353  9.115866\n",
      "  7.925687   8.987468 ]\n",
      "[15.88459349  0.04031825  0.51416564  0.25397658  7.38619709 18.46889377]\n",
      "(121.00626, 37152, 0.0032570592090038265)\n",
      "(18.06841, 31654912, 5.707932744660003e-07)\n",
      "(7.9613104, 74688, 0.0001065942371821138)\n",
      "[27.120417   7.6095095 16.756882   9.262825   9.411596  13.006494\n",
      "  6.084642  15.735746 ]\n",
      "[14.49506354  0.05045605  0.53401947  0.35598445  6.92149544 18.46889377]\n",
      "(108.605995, 37152, 0.0029232879839099554)\n",
      "(12.78815, 31654912, 4.0398627024075125e-07)\n",
      "(1.726386, 74688, 2.3114636233962286e-05)\n",
      "[15.756411   9.315771  15.675006  39.83719   15.365748   5.4612575\n",
      "  8.776981   4.6217175]\n",
      "[28.19020462  0.06492949  0.99205256  0.59227943 13.86582661 18.46889377]\n",
      "(110.022064, 37152, 0.002961403537063533)\n",
      "(14.3461075, 31654912, 4.5320320217317793e-07)\n",
      "(3.8328018, 74688, 5.1317505072403284e-05)\n",
      "\n",
      "Average loss for batch 0/15:  18.733985275030136\n",
      "[13.076378  17.95813   21.290783   8.7177925 15.727089   7.08521\n",
      " 33.096848  26.478779 ]\n",
      "[14.11917877  0.04596019  0.50864768  0.29205871  6.68312073 19.95447159]\n",
      "(180.31071, 37152, 0.004853324577995254)\n",
      "(27.679249, 31654912, 8.744061209146451e-07)\n",
      "(1.002478, 74688, 1.3422209772679575e-05)\n",
      "[ 9.412263 18.68387  13.494135  8.125415  8.891296 59.278778  5.567379\n",
      "  8.401087]\n",
      "[38.0165112   0.08887792  1.28251314  0.78620124 18.75176072 19.95447159]\n",
      "(154.75237, 37152, 0.004165384504530165)\n",
      "(26.279268, 31654912, 8.301797921526526e-07)\n",
      "(3.4688897, 74688, 4.6445074353140446e-05)\n",
      "[ 14.058708   25.862698    7.4538317  11.261312   11.097374  710.7403\n",
      "  10.752382   42.228966 ]\n",
      "[23.76254582  0.0593915   0.70772576  0.50445843 11.37420249 19.95447159]\n",
      "(116.675224, 37152, 0.0031404829969907196)\n",
      "(22.982824, 31654912, 7.260429068816088e-07)\n",
      "(2.2801628, 74688, 3.0529172173298214e-05)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.43528    7.627506   5.8433647 55.036297  21.755272   8.0596695\n",
      "  9.608751  30.725142 ]\n",
      "[48.705585    0.1116097   1.50124025  0.86652088 23.0729301  19.95447159]\n",
      "(267.22018, 37152, 0.007192619087160096)\n",
      "(43.31566, 31654912, 1.3683708414459007e-06)\n",
      "(2.9535325, 74688, 3.9544939713898946e-05)\n",
      "[20.161863  23.062447  10.150178   6.935674  13.387985  60.357365\n",
      "  7.0347514 23.251982 ]\n",
      "[30.84256816  0.07750058  1.99832582  0.58873725 14.13129735 19.95447159]\n",
      "(298.17136, 37152, 0.008025714798696487)\n",
      "(36.047394, 31654912, 1.138761459795817e-06)\n",
      "(4.8550863, 74688, 6.50049047584501e-05)\n",
      "[ 6.3508654 20.575188  37.873962  11.897501  20.72468   10.245124\n",
      "  6.6351347 13.4135275]\n",
      "[25.29577637  0.08808064  0.73260093  0.41552711 11.58589721 19.95447159]\n",
      "(134.1307, 37152, 0.003610322641771893)\n",
      "(24.686895, 31654912, 7.798756594389964e-07)\n",
      "(2.375534, 74688, 3.180610081428325e-05)\n",
      "[ 7.466657   7.771383  12.761274  24.611755   6.91741   11.1484585\n",
      " 19.111572   8.5270605]\n",
      "[16.12767172  0.05438137  0.55176806  0.35002303  7.17158914 19.95447159]\n",
      "(65.49332, 37152, 0.0017628476703916512)\n",
      "(10.853769, 31654912, 3.428778858196846e-07)\n",
      "(7.4672794, 74688, 9.997964109634883e-05)\n",
      "[ 8.222145   6.7003465 15.537853   6.5726743  6.8967986  8.689818\n",
      "  8.183067   8.1391945]\n",
      "[11.89550829  0.04098749  0.41216421  0.26118612  5.34764528 19.95447159]\n",
      "(73.10657, 37152, 0.001967769363232464)\n",
      "(15.672412, 31654912, 4.951020529970242e-07)\n",
      "(8.4761095, 74688, 0.00011348689889540096)\n",
      "[20.111458  47.1062     8.150026  12.114372  15.783086   6.9117393\n",
      " 19.655617  10.6665325]\n",
      "[23.94214797  0.05463552  0.79633498  0.49150944 11.5093689  19.95447159]\n",
      "(127.75663, 37152, 0.003438755112614332)\n",
      "(16.751957, 31654912, 5.292056076383111e-07)\n",
      "(8.483357, 74688, 0.00011358394159040802)\n",
      "[16.478868 17.466345  9.948402 28.687044 46.035477 11.835125 20.338398\n",
      "  8.030971]\n",
      "[26.35290885  0.0538187   0.76709366  0.41994143 12.16234779 19.95447159]\n",
      "(148.18048, 37152, 0.003988492704485122)\n",
      "(24.21822, 31654912, 7.650698810055159e-07)\n",
      "(4.9792047, 74688, 6.66667289885069e-05)\n",
      "[ 6.9569325 24.440828   7.8625894 18.727743   5.0632014 13.733708\n",
      "  5.3472857 16.524958 ]\n",
      "[30.03157306  0.07943177  1.16716361  0.61253119 14.5215497  19.95447159]\n",
      "(89.67626, 37152, 0.002413766739391028)\n",
      "(25.096527, 31654912, 7.928162017828189e-07)\n",
      "(6.8209805, 74688, 9.132632482940556e-05)\n",
      "[14.934599  20.630539  12.373492   6.6625457 13.054076  24.801256\n",
      "  8.121736   8.34925  ]\n",
      "[37.09989882  0.10131502  1.28263879  0.74004865 17.93673253 19.95447159]\n",
      "(94.266556, 37152, 0.002537321161340784)\n",
      "(17.37206, 31654912, 5.487951056618586e-07)\n",
      "(2.7409856, 74688, 3.669914352965335e-05)\n",
      "[12.008945  10.676426  26.730337  11.50049   16.029911   7.5127954\n",
      " 15.983219  11.048585 ]\n",
      "[24.80122757  0.07106686  0.86345696  0.53143454 12.00552917 19.95447159]\n",
      "(147.38547, 37152, 0.003967093764246794)\n",
      "(28.697002, 31654912, 9.065576429619729e-07)\n",
      "(20.972118, 74688, 0.00028079635788460724)\n",
      "[12.625721 10.282885 17.887497  8.138765  8.476072 10.663385 16.307251\n",
      "  5.016626]\n",
      "[31.22641945  0.09001803  1.22725177  0.79608512 16.09265685 19.95447159]\n",
      "(91.860794, 37152, 0.0024725665931143092)\n",
      "(20.078018, 31654912, 6.342781236756104e-07)\n",
      "(10.980008, 74688, 0.00014701167691336193)\n",
      "[ 5.9780045 12.892382  16.863823  23.289463   7.3255744 50.187965\n",
      "  4.910513  45.975597 ]\n",
      "[90.29068708  0.18321252  2.89180374  1.76970148 42.64193606 19.95447159]\n",
      "(185.91377, 37152, 0.005004139012247196)\n",
      "(38.979298, 31654912, 1.2313822776679795e-06)\n",
      "(5.432923, 74688, 7.274157615839771e-05)\n",
      "[ 8.542947  16.022045   7.8293905  8.554904  14.474066   8.206774\n",
      " 16.070684  15.655603 ]\n",
      "[31.05941868  0.07498717  1.01942706  0.71295857 14.79714608 19.95447159]\n",
      "(112.76848, 37152, 0.0030353272608084274)\n",
      "(14.011514, 31654912, 4.426331594294741e-07)\n",
      "(1.2366107, 74688, 1.6557019213477872e-05)\n",
      "[ 4.544503  36.735527  25.130701   7.9888616 11.162726  10.132657\n",
      "  7.786786   8.375488 ]\n",
      "[20.85108805  0.06846356  0.72674131  0.44460797  9.99395657 19.95447159]\n",
      "(193.43002, 37152, 0.0052064498060228085)\n",
      "(43.595947, 31654912, 1.377225350227636e-06)\n",
      "(8.494623, 74688, 0.00011373477913726571)\n",
      "[15.013766  15.308037  10.430448  19.79954   15.881916  15.984867\n",
      "  6.1347585 12.445277 ]\n",
      "[22.90123892  0.06342912  0.765064    0.4720602  10.69418812 19.95447159]\n",
      "(101.2239, 37152, 0.002724588174023164)\n",
      "(17.339355, 31654912, 5.477619229758086e-07)\n",
      "(10.818718, 74688, 0.0001448521577300633)\n",
      "[14.900627 10.053329  6.572626 15.740573  6.915242 19.50922   9.499857\n",
      "  9.901781]\n",
      "[32.24043179  0.08089232  0.96874261  0.63443017 15.68927336 19.95447159]\n",
      "(136.748, 37152, 0.0036807709167375325)\n",
      "(31.134605, 31654912, 9.835631641533182e-07)\n",
      "(5.9614077, 74688, 7.981747618677683e-05)\n",
      "[15.589889   7.5292916  7.506399   6.7141695  7.3891416  7.759511\n",
      " 22.138088  18.086163 ]\n",
      "[18.87823415  0.05934477  0.60165691  0.33555269  8.87179255 19.95447159]\n",
      "(140.60536, 37152, 0.0037845973820649376)\n",
      "(19.262224, 31654912, 6.085066417934662e-07)\n",
      "(9.703754, 74688, 0.00012992387565671632)\n",
      "[31.230297  17.340576   6.2670875 20.506296  14.763835   6.393616\n",
      " 14.979808  14.316589 ]\n",
      "[19.11609364  0.0513463   0.64136147  0.38796806  9.59005022 19.95447159]\n",
      "(124.33716, 37152, 0.003346715067913571)\n",
      "(19.933712, 31654912, 6.297193941216844e-07)\n",
      "(2.557901, 74688, 3.424781632402971e-05)\n",
      "[19.660503  5.87703  21.334799 21.891678 19.414825  7.302308 13.68763\n",
      "  5.720502]\n",
      "[29.50373769  0.0695703   1.00207686  0.53229475 14.99392247 19.95447159]\n",
      "(105.64525, 37152, 0.0028435951876907284)\n",
      "(22.33667, 31654912, 7.056304538731619e-07)\n",
      "(3.2847066, 74688, 4.3979040710151755e-05)\n",
      "[ 5.8475857  5.771694   6.290625  12.736158  16.422337   5.5365453\n",
      " 32.417545  33.571655 ]\n",
      "[34.25481367  0.08872414  1.13641524  0.75154352 16.60945344 19.95447159]\n",
      "(114.76292, 37152, 0.003089010458789336)\n",
      "(16.441164, 31654912, 5.193874497810524e-07)\n",
      "(9.979487, 74688, 0.00013361567345662514)\n",
      "[15.316851  8.283833 21.192816 19.271914 32.52381  20.750444 20.402067\n",
      " 22.643494]\n",
      "[20.79436851  0.05154729  0.65608549  0.3594389  10.0647881  19.95447159]\n",
      "(168.04642, 37152, 0.004523213211572139)\n",
      "(31.989498, 31654912, 1.010569801565954e-06)\n",
      "(2.587577, 74688, 3.464515189278708e-05)\n",
      "[10.449937  39.40638   18.557678   9.0036335 20.579887  20.716295\n",
      " 11.759409  33.176636 ]\n",
      "[28.66685724  0.07506895  1.00520182  0.61446142 13.79033732 19.95447159]\n",
      "(205.19489, 37152, 0.005523118143139165)\n",
      "(131.51039, 31654912, 4.154501874317375e-06)\n",
      "(6.5028257, 74688, 8.70665399662531e-05)\n",
      "[21.98984   18.908083   5.0759597 15.429048   5.0751386 11.682932\n",
      "  5.1698065  5.0423903]\n",
      "[20.58185005  0.05235934  0.71476197  0.44181538 10.11421967 19.95447159]\n",
      "(109.16325, 37152, 0.0029382874080582386)\n",
      "(18.749357, 31654912, 5.92304828505312e-07)\n",
      "(8.231742, 74688, 0.00011021505335813521)\n",
      "[14.839279  19.208498   5.2737465 47.239388  17.39085   17.262224\n",
      " 14.065327  19.638802 ]\n",
      "[33.43293881  0.07815528  1.14967108  0.69344783 15.76104712 19.95447159]\n",
      "(182.7129, 37152, 0.004917983039507673)\n",
      "(26.655716, 31654912, 8.420720279488635e-07)\n",
      "(2.7495043, 74688, 3.681320061822579e-05)\n",
      "[28.909134   4.7254972  4.3879533 41.234375   8.95247   33.953915\n",
      "  4.285592  18.89426  ]\n",
      "[30.41711187  0.06856322  1.02573252  5.31042027 14.29254007 19.95447159]\n",
      "(111.89961, 37152, 0.0030119404722964527)\n",
      "(15.80779, 31654912, 4.993787315709887e-07)\n",
      "(10.869768, 74688, 0.00014553567029108018)\n",
      "[ 3.271953  19.938517  35.277935   8.553932   3.6532311  3.8928533\n",
      " 13.074909  20.869    ]\n",
      "[29.41348958  0.06266618  1.05454063  0.65317464 13.78888941 19.95447159]\n",
      "(210.76793, 37152, 0.005673124705995598)\n",
      "(25.948046, 31654912, 8.197162491113802e-07)\n",
      "(5.751479, 74688, 7.700673667610254e-05)\n",
      "[ 3.1740677 18.727999   3.2036521 19.14843   13.083282   6.2732935\n",
      " 22.758467   3.9169946]\n",
      "[44.74460626  0.09886432  1.52332401  0.84735417 22.11972976 19.95447159]\n",
      "(125.338264, 37152, 0.0033736612959014865)\n",
      "(21.220684, 31654912, 6.703757082475437e-07)\n",
      "(19.269587, 74688, 0.0002580011054401022)\n",
      "[20.795883   2.7073722  2.3628924 14.484247   5.974651   6.1763563\n",
      " 10.237817   2.3291245]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_sum\n\u001b[0;32m     25\u001b[0m timings_temp \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 26\u001b[0m grad_decode \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestSpellDecode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m grad_cogitate \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_, testSpellCogitate\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     28\u001b[0m grad_attend \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_, testSpellAttend\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1107\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1108\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1109\u001b[0m           output_gradients))\n\u001b[0;32m   1110\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1113\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:635\u001b[0m, in \u001b[0;36m_aggregate_grads\u001b[1;34m(gradients)\u001b[0m\n\u001b[0;32m    627\u001b[0m   concat_grad \u001b[38;5;241m=\u001b[39m indexed_slices\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[0;32m    628\u001b[0m       array_ops\u001b[38;5;241m.\u001b[39mconcat([x\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m grads], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    629\u001b[0m       array_ops\u001b[38;5;241m.\u001b[39mconcat([x\u001b[38;5;241m.\u001b[39mindices \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m grads], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    630\u001b[0m       grads[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[0;32m    632\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concat_grad\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aggregate_grads\u001b[39m(gradients):\n\u001b[0;32m    636\u001b[0m   \u001b[38;5;124;03m\"\"\"Aggregate gradients from multiple sources.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m    Otherwise returns an aggregated 'IndexedSlices'.\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    645\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m gradients, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients to aggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this loop version loads parts of the dataset into memory at a time, cycling through them\n",
    "batches_at_once = 1 # or 3 or 7 or 21\n",
    "epochs = 1\n",
    "cfsw_train_subset = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    epoch_loss = 0\n",
    "    # really want to take these in random order\n",
    "    for k in rng.permutation(21//batches_at_once):\n",
    "        batch_loss = 0\n",
    "        del cfsw_train_subset\n",
    "        timings_temp = time()\n",
    "        cfsw_train_subset = load_cfsw_batches(k*batches_at_once, (k+1)*batches_at_once, batch_size = batch_size)\n",
    "        timings[5] = time()-timings_temp\n",
    "        \n",
    "        for input_batch, label_batch in cfsw_train_subset:\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                loss_ = process_fs_batch(input_batch, label_batch)\n",
    "                print(loss_.numpy())\n",
    "                loss_sum = tf.math.reduce_sum(loss_).numpy()\n",
    "                batch_loss += loss_sum\n",
    "                epoch_loss += loss_sum\n",
    "                \n",
    "            timings_temp = time()\n",
    "            grad_decode = tape.gradient(loss_, testSpellDecode.trainable_variables)\n",
    "            grad_cogitate = tape.gradient(loss_, testSpellCogitate.trainable_variables)\n",
    "            grad_attend = tape.gradient(loss_, testSpellAttend.trainable_variables)\n",
    "            timings[4] = time() - timings_temp\n",
    "            print(timings)\n",
    "            print(get_var_list_stats(grad_decode))\n",
    "            print(get_var_list_stats(grad_cogitate))\n",
    "            print(get_var_list_stats(grad_attend))\n",
    "            \n",
    "            var_list_decay(testSpellDecode.trainable_variables, l2_coef)\n",
    "            var_list_decay(testSpellCogitate.trainable_variables, l2_coef)\n",
    "            var_list_decay(testSpellAttend.trainable_variables, l2_coef)\n",
    "            opt.apply_gradients(zip(grad_decode, testSpellDecode.trainable_variables))\n",
    "            opt.apply_gradients(zip(grad_cogitate, testSpellCogitate.trainable_variables))\n",
    "            opt.apply_gradients(zip(grad_attend, testSpellAttend.trainable_variables))\n",
    "                     \n",
    "        print('\\nAverage loss for batch ' + str(epoch)+\"/\"+str(k) +': ', batch_loss/(256*batches_at_once))\n",
    "    print('\\nAverage loss this epoch: ', epoch_loss/(21*256))\n",
    "print('Timings: ', timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1b611a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32.718094, 37152, 0.0008806549814833741)\n",
      "(187.1872, 31654912, 5.913369616197914e-06)\n",
      "(46.870586, 74688, 0.0006275517672887702)\n"
     ]
    }
   ],
   "source": [
    "print(get_var_list_stats(testSpellDecode.trainable_variables))\n",
    "print(get_var_list_stats(testSpellCogitate.trainable_variables))\n",
    "print(get_var_list_stats(testSpellAttend.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e35ee605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_16/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
       "array([[ 0.08459155,  0.07163593, -0.08155863, ..., -0.09446944,\n",
       "         0.13766198,  0.12962008],\n",
       "       [-0.05576618,  0.06116143, -0.01730708, ..., -0.06724232,\n",
       "         0.07287413,  0.00297883],\n",
       "       [ 0.06309021,  0.11109361, -0.11438391, ...,  0.04126996,\n",
       "         0.15530567, -0.14080565],\n",
       "       ...,\n",
       "       [ 0.11576623,  0.02597345,  0.08837017, ..., -0.17141892,\n",
       "         0.13526745, -0.04232301],\n",
       "       [ 0.09379844, -0.04267445, -0.19085574, ...,  0.13805334,\n",
       "         0.04832689, -0.12917371],\n",
       "       [-0.03968348, -0.06075937,  0.04498382, ..., -0.15817237,\n",
       "        -0.09669452, -0.02687849]], dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSpellDecode.trainable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d4931ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list_decay(testSpellDecode.trainable_variables, 1/2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d891f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_16/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
       "array([[ 0.08455025,  0.07160095, -0.08151881, ..., -0.09442332,\n",
       "         0.13759476,  0.12955679],\n",
       "       [-0.05573895,  0.06113156, -0.01729863, ..., -0.06720948,\n",
       "         0.07283854,  0.00297738],\n",
       "       [ 0.0630594 ,  0.11103936, -0.11432806, ...,  0.04124981,\n",
       "         0.15522984, -0.1407369 ],\n",
       "       ...,\n",
       "       [ 0.1157097 ,  0.02596077,  0.08832703, ..., -0.17133522,\n",
       "         0.13520141, -0.04230235],\n",
       "       [ 0.09375264, -0.04265362, -0.19076255, ...,  0.13798593,\n",
       "         0.04830329, -0.12911063],\n",
       "       [-0.03966411, -0.0607297 ,  0.04496186, ..., -0.15809514,\n",
       "        -0.09664731, -0.02686536]], dtype=float32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSpellDecode.trainable_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest is scratchwork, ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25b93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc29dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet_encoder associates an integer to each character, encoding text strings as numerical sequences\n",
    "alphabet_encoder = {'blank': 0, ' ': 1, '&': 2, \"'\": 3, '.': 4, '@': 5, 'a': 6, 'b': 7, 'c': 8, 'd': 9, 'e': 10,\n",
    "                    'f': 11, 'g': 12, 'h': 13, 'i': 14, 'j': 15, 'k': 16, 'l': 17, 'm': 18, 'n': 19, 'o': 20,\n",
    "                    'p': 21, 'q': 22, 'r': 23, 's': 24, 't': 25, 'u': 26, 'v': 27, 'w': 28, 'x': 29, 'y': 30, 'z': 31}\n",
    "\n",
    "# alphabet_decoder conversely replaces numerical sequences with character strings\n",
    "alphabet_decoder = {alphabet_encoder[char] : char for char in alphabet_encoder.keys()}\n",
    "alphabet_decoder[0] = '_' # representation for the blank character\n",
    "\n",
    "def decode_label_seq(seq, tensor_chars = False, decoder=alphabet_decoder):\n",
    "    if tensor_chars:\n",
    "        return str().join([decoder.get(index.numpy(),'*') for index in seq])\n",
    "    return str().join([decoder.get(index,'*') for index in seq])\n",
    "\n",
    "# beam search returns for \n",
    "def logits_to_string_beam(logits, label_length, **kwargs):\n",
    "    # logits is a tensor of shape [max_time, batch_size, alphabet_size]\n",
    "    # label_lengths records the number of characters given for each logit set in the batch\n",
    "    \n",
    "    # top_paths = 1  controls how many search results are returned\n",
    "    # beam_width = 100  controls how many probabilities are maintained at each step of computation\n",
    "    \n",
    "    results, probs = tf.nn.ctc_beam_search_decoder(tf.expand_dims(logits, axis=1), \n",
    "                                                   [label_length], **kwargs)\n",
    "    \n",
    "    # An individual return is a SparseTensor, with bounding shape [batch_size, max_output_length], with:\n",
    "    # .indices pairs (batch_number, character_position) and .values the output characters.\n",
    "    ret = []\n",
    "    #print(results[0].values)\n",
    "    for result in results:\n",
    "        #ret.append(str().join([alphabet_decoder.get(char.numpy(), '*')for char in result.values]))\n",
    "        ret.append(decode_label_seq(result.values, True))\n",
    "    return ret, tf.squeeze(probs).numpy()\n",
    "\n",
    "def logits_to_string_greedy(logits, label_length, **kwargs):\n",
    "    results, probs = tf.nn.ctc_greedy_decoder(tf.expand_dims(logits, axis=1), \n",
    "                                                   [label_length],\n",
    "                                                   blank_index=0, **kwargs)\n",
    "    ret = []\n",
    "    #print(results[0].values)\n",
    "    for result in results:\n",
    "        #ret.append(str().join([alphabet_decoder.get(char.numpy(), '*')for char in result.values]))\n",
    "        ret.append(decode_label_seq(result.values, True))\n",
    "    return ret, tf.squeeze(probs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043efdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_cfsw_batches(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04d122e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: it \n",
      "beam A: (['_', 'i_', 's_'], array([-1.9809374, -3.7772012, -3.81764  ], dtype=float32)) \n",
      "beam B: (['_', 'i_', 's_'], array([-4.8712807, -5.3748426, -5.5574226], dtype=float32))\n",
      "label: asl literature \n",
      "beam A: (['_', '_s_', '_u_'], array([-3.7691383, -4.4382334, -4.7812605], dtype=float32)) \n",
      "beam B: (['_e_e_', '_e_e_e_', '_e_e_l_'], array([-18.223228, -18.256298, -18.472479], dtype=float32))\n",
      "label: all \n",
      "beam A: (['_', '_s_', '_e_'], array([-2.691038 , -4.0412016, -4.2800803], dtype=float32)) \n",
      "beam B: (['_', '_s_', '_e_'], array([-2.4200158, -3.9401357, -4.296234 ], dtype=float32))\n",
      "label: fotgs \n",
      "beam A: (['_e_', '_l_', '_i_'], array([-10.457163, -10.60331 , -10.723205], dtype=float32)) \n",
      "beam B: (['_', '_s_', '_e_'], array([-3.2630477, -4.228913 , -4.577375 ], dtype=float32))\n",
      "label: ok \n",
      "beam A: (['_', 'i_', '_e_'], array([-5.32949  , -5.9381557, -5.940513 ], dtype=float32)) \n",
      "beam B: (['_', '_e_', '_l_'], array([-3.7440312, -4.594789 , -4.6847715], dtype=float32))\n",
      "label: asl lit \n",
      "beam A: (['_', '_s_', '_e_'], array([-2.6221313, -3.8634055, -4.3941054], dtype=float32)) \n",
      "beam B: (['_e_', '_l_', '_i_'], array([-10.810573, -10.965918, -11.058255], dtype=float32))\n",
      "label: so \n",
      "beam A: (['_', '_e_', '_s_'], array([-2.9361515, -4.1991315, -4.2375045], dtype=float32)) \n",
      "beam B: (['_', '_s_', '_e_'], array([-2.015323, -3.882552, -4.091027], dtype=float32))\n",
      "label: aslized \n",
      "beam A: (['_', '_s_', '_u_'], array([-2.7454705, -3.8660307, -4.3889093], dtype=float32)) \n",
      "beam B: (['_e_', '_l_', '_e_e_'], array([-11.387122, -11.542407, -11.643089], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "for input_batch, label_batch test_dataset:\n",
    "    decoder_results_list_a, decoder_results_list_b, out_lengths = process_fs_batch(input_batch, label_batch, \n",
    "                                                                      return_lists=True, is_training=False)\n",
    "    for k in range(batch_size):\n",
    "        #print(decoder_results_list)\n",
    "        print('label:', decode_label_seq(label_batch[0][k][0:label_batch[1][k].numpy()].numpy()),\n",
    "              '\\nbeam A:', logits_to_string_beam(decoder_results_list_a[:,k,:], \n",
    "                                                 np.int32(out_lengths[k]), top_paths=3),\n",
    "              '\\nbeam B:', logits_to_string_beam(decoder_results_list_b[:,k,:], \n",
    "                                                 np.int32(out_lengths[k]), top_paths=3),)\n",
    "              #', greedy A:', logits_to_string_greedy(decoder_results_list_a[:,k,:], np.int32(out_lengths[k])),\n",
    "              #', greedy B:', logits_to_string_greedy(decoder_results_list_b[:,k,:], np.int32(out_lengths[k])))\n",
    "          \n",
    "    #loss = tf.nn.ctc_loss(label_batch[0],\n",
    "    #                 decoder_results_list,\n",
    "    #                 label_batch[1], \n",
    "    #                 2*out_lengths,  \n",
    "    #                 logits_time_major=True)  \n",
    "    #total_loss += tf.keras.metrics.Sum()(loss)\n",
    "    \n",
    "    break\n",
    "#print('total loss = ', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76697e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1682447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the idea of presenting the LSTM middle of the network with information at variable time-scales,\n",
    "# we consider also a 2d convolutional network that operates only on individual frames\n",
    "\n",
    "## check this makes sense for 5d input\n",
    "class testSpeller2dEncoder(tf.Module):\n",
    "    def __init__(self, conv_filters, reg_coef=0, spatial_dropout_prob=0.01):\n",
    "        super(testSpeller2dEncoder, self).__init__()\n",
    "        filters_1, filters_2, filters_3, filters_4 = conv_filters\n",
    "        self.reg = tf.keras.regularizers.L2(reg_coef)\n",
    "        self.spatial_dropout_prob = spatial_dropout_prob \n",
    "        \n",
    "        self.conv_1a_rgb = tf.keras.layers.Convolution2D(filters_1, 5, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_1b_rgb = tf.keras.layers.Convolution2D(filters_1, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_2a_rgb = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_2b_rgb = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_3a_rgb = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_3b_rgb = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, activation='relu') \n",
    "        self.conv_4a_rgb = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_4b_rgb = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "        \n",
    "        self.conv_1a_flow = tf.keras.layers.Convolution2D(filters_1, 5, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_1b_flow = tf.keras.layers.Convolution2D(filters_1, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_2a_flow = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_2b_flow = tf.keras.layers.Convolution2D(filters_2, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_3a_flow = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_3b_flow = tf.keras.layers.Convolution2D(filters_3, 3, padding='same', use_bias=True, activation='relu') \n",
    "        self.conv_4a_flow = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "        self.conv_4b_flow = tf.keras.layers.Convolution2D(filters_4, 3, padding='same', use_bias=True, activation='relu')\n",
    "\n",
    "    def __call__(self, inputs, is_training=True):\n",
    "        # 128x128xch input\n",
    "        rgb_out = tf.keras.layers.GaussianNoise(0.03)(tf.image.convert_image_dtype(inputs[0].to_tensor(), \n",
    "                                                                                   dtype=tf.float32),\n",
    "                                                      training=is_training)\n",
    "        rgb_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_1a(rgb_out),\n",
    "                                                                            training=is_training)\n",
    "        rgb_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_1b(rgb_out)) # 64x64xch        \n",
    "        rgb_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_2a(rgb_out),\n",
    "                                                                            training=is_training)\n",
    "        rgb_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_2b(rgb_out)) # 32x32xch        \n",
    "        rgb_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob) (self.conv_3a(rgb_out),\n",
    "                                                                            training=is_training)\n",
    "        rgb_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_3b(rgb_out)) # 16x16xch        \n",
    "        rgb_out = self.conv_4a(rgb_out) \n",
    "        rgb_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_4b(rgb_out)) # out: 8x8xch\n",
    "        \n",
    "        # 128x128xch input\n",
    "        flow_out = tf.keras.layers.GaussianNoise(0.03)(tf.image.convert_image_dtype(inputs[1].to_tensor(), \n",
    "                                                                                    dtype=tf.float32),\n",
    "                                                       training=is_training)\n",
    "        flow_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_1a(flow_out),\n",
    "                                                                            training=is_training)\n",
    "        flow_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_1b(flow_out)) # 64x64xch        \n",
    "        flow_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob)(self.conv_2a(flow_out),\n",
    "                                                                            training=is_training)\n",
    "        flow_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_2b(flow_out)) # 32x32xch\n",
    "        flow_out = tf.keras.layers.SpatialDropout2D(self.spatial_dropout_prob) (self.conv_3a(flow_out),\n",
    "                                                                            training=is_training)\n",
    "        flow_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_3b(flow_out)) # 16x16xch\n",
    "        flow_out = self.conv_4a(flow_out)\n",
    "        flow_out = tf.keras.layers.MaxPool2D(strides=(2,2))(self.conv_4b(flow_out)) # out: 8x8xch\n",
    "        \n",
    "        return rgb_out, flow_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "748bd460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Conv3d_1a_7x7', 'MaxPool3d_2a_3x3', 'Conv3d_2b_1x1', 'Conv3d_2c_3x3', 'MaxPool3d_3a_3x3', 'Mixed_3b', 'Mixed_3c', 'MaxPool3d_4a_3x3', 'Mixed_4b', 'Mixed_4c', 'Mixed_4d', 'Mixed_4e', 'Mixed_4f', 'MaxPool3d_5a_2x2'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0a0a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv3d_1a_7x7 (8, 24, 64, 64, 64)\n",
      "MaxPool3d_2a_3x3 (8, 24, 32, 32, 64)\n",
      "Conv3d_2b_1x1 (8, 24, 32, 32, 64)\n",
      "Conv3d_2c_3x3 (8, 24, 32, 32, 192)\n",
      "MaxPool3d_3a_3x3 (8, 24, 16, 16, 192)\n",
      "Mixed_3b (8, 24, 16, 16, 256)\n",
      "Mixed_3c (8, 24, 16, 16, 480)\n",
      "MaxPool3d_4a_3x3 (8, 24, 8, 8, 480)\n",
      "Mixed_4b (8, 24, 8, 8, 512)\n",
      "Mixed_4c (8, 24, 8, 8, 512)\n",
      "Mixed_4d (8, 24, 8, 8, 512)\n",
      "Mixed_4e (8, 24, 8, 8, 528)\n",
      "Mixed_4f (8, 24, 8, 8, 832)\n",
      "MaxPool3d_5a_2x2 (8, 12, 4, 4, 832)\n"
     ]
    }
   ],
   "source": [
    "for key in results[1].keys():\n",
    "    print(key, results[1][key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed937859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
